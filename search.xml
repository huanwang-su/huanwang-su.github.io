<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[4 maven依赖机制]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F4%20maven%E4%BE%9D%E8%B5%96%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[其中一个Maven的核心特征是依赖管理。一旦我们处理多个模块项目（包含数百个模块/子项目）,管理依赖关系变得困难的任务。Maven提供了一个高程度的控制来管理这样的场景。 传递性依赖假设你的项目依赖于一个库，而这个库又依赖于其他库。你不必自己去找出所有这些依赖，你只需要加上你直接依赖的库，Maven会隐式的把这些库间接依赖的库也加入到你的项目中。这个特性是靠解析从远程仓库中获取的依赖库的项目文件实现的。一般的，这些项目的所有依赖都会加入到项目中，或者从父项目继承，或者通过传递性依赖。 传递性依赖的嵌套深度没有任何限制，只是在出现循环依赖时会报错。 传递性依赖会导致包含库的依赖图增长的非常大。为了解决这个问题，Maven也提供了额外的机制，能让你指定哪些依赖会被包含： 依赖调解 – 当项目中出现多个版本构件依赖的情形，依赖调解决定最终应该使用哪个版本。目前，支持“短路径优先”原则，项目会选择依赖关系树中路径最短的版本作为依赖，当两个版本的依赖路径长度一致时，POM中依赖声明的顺序决定了哪个版本会被使用，也叫作”第一声明原则”。 “短路径优先”意味着项目依赖关系树中路径最短的版本会被使用。例如，假设A、B、C之间的依赖关系是A-&gt;B-&gt;C-&gt;D(2.0)和A-&gt;E-&gt;(D1.0)，那么D(1.0)会被使用，因为A通过E到D的路径更短。但如果你想要强制使用D(2.0)，那你也可以在A中显式声明对D(2.0)的依赖。 依赖管理 – 在出现传递性依赖或者没有指定版本时，可以通过依赖管理直接指定模块版本。由于传递性依赖，尽管某个依赖没有被A直接指定，但也会被引入。相反的，A也可以将D加入元素中，并在D可能被引用时决定D的版本号。 依赖范围 – 你可以指定只在当前编译范围内包含合适的依赖。 排除依赖 – 如果项目X依赖于项目Y，项目Y又依赖项目Z，项目X的所有者可以使用”exclusion”元素来显式排除项目Z。 可选依赖 – 如果项目Y依赖项目Z，项目Y的所有者可以用”optional”元素来指定项目Z作为X的可选依赖。那么当项目X依赖项目Y时，X只依赖Y并不依赖Y的可选依赖Z。项目X的所有者也可以根据自己的意愿显式指定X对Z的依赖。（你可以把可选依赖理解为默认排除）。 依赖范围依赖范围会影响传递性依赖，同时也会影响项目构建任务中使用的classpath。Maven有以下6种依赖范围： compile这是默认范围。编译依赖对项目所有的classpath都可用。此外，编译依赖会传递到依赖的项目。 provided和compile范围很类似，但provided范围表明你希望由JDK或者某个容器提供运行时依赖。例如，当使用Java EE构建一个web应用时，你会设置对Servlet API和相关的Java EE APIs的依赖范围为provided，因为web容器提供了运行时的依赖。provided依赖只对编译和测试classpath有效，并且不能传递。 runtimeruntime范围表明编译时不需要依赖，而只在运行时依赖。此依赖范围对运行和测试classpath有效，对编译classpath无效。 testtest范围表明使用此依赖范围的依赖，只在编译测试代码和运行测试的时候需要，应用的正常运行不需要此类依赖。 system系统范围与provided类似，不过你必须显式指定一个本地系统路径的JAR，此类依赖应该一直有效，Maven也不会去仓库中寻找它。 import（Maven2.0.9及以上）import范围只适用于pom文件中的部分。表明指定的POM必须使用部分的依赖。因为依赖已经被替换，所以使用import范围的依赖并不影响依赖传递。 依赖管理如果你的项目有多个子模块，而且每个模块都需要引入依赖，但为了项目的正确运行，必须让所有的子项目(以下子项目即指子模块)使用依赖项的统一版本,Maven 使用 dependencyManagement 来统一模块见的依赖版本问题。 在父项目的POM文件中，我们会使用到dependencyManagement元素。通过它来管理jar包的版本，让子项目中引用一个依赖而不用显示的列出版本号。Maven会沿着父子层次向上走，直到找到一个拥有dependencyManagement元素的项目，然后它就会使用在dependencyManagement元素中指定的版本号。 在顶层中pom.xml 123456789&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;$&#123;javaee-api.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 在子模块中的使用父pom.xml的依赖： 1234567891011121314&lt;!--继承父类--&gt; &lt;parent&gt; &lt;artifactId&gt;父artifactId&lt;/artifactId&gt; &lt;groupId&gt;父groupId&lt;/groupId&gt; &lt;version&gt;父version&lt;/version&gt; &lt;relativePath&gt;父pom.xml的相对路径&lt;/relativePath&gt; &lt;/parent&gt; &lt;!--依赖关系--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 这两个POM都依赖于同一个模块，同时每个POM又各自依赖于一个无关的模块。父项目的POM详细信息如下所示： dependencies 直接引入依赖相对于dependencyManagement，父类中直接使用所有生命在dependencies里的依赖都会自动引入，并默认被所有的子项目继承。 如果依赖只在某个子项目中使用，则可以在子项目的pom.xml中直接引入，防止父pom的过于臃肿。 dependencies与dependencyManagement 区别总结 dependencies即使在子项目中不写该依赖项，那么子项目仍然会从父项目中继承该依赖项（全部继承） dependencyManagement里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且version和scope都读取自父pom;另外如果子项目中指定了版本号，那么会使用子项目中指定的jar版本。 排除依赖传递性依赖会给项目隐式地引入很多依赖，着极大简化了项目依赖的管理，但是有些时候这种特性也会带来问题。例如，当前项目有一个第三方依赖，而这个第三方依赖由于某些原因依赖了另一个类库的SNAPSHOT版本，那么这个SNAPSHOT就会成为当前项目的传递性依赖，而SNAPSHOT的不稳定性会直接影响到当前的项目。这时就需要排除掉该SNAPSHOT，并且在当前项目中声明该类库的某个正式发布的版本。 C—&gt;B—&gt;A。加入现在不想执行C时把A下载进来，那么我们可以用 标签。 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.cc.maven&lt;/groupId&gt; &lt;artifactId&gt;project-b&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.cc.maven&lt;/groupId&gt; &lt;artifactId&gt;project-c&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;&lt;/dependencies&gt; exclusions可以包含一个或者多个exclusion子元素，因此可以排除一个或者多个传递性依赖。 可选依赖maven的依赖关系是有传递性的。如：A–&gt;B，B–&gt;C。但有时候，项目A可能不是必需依赖C，因此需要在项目A中排除对A的依赖。在maven的依赖管理中，有两种方式可以对依赖关系进行，分别是可选依赖（Optional Dependencies）以及依赖排除（Dependency Exclusions）。 pom.xml文件中, Optional标签表示该依赖是否可选，默认是false。如果为true，则表示该依赖不会传递下去，如果为false，则会传递下去。 12345678910111213&lt;project&gt; ... &lt;dependencies&gt; &lt;!-- declare the dependency to be set as optional --&gt; &lt;dependency&gt; &lt;groupId&gt;sample.ProjectB&lt;/groupId&gt; &lt;artifactId&gt;Project-B&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;!-- value will be true or false only --&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 假设以上配置是项目A的配置，即：Project-A –&gt; Project-B。在编译项目A时，是可以正常通过的。 如果有一个新的项目X依赖A，即：Project-X -&gt; Project-A。此时项目X就不会依赖项目B了。如果项目X用到了涉及项目B的功能，那么就需要在pom.xml中重新配置对项目B的依赖。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7 maven插件]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F7%20maven%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Maven 的插件Maven 是一个执行插件的框架，每一个任务实际上是由插件完成的。Maven 插件通常用于： 创建 jar 文件 创建 war 文件 编译代码文件 进行代码单元测试 创建项目文档 创建项目报告 一个插件通常提供了一组目标，可使用以下语法来执行： 1mvn [plugin-name]:[goal-name] 例如，一个 Java 项目可以使用 Maven 编译器插件来编译目标，通过运行以下命令编译 1mvn compiler:compile 插件类型Maven 提供以下两种类型插件： 类型 描述 构建插件 在生成过程中执行，并在 pom.xml 中的 元素进行配置 报告插件 在网站生成期间执行，在 pom.xml 中的 元素进行配置 以下是一些常见的插件列表： 插件 描述 clean 编译后的清理目标，删除目标目录 compiler 编译 Java 源文件 surefile 运行JUnit单元测试，创建测试报告 jar 从当前项目构建 JAR 文件 war 从当前项目构建 WAR 文件 javadoc 产生用于该项目的 Javadoc antrun 从构建所述的任何阶段运行一组 Ant 任务 例子我们使用 maven-antrun-plugin 插件在例子中来在控制台打印数据。现在在 C:\MVN\project 文件夹 创建一个 pom.xml 文件，内容如下： 1234567891011121314151617181920212223242526272829303132&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.companyname.projectgroup&lt;/groupId&gt;&lt;artifactId&gt;project&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;build&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;id.clean&lt;/id&gt; &lt;phase&gt;clean&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;clean phase&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;&lt;/project&gt; 接下来，打开命令控制台，并转到包含 pom.xml 的文件夹并执行以下命令 mvn 命令。 1C:\MVN\project&gt;mvn clean Maven 将开始处理并显示清洁周期/阶段，如下图中输出： 1234567891011121314151617[INFO] Scanning for projects...[INFO] ------------------------------------------------------------------[INFO] Building Unnamed - com.companyname.projectgroup:project:jar:1.0[INFO] task-segment: [post-clean][INFO] ------------------------------------------------------------------[INFO] [clean:clean &#123;execution: default-clean&#125;][INFO] [antrun:run &#123;execution: id.clean&#125;][INFO] Executing tasks [echo] clean phase[INFO] Executed tasks[INFO] ------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ------------------------------------------------------------------[INFO] Total time: &lt; 1 second[INFO] Finished at: Sat Jul 07 13:38:59 IST 2012[INFO] Final Memory: 4M/44M[INFO] ------------------------------------------------------------------ 上面的例子说明： 插件可在 pom.xml 使用的 plugin 元素来指定； 每个插件可以有多个目标； 从插件应使用它的相位元素开始处理定义阶段。这里已经使用 clean 阶段； 可以通过将它们绑定到插件的目标来执行配置任务。这里已经绑定 echo 任务到 maven-antrun-plugin 的运行目标； 就这样，Maven将处理其余部分。如果没有可用的本地存储库，它会下载这个插件； Maven插件调用Maven本质上是一个插件框架，它的核心并不执行任何具体的构建任务，所有这些任务都交给插件来完成。例如编译源代码是由maven-compiler-plugin完成的。 进一步说，每个任务对应了一个插件目标（goal），每个插件会有一个或者多个目标。 例如maven-compiler-plugin的compile目标用来编译位于src/main/java/目录下的主源码，testCompile目标用来编译位于src/test/java/目录下的测试源码。 用户可以通过两种方式调用Maven插件目标。 将插件目标与生命周期阶段（lifecycle phase）绑定，这样用户在命令行只是输入生命周期阶段而已。 例如Maven默认将maven-compiler-plugin的compile目标与compile生命周期阶段绑定，因此命令mvn compile实际上是先定位到compile这一生命周期阶段，然后再根据绑定关系调用maven-compiler-plugin的compile目标。 直接在命令行指定要执行的插件目标。 例如mvn archetype:generate 就表示调用maven-archetype-plugin的generate目标，这种带冒号的调用方式与生命周期无关。 常见插件Maven官方有两个插件列表： 第一个列表的GroupId为org.apache.maven.plugins，这里的插件最为成熟，具体地址为：http://maven.apache.org/plugins/index.html。 第二个列表的GroupId为org.codehaus.mojo，这里的插件没有那么核心，但也有不少十分有用，其地址为：http://mojo.codehaus.org/plugins.html。 maven-antrun-pluginhttp://maven.apache.org/plugins/maven-antrun-plugin/ maven-antrun-plugin能让用户在Maven项目中运行Ant任务。用户可以直接在该插件的配置以Ant的方式编写Target，然后交给该插件的run目标去执行。 maven-archetype-pluginhttp://maven.apache.org/archetype/maven-archetype-plugin/ Archtype指项目的骨架，Maven初学者最开始执行的Maven命令可能就是mvn archetype:generate，这实际上就是让maven-archetype-plugin生成一个很简单的项目骨架，帮助开发者快速上手。 maven-archetype-plugin还有一些其他目标帮助用户自己定义项目原型，例如你由一个产品需要交付给很多客户进行二次开发，你就可以为他们提供一个Archtype，帮助他们快速上手。 maven-dependency-pluginhttp://maven.apache.org/plugins/maven-dependency-plugin/ maven-dependency-plugin最大的用途是帮助分析项目依赖 dependency:list能够列出项目最终解析到的依赖列表 dependency:tree能进一步的描绘项目依赖树 dependency:analyze可以告诉你项目依赖潜在的问题 如果你有直接使用到的却未声明的依赖，该目标就会发出警告。 maven-dependency-plugin还有很多目标帮助你操作依赖文件，例如dependency:copy-dependencies能将项目依赖从本地Maven仓库复制到某个特定的文件夹下面。 maven-enforcer-pluginhttp://maven.apache.org/plugins/maven-enforcer-plugin/ 在一个稍大一点的组织或团队中，你无法保证所有成员都熟悉Maven，那他们做一些比较愚蠢的事情就会变得很正常。 例如给项目引入了外部的SNAPSHOT依赖而导致构建不稳定，使用了一个与大家不一致的Maven版本而经常抱怨构建出现诡异问题。 maven-enforcer-plugin能够帮助你避免之类问题，它允许你创建一系列规则强制大家遵守，包括设定Java版本、设定Maven版本、禁止某些依赖、禁止SNAPSHOT依赖。 只要在一个父POM配置规则，然后让大家继承，当规则遭到破坏的时候，Maven就会报错。 除了标准的规则之外，你还可以扩展该插件，编写自己的规则。maven-enforcer-plugin的enforce目标负责检查规则，它默认绑定到生命周期的validate阶段。 maven-release-pluginhttp://maven.apache.org/plugins/maven-release-plugin/ maven-release-plugin的用途是帮助自动化项目版本发布，它依赖于POM中的SCM信息。 release:prepare用来准备版本发布，具体的工作包括检查是否有未提交代码、检查是否有SNAPSHOT依赖、升级项目的SNAPSHOT版本至RELEASE版本、为项目打标签等等。 release:perform则是签出标签中的RELEASE源码，构建并发布。版本发布是非常琐碎的工作，它涉及了各种检查，而且由于该工作仅仅是偶尔需要，因此手动操作很容易遗漏一些细节。 maven-release-plugin让该工作变得非常快速简便，不易出错。maven-release-plugin的各种目标通常直接在命令行调用，因为版本发布显然不是日常构建生命周期的一部分。 maven-resources-pluginhttp://maven.apache.org/plugins/maven-resources-plugin/ 为了使项目结构更为清晰，Maven区别对待Java代码文件和资源文件，maven-compiler-plugin用来编译Java代码，maven-resources-plugin则用来处理资源文件。 默认的主资源文件目录是src/main/resources，很多用户会需要添加额外的资源文件目录，这个时候就可以通过配置maven-resources-plugin来实现。 此外，资源文件过滤也是Maven的一大特性，你可以在资源文件中使用${propertyName}形式的Maven属性，然后配置maven-resources-plugin开启对资源文件的过滤， 之后就可以针对不同环境通过命令行或者Profile传入属性的值，以实现更为灵活的构建。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2 maven的使用]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F2%20maven%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一些基本的操作，编译，构建，单元测试，安装，网站生成和基于Maven部署项目。 使用Maven创建项目从 Maven 模板创建一个项目使用命令行 123456789101112mvn archetype:generate -DgroupId=&#123;project-packaging&#125; -DartifactId=&#123;project-name&#125;-DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false官网例子：mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false 这告诉 Maven 来从 maven-archetype-quickstart 模板创建 Java 项目。 如果刚刚安装了Maven，则第一次运行可能需要一段时间。这是因为Maven正在将最新的文件（插件和其他文件）下载到本地存储库中。 archetype说明 简而言之，Archetype是一个Maven项目模板工具包。Archetype将帮助作者为用户创建Maven项目模板，并为用户提供生成这些项目模板的参数化版本的方法。 使用原型提供了一种很好的方式，可以让开发人员以与您的项目或组织采用的最佳实践相一致的方式快速开展工作。 Using an Archetype mvn archetype:generate Creating Archetypes 参考官网 http://maven.apache.org/guides/mini/guide-creating-archetypes.html 或者部分maven插件可以实现create Provided Archetypes Maven provides several Archetype artifacts: | Archetype ArtifactIds | Description || ————————— | ———————————————————— || maven-archetype-archetype | An archetype to generate a sample archetype project. || maven-archetype-j2ee-simple | An archetype to generate a simplifed sample J2EE application. || maven-archetype-mojo | An archetype to generate a sample a sample Maven plugin. || maven-archetype-plugin | An archetype to generate a sample Maven plugin. || maven-archetype-plugin-site | An archetype to generate a sample Maven plugin site. || maven-archetype-portlet | An archetype to generate a sample JSR-268 Portlet. || maven-archetype-quickstart | An archetype to generate a sample Maven project. || maven-archetype-simple | An archetype to generate a simple Maven project. || maven-archetype-site | An archetype to generate a sample Maven site which demonstrates some of the supported document types like APT, XDoc, and FML and demonstrates how to i18n your site. || maven-archetype-site-simple | An archetype to generate a sample Maven site. || maven-archetype-webapp | An archetype to generate a sample Maven Webapp project. | Maven目录布局123456789101112131415my-app|-- pom.xml`-- src |-- main | `-- java | `-- com | `-- mycompany | `-- app | `-- App.java `-- test `-- java `-- com `-- mycompany `-- app `-- AppTest.java 所有的源代码放在文件夹 /src/main/java/, 所有的单元测试代码放入 /src/test/java/ 附加的一个标准的 pom.xml 被生成。它描述了整个项目的信息 POM文件pom.xml文件是Maven中项目配置的核心。它是一个单独的配置文件，其中包含以您想要的方式构建项目所需的大部分信息。项目的插件，项目依赖，如何构建这个项目等 123456789101112131415161718192021&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.8.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 使用Maven构建项目1mvn package 当你运行“mvn package”命令，它会编译源代码，运行单元测试和包装这取决于在pom.xml文件的“packaging”标签。 例如, If “packaging” = jar, 将您的项目打包成一个“jar”文件 如果 “packaging” = war,将您的项目打包成“war”文件 使用Maven清理项目1mvn clean 在基于Maven的项目中，很多缓存输出在“target”文件夹中。如果想建立项目部署，必须确保清理所有缓存的输出，从面能够随时获得最新的部署。 当mvn clean执行，在target文件夹中的一切都将被删除。 部署进行生产要部署您的项目进行生产，它总是建议使用 “mvn clean package“, 以确保始终获得最新的部署。 使用Maven运行单元测试1mvn test 这会在你的项目中运行整个单元测试。 将项目安装到Maven本地资源库1mvn install 在Maven中，可以使用“mvn install”打包项目，并自动部署到本地资源库，让其他开发人员使用它。 注意，当“install”在执行阶段，上述所有阶段 “validate“, “compile“, “test“, “package“, “integration-test“, “verify” 阶段, 包括目前的“install”阶段将被执行有序。 生成基于Maven的项目文档站点在Maven中，可以使用“mvn site”，为您的项目信息生成文档站点。 1mvn site 生成的网站是在项目的“target/site”文件夹中。 ==一般用不到== 使用“mvn site-deploy”部署站点即将上节发布到服务器上 重要性低，参考https://www.yiibai.com/maven/deploy-site-with-mvn-]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1 maven简介]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F1%20maven%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Apache Maven是一个软件项目管理和综合工具。 Maven是什么？Maven是一个项目管理和综合工具。Maven提供了开发人员构建一个完整的生命周期框架。开发团队可以自动完成项目的基础工具建设，Maven使用标准的目录结构和默认构建生命周期。 在多个开发团队环境时，Maven可以设置按标准在非常短的时间里完成配置工作。Maven让开发人员的工作更轻松，同时创建报表，检查，构建和测试自动化设置。 Maven提供了开发人员的方式来管理： Builds Documentation Reporting Dependencies SCMs Releases Distribution mailing list 概括地说，Maven简化和标准化项目建设过程。处理编译，分配，文档，团队协作和其他任务的无缝连接。 Maven增加可重用性并负责建立相关的任务。 Maven目标 简化构建过程 提供统一的构建系统 提供优质的项目信息 提供最佳实践开发指南 允许透明迁移到新功能 Maven项目的结构和内容在一个XML文件中声明，pom.xml 项目对象模型（POM），这是整个Maven系统的基本单元。 pom.xmlPOM代表项目对象模型。它是 Maven 中工作的基本单位 POM 包含的项目是使用 Maven 来构建的，它用来包含各种配置信息。POM 也包含了目标和插件。==在执行任务或目标时，Maven 会使用当前目录中的 POM。它读取POM得到所需要的配置信息，然后执行目标。==部分的配置可以在 POM 使用如下： project dependencies plugins goals build profiles project version developers mailing list POM的例子123456789&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.yiibai.project-group&lt;/groupId&gt; &lt;artifactId&gt;project&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;project&gt; 每个项目只有一个POM文件(如果是父子关系可以有多个)。 所有的 POM 文件要项目元素必须有三个必填字段: groupId，artifactId，version 在库中的项目符号是：groupId:artifactId:version pom.xml 的根元素是 project，它有三个主要的子节点。 节点 描述 groupId 这是项目组的编号，这在组织或项目中通常是独一无二的。 例如，一家银行集团com.company.bank拥有所有银行相关项目。 artifactId 这是项目的ID。这通常是项目的名称。 例如，consumer-banking。 除了groupId之外，artifactId还定义了artifact在存储库中的位置。 version 这是项目的版本。与groupId一起使用，artifact在存储库中用于将版本彼此分离。 例如：com.company.bank:consumer-banking:1.0，com.company.bank:consumer-banking:1.1 超级POMMaven有一个超级POM，所有的POM均继承此文件（尽管未明确指定），并包含继承默认值，参考https://www.cnblogs.com/chowmin/articles/3877164.html Maven资源库Maven 位置 Maven本地资源库 Maven 的本地资源库是用来存储项目的依赖库，默认的文件夹是 “.m2” 目录，可能需要将其更改为另一个文件夹 Maven中央存储库 Maven 中央存储库是 Maven 用来下载所有项目的依赖库的默认位置。 远程存储库 并非所有的库存储在Maven的中央存储库，很多时候需要添加一些远程仓库来从其他位置，而不是默认的中央存储库下载库。例如用户自定义的仓库 常用命令mvn archetype:generate 创建Maven项目 mvn compile 编译源代码 mvn deploy 发布项目 mvn test-compile 编译测试源代码 mvn test 运行应用程序中的单元测试 mvn site 生成项目相关信息的网站 mvn clean 清除项目目录中的生成结果 mvn package 根据项目生成的jar mvn install 在本地Repository中安装jar mvn eclipse:eclipse 生成eclipse项目文件 mvnjetty:run 启动jetty服务 mvntomcat:run 启动tomcat服务 mvn clean package -Dmaven.test.skip=true:清除以前的包后重新打包，跳过测试类]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3 maven仓库]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F3%20maven%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Maven仓库就是放置所有JAR文件（WAR，ZIP，POM等等）的地方，所有Maven项目可以从同一个Maven仓库中获取自己所需要的依赖JAR，这节省了磁盘资源。 由于Maven仓库中所有的JAR都有其自己的坐标，该坐标告诉Maven它的组ID，构件ID，版本，打包方式等等，因此Maven项目可以方便的进行依赖版本管理。 Maven本地资源库Maven的本地资源库是用来存储所有项目的依赖关系(插件jar和其他文件，这些文件被Maven下载)到本地文件夹。当你建立一个Maven项目，所有相关文件将被存储在你的Maven本地仓库。 默认情况下，Maven的本地资源库默认为 .m2 目录文件夹： Unix/Mac OS X – ~/.m2 Windows – C:\Documents and Settings{your-username}.m2 修改Maven的本地库地址{M2_HOME}\conf\setting.xml 1234567&lt;settings&gt;&lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ~/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;D:\software\yiibai.com\apache-maven\repository&lt;/localRepository&gt; 更新依赖一般IDE上操作即可，命令不常用 12mvn clean install -e -U-e详细异常，-U强制更新 Maven中央存储库当建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – http://repo1.maven.org/maven2/ 查找下载 中央仓库也在{M2_HOME}\conf\setting.xml修改，改为阿里云的比较快。 Maven远程存储库 有时，Maven不能从依赖中央存储库找到上述库，那么它停下构建过程并输出错误消息到控制台。为了防止这种情况，Maven提供远程仓库概念，这是开发商的自定义库包含所需的库文件或其他项目 jar 文件。 仓库的优先级顺序在maven中，仓库可以分为：本地仓库、远程仓库。远程仓库可以分为：中央仓库、私服仓库。中央仓库是maven官方指定的仓库，可以理解为“寻找的最后一站”。私服仓库可以是自己建的，也可以是其它主体建的（比如aliyun的maven仓库，jboss的maven仓库等）。 maven寻找得顺序大致可以理解为： 在本地仓库中寻找，如果没有则进入下一步。 在中央仓库中寻找，如果没有则进入下一步。 在私服仓库中寻找，如果没有则终止寻找。 补充： 如果仓库的id设置成“central”，则该配置会覆盖maven默认的中央仓库配置。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5 maven pom文件]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F5%20maven%20pom%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[官网：http://maven.apache.org/pom.html 介绍定义：The POM 4.0.0 XSD and descriptor reference documentation POM是什么POM代表“项目对象模型”。 在Maven项目是一个名为pom.xml的文件表示。 Maven包含一个项目除了代码其他东西。包含配置文件，所涉及的开发人员以及他们所扮演的角色，组织和许可证，项目所在地的URL，项目的依赖关系等， 事实上，在Maven中，一个项目不需要包含任何代码，而只需要一个pom.xml。 快速概览下面是在POM的项目元素下列出的元素， 注意modelVersion包含4.0.0。 这是目前唯一支持Maven 2和3的POM版本，并且始终是必需的。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- The Basics --&gt; &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;...&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;packaging&gt;...&lt;/packaging&gt; &lt;dependencies&gt;...&lt;/dependencies&gt; &lt;parent&gt;...&lt;/parent&gt; &lt;dependencyManagement&gt;...&lt;/dependencyManagement&gt; &lt;modules&gt;...&lt;/modules&gt; &lt;properties&gt;...&lt;/properties&gt; &lt;!-- Build Settings --&gt; &lt;build&gt;...&lt;/build&gt; &lt;reporting&gt;...&lt;/reporting&gt; &lt;!-- More Project Information --&gt; &lt;name&gt;...&lt;/name&gt; &lt;description&gt;...&lt;/description&gt; &lt;url&gt;...&lt;/url&gt; &lt;inceptionYear&gt;...&lt;/inceptionYear&gt; &lt;licenses&gt;...&lt;/licenses&gt; &lt;organization&gt;...&lt;/organization&gt; &lt;developers&gt;...&lt;/developers&gt; &lt;contributors&gt;...&lt;/contributors&gt; &lt;!-- Environment Settings --&gt; &lt;issueManagement&gt;...&lt;/issueManagement&gt; &lt;ciManagement&gt;...&lt;/ciManagement&gt; &lt;mailingLists&gt;...&lt;/mailingLists&gt; &lt;scm&gt;...&lt;/scm&gt; &lt;prerequisites&gt;...&lt;/prerequisites&gt; &lt;repositories&gt;...&lt;/repositories&gt; &lt;pluginRepositories&gt;...&lt;/pluginRepositories&gt; &lt;distributionManagement&gt;...&lt;/distributionManagement&gt; &lt;profiles&gt;...&lt;/profiles&gt;&lt;/project&gt; 基础POM包含关于项目的所有必要信息，以及在构建过程中使用的插件配置。 12345678910&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-project&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt; Maven坐标groupId：artifactId：version是所有必填字段（如果groupId和version是从父项继承的，则不需要显式定义groupId和version）。 三个字段的作用非常类似于一个地址和时间戳。 这标志着存储库中的特定位置 groupId 定义当前Maven项目隶属的实际项目。首先，Maven项目和实际项目不一定是一对一的关系。比如SpringFrameWork这一实际项目，其对应的Maven项目会有很多，如spring-core,spring-context等。 artifactId : 该元素定义当前实际项目中的一个Maven项目（模块），推荐的做法是使用实际项目名称作为artifactId的前缀 version : 该元素定义了版本 packaging ：定义Maven项目打包的方式，默认jar classifier: 该元素用来帮助定义构建输出的一些附件。附属构件与主构件对应，如上例中的主构件为junit-3.8.1.jar,该项目可能还会通过一些插件生成如junit-3.8.1-javadoc.jar,junit-3.8.1-sources.jar, 这样附属构件也就拥有了自己唯一的坐标。 上述5个元素中，groupId、artifactId、version是必须定义的，packaging是可选的（默认为jar），而classfier是不能直接定义的，需要结合插件使用。 POM关系Maven的一个强大方面是处理项目关系; 其中包括依赖关系（和传递依赖关系），继承和聚合（多模块项目）。 依赖参考maven依赖机制 POM的基石是它的依赖列表。 大多数每个项目都依赖于其他人来构建和正确运行， Maven会为编译和其他它们需要的目标下载和链接依赖关系。 123456789101112131415161718&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.0&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt; ...&lt;/project&gt; 继承1234567891011&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt;&lt;/project&gt; 父级和聚合（多模块）项目需要包装类型为pom。 这些类型定义了绑定到一组生命周期阶段的目标。 例如，如果包装是jar，那么包装阶段将执行jar：jar目标。可以为父POM添加值，该值将由子元素继承。 来自父POM的大多数元素都被继承，其中包括： groupId version description url inceptionYear organization licenses developers contributors mailingLists scm issueManagement ciManagement properties dependencyManagement dependencies repositories pluginRepositories build plugin executions with matching ids plugin configuration etc. reporting profiles 下面元素不会被继承： artifactId name prerequisites 123456789101112131415&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;relativePath&gt;../my-parent&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;my-project&lt;/artifactId&gt;&lt;/project&gt; Super POM与面向对象编程中的对象继承类似，扩展父POM的POM继承了该父对象的某些值。 而且，就像Java对象最终从java.lang.Object继承而来，所有的项目对象模型都是从一个基本的超级POM继承而来的。 下面的代码片段是Maven 3.0.4的Super POM。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;build&gt; &lt;directory&gt;$&#123;project.basedir&#125;/target&lt;/directory&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/classes&lt;/outputDirectory&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;&lt;/finalName&gt; &lt;testOutputDirectory&gt;$&#123;project.build.directory&#125;/test-classes&lt;/testOutputDirectory&gt; &lt;sourceDirectory&gt;$&#123;project.basedir&#125;/src/main/java&lt;/sourceDirectory&gt; &lt;scriptSourceDirectory&gt;src/main/scripts&lt;/scriptSourceDirectory&gt; &lt;testSourceDirectory&gt;$&#123;project.basedir&#125;/src/test/java&lt;/testSourceDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;project.basedir&#125;/src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;directory&gt;$&#123;project.basedir&#125;/src/test/resources&lt;/directory&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;pluginManagement&gt; &lt;!-- NOTE: These plugins will be removed from future versions of the super POM --&gt; &lt;!-- They are kept for the moment as they are very unlikely to conflict with lifecycle mappings (MNG-4453) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.2-beta-5&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-release-plugin&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; &lt;reporting&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/site&lt;/outputDirectory&gt; &lt;/reporting&gt; &lt;profiles&gt; &lt;!-- NOTE: The release profile will be removed from future versions of the super POM --&gt; &lt;profile&gt; &lt;id&gt;release-profile&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;performRelease&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;updateReleaseInfo&gt;true&lt;/updateReleaseInfo&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;/project&gt; mvn help:effective-pom可以查看 除了继承某些顶级元素之外，父母还有元素来配置子POM和传递依赖的值。 其中一个元素是dependencyManagement。 聚合（或多模块）具有模块的项目称为多模块或聚合器项目。 模块是POM列出的项目，并作为一个组执行。 pom打包的项目可以通过将它们列为模块来集合一组项目的构建，这些模块是目录的相对路径或这些项目的POM文件。 1234567891011121314151617&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;my-project&lt;/module&gt; &lt;module&gt;another-project&lt;/module&gt; &lt;module&gt;third-project/pom-example.xml&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; 在列出模块时，您不必考虑模块间依赖关系，即POM给出的模块排序并不重要。 Maven将拓扑排序模块，以便依赖关系始终在依赖模块之前构建。 继承和聚合创建了一个很好的动态来通过单个高级POM控制构建。POM项目可以继承 - 但不一定有 - 它聚合的任何模块。 相反，POM项目可以合并不从其继承的项目。 属性Maven属性是值占位符，使用 ${X}访问 12345678910&lt;project&gt; ... &lt;properties&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;/properties&gt; ...&lt;/project&gt; 所有属性： env.X ${env.PATH} 系统变量 project.x 当前pom的project元素的值 &lt;project&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt; is accessible via ${project.version}. settings.x settings.xml 的值 Java System Properties 比如 ${java.home}. x: Set within a &lt;properties /&gt; element in the POM. 构建设置在Maven的pom.xml文件中，Build相关配置包含两个部分，一个是，另一个是 Build在Maven的pom.xml文件中，存在如下两种：一种被称为Project Build，即是的直接子元素。另一种被称为Profile Build，即是的直接子元素。 Profile Build包含了基本的build元素，而Project Build还包含两个特殊的元素，即各种&lt;…Directory&gt;和。 123456789101112131415&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;!-- "Project Build" contains more elements than just the BaseBuild set --&gt; &lt;build&gt;...&lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;!-- "Profile Build" contains a subset of "Project Build"s elements --&gt; &lt;build&gt;...&lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; Build元素集POM中两个构建元素的基本元素集。 123456789&lt;build&gt; &lt;defaultGoal&gt;install&lt;/defaultGoal&gt; &lt;directory&gt;$&#123;basedir&#125;/target&lt;/directory&gt; &lt;finalName&gt;$&#123;artifactId&#125;-$&#123;version&#125;&lt;/finalName&gt; &lt;filters&gt; &lt;filter&gt;filters/filter1.properties&lt;/filter&gt; &lt;/filters&gt; ...&lt;/build&gt; defaultGoal，执行构建时默认的goal或phase，如jar:jar或者package等 directory，构建的结果所在的路径，默认为${basedir}/target目录 finalName，构建的最终结果的名字，该名字可能在其他plugin中被改变 filter，过滤器，定义* .properties文件，过滤器文件中定义的“name = value”对将替换构建资源中的$ {name}字符串 resources资源往往不是代码，无需编译，而是一些properties或XML配置文件，构建过程中会往往会将资源文件从源路径复制到指定的目标路径。 给出各个资源在Maven项目中的具体路径。示例如下： 1234567891011121314151617181920212223&lt;build&gt; ... &lt;filters&gt; &lt;filter&gt;filters/filter1.properties&lt;/filter&gt; &lt;/filters&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;META-INF/plexus&lt;/targetPath&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/plexus&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;configuration.xml&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; ... &lt;/testResources&gt; ...&lt;/build&gt; 说明： resources，build过程中涉及的资源文件 targetPath，资源文件的目标路径 filtering，构建过程中是否对资源进行过滤，默认false directory，资源文件的路径，默认位于${basedir}/src/main/resources/目录下 includes，一组文件名的匹配模式，被匹配的资源文件将被构建过程处理 excludes，一组文件名的匹配模式，被匹配的资源文件将被构建过程忽略。同时被includes和excludes匹配的资源文件，将被忽略。 filters，给出对资源文件进行过滤的属性文件的路径，默认位于${basedir}/src/main/filters/目录下。属性文件中定义若干键值对。在构建过程中，对于资源文件中出现的变量（键），将使用属性文件中该键对应的值替换。 testResources，test过程中涉及的资源文件，默认位于${basedir}/src/test/resources/目录下。这里的资源文件不会被构建到目标构件中 plugins给出构建过程中所用到的插件。 1234567891011121314151617&lt;build&gt; ... &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;extensions&gt;false&lt;/extensions&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;configuration&gt; &lt;classifier&gt;test&lt;/classifier&gt; &lt;/configuration&gt; &lt;dependencies&gt;...&lt;/dependencies&gt; &lt;executions&gt;...&lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 说明： groupId artifactId version extensions，是否加载该插件的扩展，默认false inherited，该插件的configuration中的配置是否可以被（继承该POM的其他Maven项目）继承，默认true configuration，该插件所需要的特殊配置，在父子项目之间可以覆盖或合并 dependencies，该插件所特有的依赖类库 executions，该插件的某个goal（一个插件中可能包含多个goal）的执行方式。一个execution有如下设置： id，唯一标识 goals，要执行的插件的goal（可以有多个），如run phase，插件的goal要嵌入到Maven的phase中执行，如verify inherited，该execution是否可被子项目继承 configuration，该execution的其他配置参数 pluginManagement在中，与并列，两者之间的关系类似于与之间的关系。中也配置，其配置参数与中的完全一致。只是，往往出现在父项目中，其中配置的往往通用于子项目。子项目中只要在中以声明该插件，该插件的具体配置参数则继承自父项目中对该插件的配置，从而避免在子项目中进行重复配置。 父pom 12345678910111213141516171819202122232425262728293031&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; ... &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;pre-process-classes&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;classifier&gt;pre-process&lt;/classifier&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; ... &lt;/build&gt;&lt;/project&gt; 子pom 12345678910111213141516&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; ... &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; ... &lt;/build&gt;&lt;/project&gt; Project Build特有的&lt;…Directory&gt;往往配置在父项目中，供所有父子项目使用。示例如下： 12345678910&lt;project&gt; &lt;build&gt; &lt;sourceDirectory&gt;$&#123;basedir&#125;/src/main/java&lt;/sourceDirectory&gt; &lt;scriptSourceDirectory&gt;$&#123;basedir&#125;/src/main/scripts&lt;/scriptSourceDirectory&gt; &lt;testSourceDirectory&gt;$&#123;basedir&#125;/src/test/java&lt;/testSourceDirectory&gt; &lt;outputDirectory&gt;$&#123;basedir&#125;/target/classes&lt;/outputDirectory&gt; &lt;testOutputDirectory&gt;$&#123;basedir&#125;/target/test-classes&lt;/testOutputDirectory&gt; ... &lt;/build&gt; &lt;/project&gt; 目录可以使用绝对路径，如示例所示。如果使用相对路径，则所有的相对路径都是在${basedir}目录下。 Project Build特有的是执行构建过程中可能用到的其他工具，在执行构建的过程中被加入到classpath中。 也可以通过激活构建插件，从而改变构建的过程。 通常，通过给出通用插件的一个具体实现，用于构建过程。 的使用示例如下： 1234567891011121314151617&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; ... &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt; &lt;artifactId&gt;wagon-ftp&lt;/artifactId&gt; &lt;version&gt;1.0-alpha-3&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; ... &lt;/build&gt;&lt;/project&gt; Reporting中的配置作用于Maven的site阶段，用于生成报表。中也可以配置插件，并通过一个的为该插件配置参数。注意，对于同时出现在和中的插件，中对该插件的配置也能够在构建过程中生效，即该插件的配置是和中的配置的合并。示例如下： 123456789101112131415161718192021222324&lt;reporting&gt; &lt;excludeDefaults&gt;&lt;/excludeDefaults&gt; &lt;outputDirectory&gt;$&#123;basedir&#125;/target/site&lt;/outputDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-project-info-reports-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;reportSets&gt; &lt;reportSet&gt; &lt;id&gt;sunlink&lt;/id&gt; &lt;reports&gt; &lt;report&gt;javadoc&lt;/report&gt; &lt;/reports&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;configuration&gt; &lt;links&gt; &lt;link&gt;http://java.sun.com/j2se/1.5.0/docs/api/&lt;/link&gt; &lt;/links&gt; &lt;/configuration&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/reporting&gt; 说明： execludeDefaults，执行maven site时，是否生成报表，默认false outputDirectory，报表的生成目录，默认为${basedir}/target/site pulgin，报表中特别用到的插件 artifactId version reportSets，对于该插件的某个goal的执行参数 id reports inherited configuration 其他信息虽然上述信息足以牢牢掌握POM创作，但有更多元素让开发人员的生活更轻松。这些元素中的许多元素都与网站生成有关，但与所有POM声明一样，它们可能用于任何事情，具体取决于某些插件如何使用它。 name description: url inceptionYear Licenses12345678&lt;licenses&gt; &lt;license&gt; &lt;name&gt;Apache License, Version 2.0&lt;/name&gt; &lt;url&gt;https://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt;&lt;/licenses&gt; 许可证是定义如何以及何时使用项目（或项目的一部分）的法律文件。 请注意，项目应仅列出可能直接应用于此项目的许可证，而不列出适用于此项目依存关系的许可证。 Organization大多数项目都是由某种组织（企业，私人团体等）运营的。 这是最基本的信息设置的地方。 12345678910&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;organization&gt; &lt;name&gt;Codehaus Mojo&lt;/name&gt; &lt;url&gt;http://mojo.codehaus.org&lt;/url&gt; &lt;/organization&gt;&lt;/project&gt; Developers虽然组织可能有许多开发人员（程序员）作为成员，但将其全部列为开发人员并不是一种好的方式，但只有那些立即对代码负责的人才能列出。 一个好的经验法则是，如果不应该联系该项目的人员，他们不必在此处列出。 12345678910111213141516171819202122232425&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;developers&gt; &lt;developer&gt; &lt;id&gt;jdoe&lt;/id&gt; &lt;name&gt;John Doe&lt;/name&gt; &lt;email&gt;jdoe@example.com&lt;/email&gt; &lt;url&gt;http://www.example.com/jdoe&lt;/url&gt; &lt;organization&gt;ACME&lt;/organization&gt; &lt;organizationUrl&gt;http://www.example.com&lt;/organizationUrl&gt; &lt;roles&gt; &lt;role&gt;architect&lt;/role&gt; &lt;role&gt;developer&lt;/role&gt; &lt;/roles&gt; &lt;timezone&gt;America/New_York&lt;/timezone&gt; &lt;properties&gt; &lt;picUrl&gt;http://www.example.com/jdoe/pic&lt;/picUrl&gt; &lt;/properties&gt; &lt;/developer&gt; &lt;/developers&gt; ...&lt;/project&gt; Contributors贡献者就像开发者一样，在项目的生命周期中扮演一个辅助角色。 1234567891011121314151617181920212223&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;contributors&gt; &lt;contributor&gt; &lt;name&gt;Noelle&lt;/name&gt; &lt;email&gt;some.name@gmail.com&lt;/email&gt; &lt;url&gt;http://noellemarie.com&lt;/url&gt; &lt;organization&gt;Noelle Marie&lt;/organization&gt; &lt;organizationUrl&gt;http://noellemarie.com&lt;/organizationUrl&gt; &lt;roles&gt; &lt;role&gt;tester&lt;/role&gt; &lt;/roles&gt; &lt;timezone&gt;America/Vancouver&lt;/timezone&gt; &lt;properties&gt; &lt;gtalk&gt;some.name@gmail.com&lt;/gtalk&gt; &lt;/properties&gt; &lt;/contributor&gt; &lt;/contributors&gt; ...&lt;/project&gt; 环境设置问题管理这定义了使用的缺陷跟踪系统（Bugzilla，TestTrack，ClearQuest等）。 虽然没有任何东西阻止插件使用这些信息，但它主要用于生成项目文档。 1234567891011&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;issueManagement&gt; &lt;system&gt;Bugzilla&lt;/system&gt; &lt;url&gt;http://127.0.0.1/bugzilla/&lt;/url&gt; &lt;/issueManagement&gt; ...&lt;/project&gt; 持续集成管理基于触发器或定时（例如，每小时或每天）的持续集成构建系统, 123456789101112131415161718192021&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;ciManagement&gt; &lt;system&gt;continuum&lt;/system&gt; &lt;url&gt;http://127.0.0.1:8080/continuum&lt;/url&gt; &lt;notifiers&gt; &lt;notifier&gt; &lt;type&gt;mail&lt;/type&gt; &lt;sendOnError&gt;true&lt;/sendOnError&gt; &lt;sendOnFailure&gt;true&lt;/sendOnFailure&gt; &lt;sendOnSuccess&gt;false&lt;/sendOnSuccess&gt; &lt;sendOnWarning&gt;false&lt;/sendOnWarning&gt; &lt;configuration&gt;&lt;address&gt;continuum@127.0.0.1&lt;/address&gt;&lt;/configuration&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; ...&lt;/project&gt; 邮件列表邮件列表是与人们保持联系的重要工具。 大多数邮件列表是针对开发人员和用户的 12345678910111213141516171819&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;mailingLists&gt; &lt;mailingList&gt; &lt;name&gt;User List&lt;/name&gt; &lt;subscribe&gt;user-subscribe@127.0.0.1&lt;/subscribe&gt; &lt;unsubscribe&gt;user-unsubscribe@127.0.0.1&lt;/unsubscribe&gt; &lt;post&gt;user@127.0.0.1&lt;/post&gt; &lt;archive&gt;http://127.0.0.1/user/&lt;/archive&gt; &lt;otherArchives&gt; &lt;otherArchive&gt;http://base.google.com/base/1/127.0.0.1&lt;/otherArchive&gt; &lt;/otherArchives&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; ...&lt;/project&gt; SCMSCM（软件配置管理，也称为源代码/控制管理，或简洁的版本控制） 12345678910111213&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;scm&gt; &lt;connection&gt;scm:svn:http://127.0.0.1/svn/my-project&lt;/connection&gt; &lt;developerConnection&gt;scm:svn:https://127.0.0.1/svn/my-project&lt;/developerConnection&gt; &lt;tag&gt;HEAD&lt;/tag&gt; &lt;url&gt;http://127.0.0.1/websvn/my-project&lt;/url&gt; &lt;/scm&gt; ...&lt;/project&gt; 先决条件为了正确执行，POM可能具有某些先决条件。 例如，也许在Maven 2.0.3中有一个修复，您需要使用sftp进行部署。 12345678910&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;prerequisites&gt; &lt;maven&gt;2.0.6&lt;/maven&gt; &lt;/prerequisites&gt; ...&lt;/project&gt; Repositories12345678910111213141516171819202122232425262728&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;repositories&gt; &lt;repository&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; ... &lt;/pluginRepositories&gt; ...&lt;/project&gt; Plugin Repositories存储库是两种主要类型的工件。 第一个是用作其他工件的依赖关系的工件。 这些是驻留在中央的大部分插件。 其他类型的工件是插件。 Maven插件本身就是一种特殊类型的工件。 正因为如此，插件存储库可能会与其他存储库分开 ProfilesPOM 4.0的一项新功能是项目能够根据所建环境来更改设置。 配置文件元素包含可选的激活（配置文件触发器）以及如果该配置文件已激活，将对POM进行的一组更改。 例如，为测试环境构建的项目可能会指向与最终部署不同的数据库。 或者根据所使用的JDK版本，可能会从不同的存储库中提取依赖关系。 配置文件的元素如下所示： 1234567891011121314151617181920&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;activation&gt;...&lt;/activation&gt; &lt;build&gt;...&lt;/build&gt; &lt;modules&gt;...&lt;/modules&gt; &lt;repositories&gt;...&lt;/repositories&gt; &lt;pluginRepositories&gt;...&lt;/pluginRepositories&gt; &lt;dependencies&gt;...&lt;/dependencies&gt; &lt;reporting&gt;...&lt;/reporting&gt; &lt;dependencyManagement&gt;...&lt;/dependencyManagement&gt; &lt;distributionManagement&gt;...&lt;/distributionManagement&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 激活激活是配置文件的关键。只能在特定情况下修改基本POM的能力。 这些情况通过激活元素指定。 123456789101112131415161718192021222324252627282930&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;jdk&gt;1.5&lt;/jdk&gt; &lt;os&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;property&gt; &lt;name&gt;sparrow-type&lt;/name&gt; &lt;value&gt;African&lt;/value&gt; &lt;/property&gt; &lt;file&gt; &lt;exists&gt;$&#123;basedir&#125;/file2.properties&lt;/exists&gt; &lt;missing&gt;$&#123;basedir&#125;/file1.properties&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; ... &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt;]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6 maven生命周期]]></title>
    <url>%2F2018%2F06%2F16%2Fmaven%2F6%20maven%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[官方的生命周期解释https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html 构建生命周期基础Maven基于构建生命周期的中心概念。这意味着构建和分发特定工件（项目）的过程被明确定义。 对于构建项目的人员，这意味着只需要学习一小堆命令即可构建任何Maven项目，POM将确保他们获得所需的结果。 有三个内置的生命周期：默认（default），清洁（clean）和站点（site）。在默认（default）的生命周期处理你的项目部署，将清洁（clean）的生命周期处理项目的清理，而网站（site）的生命周期处理你的项目站点文档的创建。 Clean Lifecycle在进行真正的构建之前进行一些清理工作。 Default Lifecycle构建的核心部分，编译，测试，打包，部署等等。 Site Lifecycle生成项目报告，站点，发布站点。 每个生命周期包含一些阶段，这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段，用户和Maven最直接的交互方式就是调用这些生命周期阶段。 较之于生命周期阶段的前后依赖关系，三套生命周期本身是相互独立的，用户可以仅仅调用clean生命周期的某个阶段，或者仅仅调用default生命周期的某个阶段，而不会对其他生命周期产生任何影响。 构建生命周期是由阶段组成的生命周期由构建阶段的不同列表定义，其中构建阶段表示生命周期中的阶段。 默认（default）的生命周期包括以下阶段（注意：这里是简化的阶段，用于生命周期阶段的完整列表，请参阅下方生命周期参考）： 验证（validate） - 验证项目是否正确，所有必要的信息可用 编译（compile） - 编译项目的源代码 测试（test） - 使用合适的单元测试框架测试编译的源代码。这些测试不应该要求代码被打包或部署 打包（package） - 采用编译的代码，并以其可分配格式（如JAR）进行打包。 验证（verify） - 对集成测试的结果执行任何检查，以确保满足质量标准 安装（install） - 将软件包安装到本地存储库中，用作本地其他项目的依赖项 部署（deploy） - 在构建环境中完成，将最终的包复制到远程存储库以与其他开发人员和项目共享。 这些生命周期阶段（以及此处未显示的其他生命周期阶段）依次执行，以完成默认生命周期。给定上述生命周期阶段，这意味着当使用默认生命周期时，Maven将首先验证项目，然后尝试编译源代码，运行这些源代码，打包二进制文件（例如jar），运行集成测试软件包，验证集成测试，将验证的软件包安装到本地存储库，然后将安装的软件包部署到远程存储库。 换句话说，在生命周期里面阶段是连续的，在不出错的前提下，比如执行打包（package）时就一定是执行了测试（test）之后再执行。 生命周期参考以下列出了默认（default），清洁（clean）和站点（site）生命周期的所有构建阶段，它们按照指定的顺序执行的顺序执行。 清洁（clean）生命周期 预清洁（pre-clean） 执行实际项目清理之前所需的流程 清洁（clean） 删除以前构建生成的所有文件 后清洁（post-clean） 执行完成项目清理所需的流程 默认（default）生命周期 验证（validate） 验证项目是正确的，所有必要的信息可用。 初始化（initialize） 初始化构建状态，例如设置属性或创建目录。 产生来源（generate-sources） 生成包含在编译中的任何源代码。 流程源（process-sources） 处理源代码，例如过滤任何值。 生成资源（generate-resources） 生成包含在包中的资源。 流程资源（process-resources） 将资源复制并处理到目标目录中，准备打包。 编译（compile） 编译项目的源代码。 工艺类（process-classes） 从编译后处理生成的文件，例如对Java类进行字节码增强。 生成测试来源（generate-test-sources） 生成包含在编译中的任何测试源代码。 流程测试来源（process-test-sources） 处理测试源代码，例如过滤任何值。 生成测试资源（generate-test-resources） 创建测试资源。 流程测试资源（process-test-resources） 将资源复制并处理到测试目标目录中。 测试编译（test-compile） 将测试源代码编译到测试目标目录中 流程检验类（process-test-classes） 从测试编译中处理生成的文件，例如对Java类进行字节码增强。对于Maven 2.0.5及以上版本。 测试（test） 使用合适的单元测试框架运行测试。这些测试不应该要求代码被打包或部署。 制备包（prepare-package） 在实际包装之前，执行必要的准备包装的操作。这通常会导致打包的处理版本的包。（Maven 2.1及以上） 打包（package） 采取编译的代码，并以其可分发的格式（如JAR）进行打包。 预集成测试（pre-integration-test） 在执行集成测试之前执行所需的操作。这可能涉及诸如设置所需环境等。 集成测试（integration-test） 如果需要，可以将该包过程并部署到可以运行集成测试的环境中。 整合后的测试（post-integration-test） 执行集成测试后执行所需的操作。这可能包括清理环境。 校验（verify） 运行任何检查以验证包装是否有效并符合质量标准。 安装（install） 将软件包安装到本地存储库中，以作为本地其他项目的依赖关系。 部署（deploy） 在集成或发布环境中完成，将最终软件包复制到远程存储库，以与其他开发人员和项目共享。 站点（site）生命周期 预网站（pre-site） 在实际的项目现场生成之前执行所需的进程 网站（site） 生成项目的站点文档 后网站（post-site） 执行完成站点生成所需的进程，并准备站点部署 网站部署（site-deploy） 将生成的站点文档部署到指定的Web服务器 生命周期阶段在命令行中的调用在开发环境中，使用以下调用构建并将工件安装到本地存储库中。 1mvn install 此命令在执行安装之前按顺序（验证（validate），编译（compile），打包（package）等）执行每个默认生命周期阶段。在这种情况下，您只需要调用最后一个构建阶段来执行，安装（install）。 在构建环境中，使用以下调用将工件清理地构建并部署到共享存储库中。 1mvn clean deploy 相同的命令可以在多模块场景（即具有一个或多个子项目的项目）中使用。Maven遍历每个子项目并执行清洁（clean），然后执行部署（deploy）（包括所有之前的构建阶段步骤）。 注意：在我们开发阶段，有一些生命周期的阶段，比如验证（validate）这些，基本很少用到。只要使用关键的几个基本能满足需求。 通常情况在命令行只调用某些特定的阶段 以连字符（pre-，post-或process-）命名的阶段通常不会从命令行直接调用。在生命周期的阶段上，只有特定的几个阶段对于构建有意义。一些无用的阶段只起到了中间阶段的作用，换句话说只是一个过客。 调用由插件目标（Plugin Goals）组成的构建阶段即使构建阶段负责构建生命周期中的特定步骤，其执行这些职责的方式可能会有所不同。这是通过声明绑定到这些构建阶段的插件目标来完成的。 插件目标代表一个特定的任务（比构建阶段更精细），有助于项目的构建和管理。它可能被限制在零个或多个构建阶段。不限于任何构建阶段的目标可以通过直接调用在构建生命周期之外执行。执行顺序取决于调用目标和构建阶段的顺序。例如，考虑下面的命令。该清洁（clean）和打包（package）是构建阶段，而dependency:copy-dependencies是一个插件的目标。 1mvn clean dependency:copy-dependencies package 如果要执行此操作，则将首先执行清洁（clean）阶段，然后执行dependency:copy-dependencies目标，然后才能最终执行打包（package）阶段（以及默认生命周期的所有之前的构建阶段）。 而且，如果一个目标被绑定到一个或者多个构建阶段，那么在所有这些阶段都会调用这个目标。 此外，构建阶段也可以有零个或多个目标。如果构建阶段没有绑定目标，则构建阶段将不会执行。但是，如果它有一个或多个目标，它将执行所有这些目标 提示：其实简单点理解就是说dependency是一个插件，在我们执行生命周期阶段时，可以调用这个插件做特定的事，其中copy-dependencies就是特定的事，那么上面的命令可以这么理解，在执行clean后就会执行dependecy这个插件，最后再执行package；如果dependecy这个插件执行过程异常，package就不会执行到。一个命令可以有多个插件，也可以一个插件都没有。 使用构建生命周期来设置项目构建生命周期足够简单，但是当您为项目配置Maven构建时，您如何将任务分配到每个构建阶段？ 打包第一个也是最常见的方法是通过同样命名的POM元素为您的项目设置打包。一些有效的打包值是jar，war，ear和pom。如果没有指定包装值，它将默认为jar。每个不同类型的打包都包含要绑定到特定阶段的目标列表。例如，jar包将绑定以下目标来构建默认生命周期的阶段。 流程资源（process-resources） resources:resources 编译（compile） compiler:compile 流程测试资源（process-test-resources） resources:testResources 测试编译（test-compile） compiler:testCompile 测试（test） surefire:test 打包（package） jar:jar 安装（install） install:install 部署（deploy） deploy:deploy 这里简单点可以说不同的包对应不同的生命周期阶段，比如jar包和war包的区别可以参考：https://maven.apache.org/ref/3.5.0/maven-core/default-bindings.html。上方列表可以这么理解，左边是简化的命令，右侧是详细的插件加目标（命令行参数）的形式；切记，Maven都是以插件的形式存在的，包括生命周期的阶段同样也是一个个不同的插件组成，比如上面的编译（compile）就是由compiler插件提供，其中compile为这个插件的目标，也可以说是插件的命令行参数。 插件将目标添加到阶段的第二种方法是在项目中配置插件。插件是为Maven提供目标的工件。此外，插件可以具有一个或多个目标，其中每个目标代表该插件的能力。例如，编译器（compiler）插件有两个目标：compile和testCompile。前者编译主代码的源代码，后者编译测试代码的源代码。 插件可以包含指示将目标绑定到的生命周期阶段的信息。请注意，自己添加插件是不够的，您还必须指定要作为构建的一部分运行的目标。 配置的目标将被添加到已经从选定的打包绑定到生命周期的目标。如果将多个目标绑定到特定阶段，则使用的顺序是首先执行来自打包装的顺序，然后执行在POM中配置。请注意，您可以使用元素来获得对特定目标的顺序更多的控制。 例如，Modello插件默认将目标modello:java绑定到generate-sources阶段（注意：modello:java目标生成Java源代码）。因此，要使用Modello插件，并从模型生成源代码并将其合并到构建中，您可以在的部分中将以下内容添加到POM中： 1234567891011121314151617181920... &lt;plugin&gt; &lt;groupId&gt;org.codehaus.modello&lt;/groupId&gt; &lt;artifactId&gt;modello-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.8.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;configuration&gt; &lt;models&gt; &lt;model&gt;src/main/mdo/maven.mdo&lt;/model&gt; &lt;/models&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;/configuration&gt; &lt;goals&gt; &lt;goal&gt;java&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;... Modello插件有默认的生命周期阶段，而无需自己手动配置这些阶段。 元素作用：可以使用不同的配置多次运行相同的目标。还可以使用单独的执行标识，以便在继承或应用配置文件期间，您可以控制目标配置是合并还是转为额外的执行。 在modello:java的情况下，它只在generate-sources阶段才有意义。 可以自己指定阶段。例如，假设您有一个目标display:time当前时间到命令行的时间，并希望它在process-test-resources阶段运行以指示测试何时开始。这将被配置如下： 123456789101112131415... &lt;plugin&gt; &lt;groupId&gt;com.mycompany.example&lt;/groupId&gt; &lt;artifactId&gt;display-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;process-test-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;time&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;... 内置生命周期的绑定根据https://maven.apache.org/ref/3.5.0/maven-core/default-bindings.html提供的不同类型对应不同的生命周期阶段 默认情况下，某些阶段的目标与之相关。对于默认生命周期，这些绑定取决于包装值。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming 数据计算阶段分析]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2FSpark%20Streaming%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E9%98%B6%E6%AE%B5%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Spark Streaming 数据计算阶段分析SparkStreaming的全过程分为两个阶段：数据准备阶段和数据计算阶段。两个阶段在功能上相互独立，仅通过数据联系在一起。 Spark Streaming数据计算阶段包含批次数据划分，批作业生成，批wt提交三个部分。 JobGenerator 启动JobGenerator用于定期生成Job并进行提交 。在启动JobScheduler时，其会调用JobGenerator的start方法，启动JobGenerator.JobGenerator的start方法实现如下： 123456789101112131415/** Start generation of jobs */ def start(): Unit = synchronized &#123; eventLoop = new EventLoop[JobGeneratorEvent]("JobGenerator") &#123; override protected def onReceive(event: JobGeneratorEvent): Unit = processEvent(event) override protected def onError(e: Throwable): Unit = &#123; jobScheduler.reportError("Error in job generator", e) &#125; &#125; eventLoop.start() if (ssc.isCheckpointPresent) &#123; restart() &#125; else &#123; startFirstTime() &#125; &#125; 在JobGenerator.start()被调用时，其将创建 eventLoop对象并启动，其中eventLoop定义事件交由processEvent(event). processEvent其依据事件的类型，对其进行不同的处理。 调用startFirstTime（）方法。其进行两项主要工作： 调用 timer.start方法、 定期生成Job 调用graph.start方法 Job 生成及提交周期性触发Job生成事件startFirstTime()方法中调用了timer.start方法，其中timer[RecurringTimer]为定时器，与Spark Streaming 数据准备阶段分析一文中介绍切片时所有定时器一样。其按设置的时间周期，重复的执行计划的任务。 1private val timer = new RecurringTimer(clock, ssc.graph.batchDuration.milliseconds,longTime =&gt; eventLoop.post(GenerateJobs(new Time(longTime))), "JobGenerator") 其每个batchDuration规定时间，都会向eventLoop发送一GenerateJobs事件，eventLoop收到GenerateJobs事件，则使用processEvent进行相应处理，此处为调用 generateJobs()方法 ,生成job. 12345678910private def processEvent(event: JobGeneratorEvent) &#123; logDebug("Got event " + event) event match &#123; case GenerateJobs(time) =&gt; generateJobs(time) case ClearMetadata(time) =&gt; clearMetadata(time) case DoCheckpoint(time, clearCheckpointDataLater) =&gt; doCheckpoint(time, clearCheckpointDataLater) case ClearCheckpointData(time) =&gt; clearCheckpointData(time) &#125; &#125; Job生成详细过程Timer分周期性的触发Job生成事件，并通过generateJobs来生成Job. JobGenerator在每个Batch Interval都会为应用中的每个Output Stream建立一个Job, 该批次中的所有Job组成一个JobSet.使用JobScheduler的submitJobSet进行批量Job提交。 在generateJobs生成Job时， 其首先通过ReceiverTracker 取得其中注册的未分配的数据信息。然后通过DStreamGraph生成Job。 批数据信息划分在生成Job时，首先调用如下语句： 1jobScheduler.receiverTracker.allocateBlocksToBatch(time) 该语句用来划分某批次(time)要处理的数据。 123456/** Allocate all unallocated blocks to the given batch. */def allocateBlocksToBatch(batchTime: Time): Unit = &#123; if (receiverInputStreams.nonEmpty) &#123; receivedBlockTracker.allocateBlocksToBatch(batchTime) &#125;&#125; 其将调用receivedBlockTracker的allocateBlocksToBatch方法，将未分配数据信息取出，并划分给batchTime所指批次。首先receivedBlockTracker从streamIdToUnallocatedBlockQueues中取出未分配的block信息，将其包装为AllocatedBlocks，并注册在timeToAllocatedBlocks表中，等待某批次（batchTime）生成Job时，与Job进行绑定。 批作业（Job）生成通过graph.generateJobs(time)方法分别将DStreamGraph中的每个OutputStream转换了一个Job(如果应用中有多个OutputStream算子，则一个批次会生成多个Job)。 RDD 生成 将划分过批次的数据信息（blockInfos)取出，包装成StreamInputInfo，然后通过createBlockRDD方法生成RDD. 此处，如果blockInfos信息不空，则生成正常的RDD；若blockInfos为空，则没有Block的空RDD（new BlockRDD(ssc.sc, Array.empty)）。作者：barrenlake_陈诚链接：https://www.jianshu.com/p/c6296162deb7來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Job 的提交当成功转化为Job之后，然后通过JobScheduler对JobSet进行提交。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming Executor DynamicAllocation 机制分析]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2FSpark%20Streaming%20Executor%20DynamicAllocation%20%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Spark Streaming Executor DynamicAllocation 机制分析Spark Streaming Executor DynamicAllocation 机制分析在Spark Streaming，作业的执行是以批处理的方式进行的，批处理间隔内(batch interval)要完成对批量作业的执行，这就要求==作业的执行时间（process time）不大于设定的批处理间隔==。在计算资源一定的情况下，执行时间与待处理的数据规模成正比。在大数据流式计算环境中，数据的产生完全由数据源决定,由于不同的数据源在不同时空范围内的状态不统一且发生动态变化，导致数据流的速率呈现出了突发性的特征。前一时刻数据速率和后一时刻数据速率可能会有巨大的差异， 因此不同批次接收的数据总量存在差异，不同批次的执行时间也就会存在差异。这种差异是由于计算资源与数据规模不匹配造成的。==在Spark 2.0以前， Streaming的资源分配都采取资源预先分配的策略，资源管理器依据应用的申请量予以提前分配，在应用执行期间不能依据应用实际的计算资源需求量进行调整，执行作业时会存在资源过剩或资源不足的情况。== Spark Streaming 的计算特征Spark Streaming应用的执行过程可以分成数据准备和数据计算两个阶段，两个阶段分别由不同的作业进行处理，分别是数据接收作业（提交Receiver时的Job）和数据处理作业(Batch Job)。在大数据流式计算中，数据是实时产生、动态增加的，只要数据源处于活动状态,数据就会一直产生，因此要求数据接收作业不间断的接收数据，并将接收的数据分割成批，供数据处理作业消费。Spark Streaming的计算特征如下图所示（实际处理时间&lt;批处理间隔）: Streaming Executor DynamicAllocation机制的评价指标作业的实际处理时间与设定的批处理间隔之间存在的关系如下： 实际处理时间 远小于 批处理间隔此时，计算资源大部分时间处于空闲，造成资源浪费 作业的实际处理时间 约等于 批处理间隔此时，能满足处理，但数据具有波动性，可能下一个时间不能满足，造成延迟。计算资源初显不足。 介于以上两者之间系统资源能正常满足计算，又不至于造成过多浪费，此时系统稳定性良好。 作业的实际处理时间 大于 批处理间隔批处理间隔内不能完成批作业，说明计算资源不足。 因此，采用 有效处理时间占比【有效处理时间占比 = 实际处理时间 / 批处理间隔】来评价当前计算资源的过剩或不足。 Streaming Executor DynamicAllocation机制的工作流程 通过监控组件，获取已完成批作业的实际执行时间 计算有效处理时间占比，然后与设置的阈值进行比较，如果小于下限down,则结束一个Executor; 如果大于上限up， 则增加Executor(个数由比率决定). 参数spark.streaming.dynamicAllocation.enabled参数决定，默认为false不开启。 分析与Spark Streaming Backpressure机制相同，Spark Streaming Executor动态伸缩机制也是以事件驱动的形式工作的。其负责动态伸缩的类为ExecutorAllocationManager。其通过分析Streaming作业的监控信息，动态的伸请或释放executor. ExecutorAllocationManager 注册与启动JobScheduler启动时会创建ExecutorAllocationManager 并向ListenerBus注册并开启监听。 其将开启定时器，周期性的执行manageAllocation方法，其时间周期由参数 “spark.streaming.dynamicAllocation.scalingInterval”决定，默认为60s. 即：默认情况下每隔60s 执行一次manageAllocation. managerAllocation会使用历史作业执行信息计算出有效处理时间占比ratio, 并依据占比与预设阈值的关系决定增减资源。预设的阈值信息及增减策略为： scalingUpRatio阈值上限，由参数“park.streaming.dynamicAllocation.scalingUpRatio”控制，默认值为0.9。当计算出的ratio大于scalingUpRatio 时，将按如下算式计算出的值，增加若干Executor。 scalingDownRatio阈值下限， 由参数“spark.streaming.dynamicAllocation.scalingDownRatio”，默认值为0.3。 当计算出的ratio小于scalingDownRatio时，将减少一个Executor. 事件触发Executor dynamicAllocation机制是依据完成的批Job的执行信息进行决策，其在批Job执行完成时会收集作业。其事件触发过程，同SparkStreaming Backpressure分析 BatchCompleted触发过程 与Spark Core中Executor DynamicAllocation 机制的区别Spark Core 中的DynamicAllocation机制同Spark Streaming中一样，也是周期性调度评估机制确定是否要进行Executor增减。不同的是，Spark Core中应用是批处理应用，执行完成之后，即可以结束，因此其使用idle策略来评估Executor的增减，具体策略为： 减少Executor应用拥有的Executor数量在处理当前负载时绰绰有余，通过缩减Executor仍然能够一次性执行所有任务（running + pending task）时，会通过一定的策略结束掉部分Executor，达到节省资源的目的。Spark Core中的策略为如果监测到一个Executor空闲了K 秒，这意味着其在未来不再执行任务，则可以将其移除。其中参数K由“spark.dynamicAllocation.executorIdleTimeout ”配制，默认值为60s. 增加Executor应用拥有的Executor数量不足以及时处理当前负载，存在任务长时间堆集，则要增加Executor。Spark Core中的策略为如果监测到任务调度队列中的任务N秒内没有被调度，则会增加新的Executor，如果再过M秒还未进行调度，则以指数方式继续增加，直到上限。 由于Spark Streaming应用是长期存在的、微批处理应用。其每隔一个小的时间间隔(batchInterval)就会提交微批处理应用。其会使Spark Core中的Executor DynamicAllocation 机制的idle 策略受阻，因此Spark Streaming 采用有效处理时间占比radio来进行决策。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming Receiver启动过程]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2FSpark%20Streaming%20Receiver%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Spark Streaming Receiver启动过程Receiver是数据准备阶段的一个主要组件，其负载接入外部数据，其生命周期由ReceiverTracker负责管理。 Receiver的启动Receiver抽取与Executor准备Spark Streaming 初始化过程中提到 JobScheduler在启动时会创建和启动ReceiverTracker. 在ReceiverTracker创建时，其会从DStreamGraph中抽取出ReceiverInputStream，以便在启动Receiver时从中抽取出Receiver,然后一一启动。 JobScheduler是Spark Streaming中核心的组件，在其开始执行时，会开启数据接收相关组件及Job生成相关组件，从而使数据准备和数据计算两个流程开始工作。 1private val receiverInputStreams = ssc.graph.getReceiverInputStreams() 在ReceiverTracker启动时，其主要做如下两件事： 创建ReceiverTrackerEndpoint，用于接收Receiver的信息 启动Receiver. ReceiverTracker的Start方法如下所示： 12345678def start(): Unit = synchronized &#123; if (!receiverInputStreams.isEmpty) &#123; endpoint = ssc.env.rpcEnv.setupEndpoint( "ReceiverTracker", new ReceiverTrackerEndpoint(ssc.env.rpcEnv)) if (!skipReceiverLaunch) launchReceivers() trackerState = Started &#125; &#125; 其中 launchReceivers（）方法用于启动Receiver 12345678910private def launchReceivers(): Unit = &#123; val receivers = receiverInputStreams.map &#123; nis =&gt; val rcvr = nis.getReceiver() rcvr.setReceiverId(nis.id) rcvr &#125; runDummySparkJob() // 发送启动指令 endpoint.send(StartAllReceivers(receivers)) &#125; 从ReceiverInputStreams中抽取Receiver, 并将streamId做为Receiver的id. 执行runDummySparkJob，此方法是执行一个简单的SparkJob，目的是为确保应用申请的Executor的最小份额得以满足，最小份额由参数“spark.cores.max” 和 “spark.scheduler.minRegisteredResourcesRatio” 共同决定，默认为申请的所有Executor。当应用已获得的Executor数量小于最小份额时，Job将阻塞并等待Executor注册，直到满足其运行需要的最小限额。 向ReceiverTrackerEndpoint发送启动所有executor指令（StartAllReceivers） 在ReceiverTrackerEndpoint收到StartAllReceivers指令后，其将 调度Receiver： 为Receiver设置执行位置信息 启动Receiver Receiver 调度Receiver调度工作由ReceiverSchedulingPolicy进行，对Receiver的调度工作主要可以分为如下两个阶段： 全局调度阶段此阶段发生在首次调度Receiver时，此阶段会保证receivers尽量均匀的分散在Executors中。调度过程中会为每一个Receiver指定启动的位置信息（location） 局部调度阶段此阶段发生在Receiver重启时，仅需启动失败Receiver 全局调度阶段 获取所有executor的主要地址信息 创建numReceiversOnExecutor用于记录每个Executor分配的Receiver数目 创建scheduledLocations用于记录用户指定偏好位置的Receiver 调度指定preferredLocation信息的Receiver. 遍历Receivers, 为用户指定的preferredLocation的主机中选择启动Receiver数 最少的Executor做为当前Receiver启动位置，并更新记录scheduledLocations 和numReceiversOnExecutor。 调度未指定preferredLocation信息的Receiver.将Executor依照分配的Receiver数目从小到大排序，为Receiver分配一个Executor. 若还有剩余Executor, 将这些Executor 加入到拥有最少候选对象的Receiver列表中。 Receiver 启动在为Receiver设置完启动位置之后，将调用startReceiver方法启动Receiver 依据preferredLocation将Receiver包装成RDD 以SparkJob的形式提交作业, Receiver作为Task 以线程方式执行 12345678910111213141516val future = ssc.sparkContext.submitJob[Receiver[_], Unit, Unit]( receiverRDD, startReceiverFunc, Seq(0), (_, _) =&gt; Unit, ()) future.onComplete &#123; case Success(_) =&gt; if (!shouldStartReceiver) &#123; onReceiverJobFinish(receiverId) &#125; else &#123; self.send(RestartReceiver(receiver)) &#125; case Failure(e) =&gt; if (!shouldStartReceiver) &#123; onReceiverJobFinish(receiverId) &#125; else &#123; self.send(RestartReceiver(receiver)) &#125; &#125;(ThreadUtils.sameThread) Task执行， 执行的startReceiverFunc方法，该方法会创建并启动ReceiverSupervisorImpl（Job及Task调度过程此处不再详细说明，同批处理） 其中ReceiverSupervisorImpl 提供了处理Receiver接收数据的所有必要的方法。并且它还创建了BlockGenerator，用于对Receiver接收的数据流进行切片操作。 ​]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark stream介绍]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2Fspark%20stream%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[spark stream简介流式计算的实现方案有如下两种模型： 连续算子模型(continuous operator model)，连续算子模型中数据流被表达成算子图。数据被一个算子处理后，发送给另一个算子做进一步处理,如Storm, S4等; 批量流式计算模型，这类模型将流入的数据流按固定时间片分割为一系列数据块，并按时间窗口将流式计算分解成一连串短小的批处理作业，各批处理作业处理落在其时间窗口内的数据集合。 概述Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库。还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。 Spark Streaming处理的数据流图： Spark的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。 对应的批数据，在Spark内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何==协调生产速率和消费速率==。 术语定义 离散流（discretized stream）或DStream：Spark Streaming对内部持续的实时数据流的抽象描述，即我们处理的一个实时数据流，在Spark Streaming中对应于一个DStream 实例。 批数据（batch data）：将实时流数据以时间片为单位进行分批，将流处理转化为时间片数据的批处理。随着持续时间的推移，这些处理结果就形成了对应的结果数据流了。 时间片或批处理时间间隔（ batch interval）：以时间片作为我们拆分流数据的依据。一个时间片的数据对应一个RDD实例。 窗口长度（window length）：一个窗口覆盖的流数据的时间长度。必须是批处理时间间隔的倍数， 滑动时间间隔：前一个窗口到后一个窗口所经过的时间长度。必须是批处理时间间隔的倍数 Input DStream :一个input DStream是一个特殊的DStream，将Spark Streaming连接到一个外部数据源来读取数据。 Storm与Spark Streming比较处理模型以及延迟 虽然两框架都提供了可扩展性(scalability)和可容错性(fault tolerance)，但是它们的处理模型从根本上说是不一样的。Storm可以实现==亚秒级时延==的处理，而每次只处理一条event，而Spark Streaming可以在一个==短暂的时间窗口==里面处理多条(batches)Event。所以说==Storm可以实现亚秒级时延的处理，而Spark Streaming则有一定的时延。== 容错和数据保证 Spark Streaming的容错为有状态的计算提供了更好的支持。==在Storm中，每条记录在系统的移动过程中都需要被标记跟踪==，所以Storm只能保证每条记录最少被处理一次，但是允许从错误状态恢复时被处理多次。这就意味着可变更的状态可能被更新两次从而导致结果不正确。 任一方面，==Spark Streaming仅仅需要在批处理级别对记录进行追踪，所以他能保证每个批处理记录仅仅被处理一次==，即使是node节点挂掉。虽然说Storm的 Trident library可以保证一条记录被处理一次，但是它依赖于事务更新状态，而这个过程是很慢的，并且需要由用户去实现。 编程API Storm提供了Java API，同时也支持其他语言的API。 Spark Streaming支持Scala和Java语言(其实也支持Python)。 批处理框架集成 Spark Streaming的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写Spark Streaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。 运行原理Streaming架构计算流程：Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。 容错性对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。 对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。 实时性：对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其==最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右）==，所以Spark Streaming能够满足除对实时性要求非常高（如高频实时交易）之外的所有流式准实时计算场景。 扩展性与吞吐量Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍，图4是Berkeley利用WordCount和Grep两个用例所做的测试，在Grep这个测试中，Spark Streaming中的每个节点的吞吐量是670k records/s，而Storm是115k records/s。 编程模型DStream（Discretized Stream）作为Spark Streaming的基础抽象，它代表持续性的数据流。这些数据流既可以通过外部输入源赖获取，也可以通过现有的Dstream的transformation操作来获得。在内部实现上，DStream由一组时间序列上连续的RDD来表示。每个RDD都包含了自己特定时间间隔内的数据流。 使用Spark Streaming123456789101112131415161718192021import org.apache.spark._import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._// Create a local StreamingContext with two working thread and batch interval of 1 second.// The master requires 2 cores to prevent from a starvation scenario.val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")val ssc = new StreamingContext(conf, Seconds(1))val lines = ssc.socketTextStream("localhost", 9999)val words = lines.flatMap(_.split(" "))val pairs = words.map(word =&gt; (word, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.print()ssc.start()ssc.awaitTermination() 创建StreamingContext对象使用Spark Streaming就需要创建StreamingContext对象。创建StreamingContext对象所需的参数与SparkContext基本一致，包括指明Master，设定名称(如NetworkWordCount)。参数Seconds(1)，Spark Streaming需要指定处理数据的时间间隔，如上例所示的1s，那么Spark Streaming会以1s为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置； 创建InputDStream 如同Storm的Spout，Spark Streaming需要指明数据源。如上例所示的socketTextStream，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括Kafka、 Flume、HDFS/S3、Kinesis和Twitter等数据源； 操作DStream对于从数据源得到的DStream，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的WordCount执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用Map和ReduceByKey方法进行计算，当然最后还有使用print()方法输出结果； 启动Spark Streaming之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当ssc.start()启动后程序才真正进行所有预期的操作。 DStream的输入源在Spark Streaming中所有的操作都是基于流的，而输入源是这一系列操作的起点。输入 DStreams 和 DStreams 接收的流都代表输入数据流的来源，在Spark Streaming 提供两种内置数据流来源： 基础来源 在 StreamingContext API 中直接可用的来源。例如：文件系统、Socket（套接字）连接和 Akka actors； 高级来源 如 Kafka、Flume、Kinesis、Twitter 等，可以通过额外的实用工具类创建。 基础来源使用Spark Streaming的例子中我们已看到ssc.socketTextStream()方法，可以通过 TCP 套接字连接 Spark Streaming提供了streamingContext.fileStream(dataDirectory)方法可以从任何文件系统(如：HDFS、S3、NFS 等）的文件中读取数据，然后创建一个DStream。Spark Streaming 监控 dataDirectory 目录和在该目录下任何文件被创建处理(不支持在嵌套目录下写文件)。 Spark Streaming也可以基于自定义 Actors 的流创建DStream ，通过 Akka actors 接受数据流，使用方法streamingContext.actorStream(actorProps, actor-name)。Spark Streaming使用 streamingContext.queueStream(queueOfRDDs)方法可以创建基于 RDD 队列的DStream，每个RDD 队列将被视为 DStream 中一块数据流进行加工处理。 高级来源这一类的来源需要外部 non-Spark 库的接口，其中一些有复杂的依赖关系(如 Kafka、Flume)。因此通过这些来源创建 DStreams 需要明确其依赖。例如，如果想创建一个使用 Twitter tweets 的数据的DStream 流，必须按以下步骤来做： 1）在 SBT 或 Maven工程里添加 spark-streaming-twitter_2.10 依赖。 2）开发：导入 TwitterUtils 包，通过 TwitterUtils.createStream 方法创建一个DStream。 3）部署：添加所有依赖的 jar 包(包括依赖的spark-streaming-twitter_2.10 及其依赖)，然后部署应用程序。 DStream的操作与RDD类似，DStream也提供了自己的一系列操作方法，这些操作可以分成三类：普通的转换操作、窗口转换操作和输出操作。 普通的转换操作普通的转换操作如下表所示： 转换 描述 map(func) 源 DStream的每个元素通过函数func返回一个新的DStream。 flatMap(func) 类似与map操作，不同的是每个输入元素可以被映射出0或者更多的输出元素。 filter(func) 在源DSTREAM上选择Func函数返回仅为true的元素,最终返回一个新的DSTREAM 。 repartition(numPartitions) 通过输入的参数numPartitions的值来改变DStream的分区大小。 union(otherStream) 返回一个包含源DStream与其他 DStream的元素合并后的新DSTREAM。 count() 对源DStream内部的所含有的RDD的元素数量进行计数，返回一个内部的RDD只包含一个元素的DStreaam。 reduce(func) 使用函数func（有两个参数并返回一个结果）将源DStream 中每个RDD的元素进行聚 合操作,返回一个内部所包含的RDD只有一个元素的新DStream。 countByValue() 计算DStream中每个RDD内的元素出现的频次并返回新的DStream[(K,Long)]，其中K是RDD中元素的类型，Long是元素出现的频次。 reduceByKey(func, [numTasks]) 当一个类型为（K，V）键值对的DStream被调用的时候,返回类型为类型为（K，V）键值对的新 DStream,其中每个键的值V都是使用聚合函数func汇总。注意：默认情况下，使用 Spark的默认并行度提交任务（本地模式下并行度为2，集群模式下位8），可以通过配置numTasks设置不同的并行任务数。 join(otherStream, [numTasks]) 当被调用类型分别为（K，V）和（K，W）键值对的2个DStream时，返回类型为（K，（V，W））键值对的一个新 DSTREAM。 cogroup(otherStream, [numTasks]) 当被调用的两个DStream分别含有(K, V) 和(K, W)键值对时,返回一个(K, Seq[V], Seq[W])类型的新的DStream。 transform(func) 通过对源DStream的每RDD应用RDD-to-RDD函数返回一个新的DStream，这可以用来在DStream做任意RDD操作。 updateStateByKey(func) 返回一个新状态的DStream,其中每个键的状态是根据键的前一个状态和键的新值应用给定函数func后的更新。这个方法可以被用来维持每个键的任何状态数据。 transform()方法和updateStateByKey()方法值得我们深入的探讨一下： transform(func) 该transform操作（转换操作）连同其其类似的 transformWith操作允许DStream 上应用任意RDD-to-RDD函数。 updateStateByKey 该 updateStateByKey 操作可以让你保持任意状态，同时不断有新的信息进行更新。要使用此功能，必须进行两个步骤 ： （1） 定义状态 - 状态可以是任意的数据类型。 （2） 定义状态更新函数 - 用一个函数指定如何使用先前的状态和从输入流中获取的新值 更新状态。 让我们用一个例子来说明，假设你要进行文本数据流中单词计数。在这里，正在运行的计数是状态而且它是一个整数。 窗口转换操作Spark Streaming 还提供了窗口的计算，它允许你通过滑动窗口对数据进行转换，窗口转换操作如下： 转换 描述 window(windowLength, slideInterval) 返回一个基于源DStream的窗口批次计算后得到新的DStream。 countByWindow(windowLength,slideInterval) 返回基于滑动窗口的DStream中的元素的数量。 reduceByWindow(func, windowLength,slideInterval) 基于滑动窗口对源DStream中的元素进行聚合操作，得到一个新的DStream。 reduceByKeyAndWindow(func,windowLength,slideInterval, [numTasks]) 基于滑动窗口对（K，V）键值对类型的DStream中的值按K使用聚合函数func进行聚合操作，得到一个新的DStream。 reduceByKeyAndWindow(func,invFunc,windowLength, slideInterval, [numTasks]) 一个更高效的reduceByKkeyAndWindow()的实现版本，先对滑动窗口中新的时间间隔内数据增量聚合并移去最早的与新增数据量的时间间隔内的数据统计量。例如，计算t+4秒这个时刻过去5秒窗口的WordCount，那么我们可以将t+3时刻过去5秒的统计量加上[t+3，t+4]的统计量，在减去[t-2，t-1]的统计量，这种方法可以复用中间三秒的统计量，提高统计的效率。 countByValueAndWindow(windowLength,slideInterval, [numTasks]) 基于滑动窗口计算源DStream中每个RDD内每个元素出现的频次并返回DStream[(K,Long)]，其中K是RDD中元素的类型，Long是元素频次。与countByValue一样，reduce任务的数量可以通过一个可选参数进行配置。 在Spark Streaming中，数据处理是按批进行的，而数据采集是逐条进行的，因此在Spark Streaming中会先设置好批处理间隔（batch duration），当超过批处理间隔的时候就会把采集到的数据汇总起来成为一批数据交给系统去处理。 对于窗口操作而言，在其窗口内部会有N个批处理数据，批处理数据的大小由窗口间隔（window duration）决定，而窗口间隔指的就是窗口的持续时间，在窗口操作中，只有窗口的长度满足了才会触发批数据的处理。除了窗口的长度，窗口操作还有另一个重要的参数就是滑动间隔（slide duration），它指的是经过多长时间窗口滑动一次形成新的窗口，滑动窗口默认情况下和批次间隔的相同，而窗口间隔一般设置的要比它们两个大。在这里必须注意的一点是滑动间隔和窗口间隔的大小一定得设置为批处理间隔的整数倍。 如批处理间隔示意图所示，批处理间隔是1个时间单位，窗口间隔是3个时间单位，滑动间隔是2个时间单位。对于初始的窗口time 1-time 3，只有窗口间隔满足了才触发数据的处理。这里需要注意的一点是，初始的窗口有可能流入的数据没有撑满，但是随着时间的推进，窗口最终会被撑满。当每个2个时间单位，窗口滑动一次后，会有新的数据流入窗口，这时窗口会移去最早的两个时间单位的数据，而与最新的两个时间单位的数据进行汇总形成新的窗口（time3-time5）。 批处理间隔、窗口间隔和滑动间隔是非常重要的三个时间概念，是理解窗口操作的关键所在。 批处理间隔 sparkStreaming系统要等到数据汇总到一定的量后再一并操作 窗口间隔和滑动间隔 批处理间隔的整数倍，处理数据的间隔取决于滑动间隔，所使用的数据量是窗口间隔来决定的。比如长度为四的窗口间隔，如果滑动间隔为2，那么在这四个批处理间隔内，会有两次的滑动操作，这两次的滑动操作中，会有两个批处理间隔的数据会重复的，但是sparkstreaming系统中做了优化，不会让重复计算发生 输出操作Spark Streaming允许DStream的数据被输出到外部系统，如数据库或文件系统。 转换 描述 print() 在Driver中打印出DStream中数据的前10个元素。 saveAsTextFiles(prefix, [suffix]) 将DStream中的内容以文本的形式保存为文本文件，其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。 saveAsObjectFiles(prefix, [suffix]) 将DStream中的内容按对象序列化并且以SequenceFile的格式保存。其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。 saveAsHadoopFiles(prefix, [suffix]) 将DStream中的内容以文本的形式保存为Hadoop文件，其中每次批处理间隔内产生的文件以prefix-TIME_IN_MS[.suffix]的方式命名。 foreachRDD(func) 最基本的输出操作，将func函数应用于DStream中的RDD上，这个操作会输出数据到外部系统，比如保存RDD到文件或者网络数据库等。需要注意的是func函数是在运行该streaming应用的Driver进程里执行的。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming Backpressure分析]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2FSparkStreaming%20Backpressure%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[SparkStreaming Backpressure分析引入Backpressure的原因默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，其中batch processing time为实际计算一个批次花费时间，batch interval为Streaming应用设置的批处理间隔。这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk,则内存存放不下的数据会溢写至disk,加大延迟）。Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数 “spark.streaming.receiver.maxRate”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。 BackPressure架构模型Spark Streaming Backpressure:根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。 Spark Streaming架构 BackPressure执行过程在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted”事件，然后从中==抽取processingDelay 及schedulingDelay信息==. Estimator依据这些信息==估算出最大处理速度（rate）==，最后由基于Receiver的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）. BackPressure 源码解析RateController类体系结构RateController继承自StreamingListener.用于处理BatchCompleted事件。 RateController的注册JobScheduler启动时会抽取在DStreamGraph中注册的所有InputDstream中的rateController，并向ListenerBus注册并开启监听。 12345for &#123; inputDStream &lt;- ssc.graph.getInputStreams rateController &lt;- inputDStream.rateController &#125; ssc.addStreamingListener(rateController) listenerBus.start() BackPressure 执行过程分析BackPressure 执行过程分为BatchCompleted事件触发时机和事件处理两个过程： BatchCompleted触发过程 BatchCompleted事件处理过程 BatchCompleted触发过程BatchedCompleted是批次处理结束的标志，也就是JobScheduler调度的作业执行完成时触发的 JobGenerater在调用generateJobs（）方法生成Job后，会使用JobScheduler的submitJobSet方法对Job进行提交. 12345678910def submitJobSet(jobSet: JobSet) &#123; if (jobSet.jobs.isEmpty) &#123; logInfo(&quot;No jobs added for time &quot; + jobSet.time) &#125; else &#123; listenerBus.post(StreamingListenerBatchSubmitted(jobSet.toBatchInfo)) jobSets.put(jobSet.time, jobSet) jobSet.jobs.foreach(job =&gt; jobExecutor.execute(new JobHandler(job))) logInfo(&quot;Added jobs for time &quot; + jobSet.time) &#125;&#125; 其中，jobSet中的Job将通过jobExecutor进行处理，对Job进行处理的处理器为JobHandler。JobHandler用于执行Job及处理Job执行结果信息。当Job执行完成时会产生JobCompleted事件. 当 Job执行完成时，向eventLoop发送JobCompleted事件。EventLoop事件处理器接到JobCompleted事件后将调用handleJobCompletion 来处理Job完成事件。 handleJobCompletion使用Job执行信息创建StreamingListenerBatchCompleted事件并通过StreamingListenerBus向监听器发送。 BatchCompleted事件的处理过程StreamingListenerBus将事件转交给具体的StreamingListener，因此BatchCompleted将交由RateController进行处理。RateController接到BatchCompleted事件后将调用onBatchCompleted对事件进行处理。 12345678910override def onBatchCompleted(batchCompleted: StreamingListenerBatchCompleted) &#123; val elements = batchCompleted.batchInfo.streamIdToInputInfo for &#123; processingEnd &lt;- batchCompleted.batchInfo.processingEndTime workDelay &lt;- batchCompleted.batchInfo.processingDelay waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay elems &lt;- elements.get(streamUID).map(_.numRecords) &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay) &#125; onBatchCompleted会从完成的任务中抽取任务的执行延迟和调度延迟，然后用这两个参数用RateEstimator估算出新的rate并发布。 借助ReceiverTracker进行转发。ReceiverTracker将rate包装成UpdateReceiverRateLimit事件并发送给ReceiverTrackerEndpoint. ReceiverTrackerEndpoint接到消息后，其将会从receiverTrackingInfos列表中获取Receiver注册时使用的endpoint(实为ReceiverSupervisorImpl)，再将rate包装成UpdateLimit发送至endpoint.其接到信息后，使用updateRate更新BlockGenerators(RateLimiter子类),来计算出一个固定的间隔。 backpressure反压机制调整rate结束。 流量控制点（生效位置）当Receiver开始接收数据时，会通过supervisor.pushSingle()方法将接收的数据存入currentBuffer等待BlockGenerator定时将数据取走，包装成block. 在将数据存放入currentBuffer之时，要获取许可（令牌）。如果获取到许可就可以将数据存入buffer, 否则将被阻塞，进而阻塞Receiver从数据源拉取数据。 12345678910111213def addData(data: Any): Unit = &#123; if (state == Active) &#123; waitToPush() synchronized &#123; if (state == Active) &#123; currentBuffer += data &#125; else &#123; throw new SparkException&#125; &#125; &#125; else &#123; throw new SparkException() &#125;&#125; 其令牌投放采用令牌桶机制进行， 原理如下图所示:(==参考生产者消费者模式==) 令牌桶机制： 如果令牌被消耗的速度小于产生的速度，令牌就会不断地增多，直到把桶填满。最后桶中可以保存的最大令牌数永远不会超过桶的大小。当进行某操作时需要令牌时会从令牌桶中取出相应的令牌数，如果获取到则继续操作，否则阻塞。用完之后不用放回。 Streaming 数据流被Receiver接收后，按行解析后存入iterator中。然后逐个存入Buffer，在存入buffer时会先获取token，如果没有token存在，则阻塞；如果获取到则将数据存入buffer. 然后等价后续生成block操作]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark stream初始化过程]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2FSpark%20Streaming%20%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E9%98%B6%E6%AE%B5%E5%88%86%E6%9E%90%EF%BC%88Receiver%E6%96%B9%E5%BC%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Spark Streaming 数据准备阶段分析（Receiver方式）Spark Streaming数据准备流程SparkStreaming的全过程分为两个阶段：数据准备阶段和数据计算阶段。两个阶段在功能上相互独立，仅通过数据联系在一起。 Spark Streaming数据准备阶段包含对流入数据的接收、分片（按照时间片划分为数据集）以及分片数据的分发工作。 Receiver接收外部数据流，其将接收的数据流交由BlockGenerator存储在ArrayBuffer中，在存储之前会先获取许可（由“spark.streaming.receiver.maxRate”指定，spark 1.5之后由backpressure进行自动计算，代表可以存取的最大速率，每存储一条数据获取一个许可，若未获取到许可接收将阻塞）。 BlockGenerater中定义一Timer,其依据设置的Interval定时将ArrayBuffer中的数据取出，包装成Block,并将Block存放入blocksForPushing中（阻塞队列ArrayBlockingQueue），并将ArrayBuffer清空 BlockGenerater中的blockPushingThread线程从阻塞队列中取出取出block信息，并以onPushBlock的方式将消息通过监听器（listener）发送给ReceiverSupervisor. ReceiverSupervisor收到消息后，将对消息中携带数据进行处理，其会通过调用BlockManager对数据进行存储，并将存储结果信息向ReceiverTracker汇报 ReceiverTracker收到消息后，将信息存储在未分配Block队列（streamidToUnallocatedBlock）中，等待JobGenerator生成Job时将其指定给RDD 1过程持续进行，2-5 以BlockInterval为周期重复执行. 源码分析以WordCount应用为例 数据接收在Receiver启动之后，其将开始接收外部数据源的数据（WordCount程序中使用的SocketReceiver是以主动接收的方式获取数据），并对数据进行存储。SocketReceiver实现代码如下： 123456789101112131415def receive() &#123; try &#123; val iterator = bytesToObjects(socket.getInputStream()) while(!isStopped &amp;&amp; iterator.hasNext) &#123; store(iterator.next()) &#125;......&#125;def store(dataItem: T) &#123; supervisor.pushSingle(dataItem)&#125;def pushSingle(data: Any) &#123; defaultBlockGenerator.addData(data)&#125; 其调用defaultBlockGenerator的addData将数据添加进currentBuffer，其中defaultBlockGenerator 即为BlockGenerator 12345678910def addData(data: Any): Unit = &#123; if (state == Active) &#123; waitToPush() synchronized &#123; if (state == Active) &#123; currentBuffer += data &#125; &#125; &#125; &#125; 其中waitToPush（）方法，是用来控制接收速率的，与BackPressure机制相关 Receiver会不断重复上述过程，接收数据，存入currentBuffer. 数据切片在启动Receiver进会创建ReceiverSupervisorImpl， ReceiverSupervisorImpl又会创建并启动BlockGenerator，用于对Receiver接收的数据流进行切片操作。其切片是以定时器的方式进行的。其时间周期由“spark.streaming.blockInterval”进行设置，默认为200ms. BlockGenerator的start方法实现如下： 1234567def start(): Unit = synchronized &#123; if (state == Initialized) &#123; state = Active blockIntervalTimer.start() blockPushingThread.start() &#125; &#125; blockIntervalTimer为定时器任务，其会周期性的执行计划任务 blockPushingThread为新线程，负载不断的从阻塞队列中取出打包的数据 数据流切分RecurringTimer为定时器，其每隔blockIntervalMs时间，执行一次updateCurrentBuffer方法，将currentBuffer中的数据进行打包，并添加到阻塞队列blocksForPushing中。 数据传输blockPushingThread 线程启动会，将执行keepPushingBlocks（）方法，从阻塞队列中取出切片后的数据，并通过defaultBlockGeneratorListener转发,并等待下一步存储、分发操作。（defaultBlockGeneratorListener在ReceiverSupervisorImpl中定义）。 Block 存储与汇报BlockGeneratorListener 监控到onPushBlock事件后，会对传输的数据分片进行存储操作，并向ReceiverTracker汇报。 Block存储BlockGeneratorListener 监控到onPushBlock事件后，经过一系列调整，最后将调用 pushAndReportBlock对数据分片进行存储， BlockManagerBasedBlockHandler通过BlockManager的接口对数据在Receiver所在节点进行保存，并依据StorageLevel 设置的副本数，在其它Executor中保存副本。 Block 汇报当Block保存完成，并且副本制作完成后，将通过trackerEndpoint向ReceiverTrack进行汇报。 ReceiverTrackEndpoint收到“AddBlock”信息后，将receivedBlockTracker将block信息保存入队列streamIdToUnallocatedBlockQueues中，以用于生成Job。 至此数据准备阶段完成，保存在streamIdToUnallocatedBlockQueues中的数据信息，在下一个批次生成Job时会被取出用于封装成RDD]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark stream初始化过程]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%20stream%2Fspark%20stream%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[spark stream初始化过程Spark Streaming是一种构建在Spark上的实时计算框架。Spark Streaming应用以Spark应用的方式提交到Spark平台，其组件以长期批处理任务的形式在Spark平台运行。这些任务主要负责接收实时数据流及定期产生批作业并提交至Spark集群， 数据接收 Job 生成 流量控制 动态资源伸缩 以WordCount程序为例分析Spark Streaming运行环境的初始化过程。 123456789val conf = new SparkConf().setAppName("wordCount").setMaster("local[4]") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(10)) val lines = ssc.socketTextStream("localhost", 8585, StorageLevel.MEMORY_ONLY) val words = lines.flatMap(_.split(" ")).map(w =&gt; (w,1)) val wordCount = words.reduceByKey(_+_) wordCount.print ssc.start()ssc.awaitTermination() StreamingContext的初始化过程StreamingContext是Spark Streaming应用的执行环境，其定义很多Streaming功能的入口，如：它提供从多种数据源创建DStream的方法等。 在创建Streaming应用时，首先应创建StreamingContext（WordCount应用可知），伴随StreamingContext的创建将会创建以下主要组件： DStreamGraphDStreamGraph的主要功能是记录InputDStream及OutputStream及从InputDStream中抽取出ReceiverInputStreams。因为DStream之间的依赖关系类似于RDD，并在任务执行时转换成RDD，因此，可以认为DStream Graph与RDD Graph存在对应关系. 即：DStreamGraph以批处理间隔为周期转换成RDDGraph. ReceiverInputStreams: 包含用于接收数据的Receiver信息，并在启动Receiver时提供相关信息 OutputStream：每个OutputStream会在批作业生成时，生成一个Job. JobSchedulerJobScheduler是Spark Streaming中最核心的组件，其负载Streaming各功作组件的启动。 数据接收 Job 生成 流量控制 动态资源伸缩以及负责生成的批Job的调度及状态管理工作。 DStream的创建与转换StreamingContext初始化完毕后，通过调用其提供的创建InputDStream的方法创建SocketInputDStream. SocketInputDStream的继承关系为：SocketInputDStream-&gt;ReceiverInputDStream-&gt;InputDStream-&gt;DStream 1ssc.graph.addInputStream(this) 在创建SocketInputDStream时，会先初始化InputDStream，在InputDStream中实现将自身加入DStreamGraph中，以标识其为输入数据源。 DStream中算子的转换，类似于RDD中的转换，都是延迟计算，仅形成pipeline链。当上述应用遇到print（Output算子）时，会将DStream转换为ForEachDStream,并调register方法作为OutputStream注册到DStreamGraph的outputStreams列表，以待生成Job。 12345678910111213141516171819202122def print(num: Int): Unit = ssc.withScope &#123; def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123; (rdd: RDD[T], time: Time) =&gt; &#123; val firstNum = rdd.take(num + 1) // scalastyle:off println println("-------------------------------------------") println(s"Time: $time") println("-------------------------------------------") firstNum.take(num).foreach(println) if (firstNum.length &gt; num) println("...") println() // scalastyle:on println &#125; &#125; foreachRDD(context.sparkContext.clean(foreachFunc), displayInnerRDDOps = false) &#125; private def foreachRDD( foreachFunc: (RDD[T], Time) =&gt; Unit, displayInnerRDDOps: Boolean): Unit = &#123; new ForEachDStream(this, context.sparkContext.clean(foreachFunc, false), displayInnerRDDOps).register() &#125; ForEachDStream 不同于其它DStream的地方为其重写了generateJob方法，以使DStream Graph操作转换成RDD Graph操作，并生成Job. SparkContext启动以线程的方式启动JobScheduler，从而开启各功能组件。 1234567ThreadUtils.runInNewThread("streaming-start") &#123; sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, "false") savedProperties.set(SerializationUtils.clone(sparkContext.localProperties.get())) scheduler.start()&#125; JobScheduler的启动JobScheduler主要负责以下几种任务： 数据接收相关组件的初始化及启动ReceiverTracker的初始化及启动。ReceiverTracker负责管理Receiver，包括Receiver的启停，状态维护 等。 Job生成相关组件的启动JobGenerator的启动。JobGenerator负责以BatchInterval为周期生成Job. Streaming监听的注册与启动 作业监听 反压机制BackPressure机制，通过RateController控制数据摄取速率。 Executor DynamicAllocation 的启动Executor 动态伸缩管理, 动态增加或减少Executor，来达到使用系统稳定运行 或减少资源开销的目的。 Job的调度及状态维护。 123456789101112def start(): Unit = synchronized &#123; //用以接收和处理事件 eventLoop = new EventLoop[JobSchedulerEvent]("JobScheduler") &#123; override protected def onReceive(event: JobSchedulerEvent): Unit = processEvent(event) override protected def onError(e: Throwable): Unit = reportError("Error in job scheduler", e) &#125; eventLoop.start() executorAllocationManager.foreach(ssc.addStreamingListener) receiverTracker.start() jobGenerator.start() executorAllocationManager.foreach(_.start())&#125; EventLoop开始执行时，会开启一deamon线程用于处理队列中的事件。EventLoop是一个抽象类，JobScheduler中初始化EventLoop时实现了其OnReceive方法。该方法中指定接收的事件由processEvent（event）方法处理]]></content>
      <categories>
        <category>大数据</category>
        <category>spark stream</category>
      </categories>
      <tags>
        <tag>spark stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git命令]]></title>
    <url>%2F2018%2F05%2F27%2Fgit%2Fgit%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Git 命令git中文件内容并没有真正存储在索引(.git/index)或者提交对象中，而是以blob的形式分别存储在数据库中(.git/objects)，并用SHA-1值来校验。 索引文件用识别码列出相关的blob文件以及别的数据。对于提交来说，以树(tree)的形式存储，同样用对于的哈希值识别。树对应着工作目录中的文件夹，树中包含的 树或者blob对象对应着相应的子目录和文件。每次提交都存储下它的上一级树的识别码。 基本用法 git add files 把当前文件放入暂存区域。 git commit 给暂存区域生成快照并提交。 git reset -- files 用来撤销最后一次git add files，你也可以用git reset 撤销所有暂存区域文件。（操作对象是HEAD） git checkout -- files 把文件从暂存区域复制到工作目录，用来丢弃本地修改。（目的是working Directory） git commit -a相当于运行 git add 把所有当前目录下的文件加入暂存区域再运行。git commit. git commit files 进行一次包含最后一次提交加上工作目录中文件快照的提交。并且文件被添加到暂存区域。 git checkout HEAD -- files 回滚到复制最后一次提交。 diff commit提交时，git用暂存区域的文件创建一个新的提交 更改一次提交，使用 git commit --amend，git会使用与当前提交相同的父节点进行一次新提交，旧的提交会被取消，可以让你==合并你暂存区的修改和上一次commit==，amend不是修改最近一次commit, 而是整个替换掉他. 对于Git来说是一个新的commit. checkoutcheckout命令用于从历史提交（或者暂存区域）中拷贝文件到工作目录，也可用于切换分支。 注意当前分支不会发生变化(HEAD指向原处)。 当不指定文件名，而是给出一个（本地）分支时，那么HEAD标识会移动到那个分支 git checkout -b name来创建一个新的分支 git checkout . 或者 git checkout -- &lt;file&gt; 命令时，会用暂存区全部或指定的文件替换工作区的文件。这个操作很危险，会清除工作区中未添加到暂存区的改动。 git checkout HEAD . 或者 git checkout HEAD &lt;file&gt; 命令时，会用 HEAD 指向的 master 分支中的全部或者部分文件替换暂存区和以及工作区中的文件。这个命令也是极具危险性的，因为不但会清除工作区中未提交的改动，也会清除暂存区中未提交的改 动。 git checkout -f =等价=&gt; git checkout HEAD . resetreset命令把当前分支指向另一个位置，并且有选择的变动工作目录和index。也用来在从历史仓库中复制文件到index，而不动工作目录。 --hard 重置HEAD返回到另外一个commit，重置index和working directory --soft 重置HEAD返回到另外一个commit --mix 默认参数，重置HEAD返回到另外一个commit，重置index 如果给了文件名, 那么工作效果和带文件名的checkout差不多，除了索引被更新。 revertgit revert用一个新提交来消除一个历史提交所做的任何修改.，不是从项目历史中移除这个提交。这避免了Git丢失项目历史 revert 之后你的本地代码会回滚到指定的历史版本,这时你再 git push 既可以把线上的代码更新. 与reset区别： git revert 是撤销某次操作，此次操作之前的commit都会被保留，而git reset 是撤销某次提交，但是此次之后的修改都会被退回到暂存区中。 git reset 是把HEAD向后移动了一下，而git revert是HEAD继续前进 merge合并分支，索引必须和当前提交相同。如果当前提交是另一个分支的祖父节点，就导致fast-forward合并。指向只是简单的移动，并生成一个新的提交 否则就是一次真正的合并。默认把当前提交(ed489 如下所示)和另一个提交(33104)以及他们的共同祖父节点(b325c)进行一次三方合并。 结果是先保存当前目录和索引，然后和父节点33104一起做一次新提交。 rebase合并把两个父分支合并进行一次提交，提交历史不是线性的。衍合在当前分支上重演另一个分支的历史，提交历史是线性的。 上面的命令都在topic分支中进行，而不是master分支 GIT中版本的保存文件状态 git directory就是我们的本地仓库.git目录，里面保存了所有的版本信息等内容。 working driectory，工作目录，就是我们的工作目录，其中包括未跟踪文件及已跟踪文件，而已跟踪文件都是从git directory取出来的文件的某一个版本或新跟踪的文件。 staging area，暂存区，不对应一个具体目录，其时只是git directory中的一个特殊文件。 当我们修改了一些文件后，要将其放入暂存区然后才能提交，每次提交时其实都是提交暂存区的文件到git仓库，然后==清除暂存区。== Git 常用命令]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git内部原理]]></title>
    <url>%2F2018%2F05%2F27%2Fgit%2Fgit%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Git 内部原理从根本上来讲 Git 是一套内容寻址 (content-addressable) 文件系统，在此之上提供了一个 VCS 用户界面。 底层命令 (Plumbing) 和高层命令 (Porcelain)在一个新目录或已有目录内执行 git init 时，Git 会创建一个 .git 目录，几乎所有 Git 存储和操作的内容都位于该目录下。如果你要备份或复制一个库，基本上将这一目录拷贝至其他地方就可以了。 hooks 目录保存了客户端或服务端钩子脚本。gitweb系统或其他git托管系统会经常用到hook script。 info 包含仓库的一些信息 logs:保存所有更新的引用记录 objects:该目录存放所有的Git对象，对象的SHA1哈希值的前两位是文件夹名称，后38位作为对象文件名。 refs:具体的引用，Reference Specification，存储指向数据 (分支) 的提交对象的指针，这个目录一般包括三个子文件夹，heads、remotes和tags HEAD 文件指向当前分支 config:这个是GIt仓库的配置文件 description:仓库的描述信息，主要给gitweb等git托管系统使用 index:这个文件就是我们前面提到的暂存区（stage），是一个二进制文件 Git对象Git 是一套内容寻址文件系统。它允许插入任意类型的内容，并会返回一个键值，通过该键值可以在任何时候再取出该内容。 初使化一个 Git 仓库并确认 objects 目录是空的，Git 初始化了 objects 目录，同时在该目录下创建了 pack 和 info 子目录，但是该目录下没有其他常规文件。 Git 存储数据内容的方式──为每份内容生成一个文件，取得该内容与头信息的 SHA-1 校验和，创建以该校验和前两个字符为名称的子目录，并以 (校验和) 剩下 38 个字符为文件命名 (保存至子目录下)。 tree (树) 对象Git 以一种类似 UNIX 文件系统但更简单的方式来存储内容。所有内容以 tree 或 blob 对象存储，其中 tree 对象对应于 UNIX 中的目录，blob 对象则大致对应于 inodes 或文件内容。一个单独的 tree 对象包含一条或多条 tree 记录，每一条记录含有一个指向 blob 或子 tree 对象的 SHA-1 指针，并附有该对象的权限模式 (mode)、类型和文件名信息。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD简介和RDD模型]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2Frdd%2F</url>
    <content type="text"><![CDATA[RDD弹性分布式数据集（Resilient Distributed Dataset，简称 RDD）。RDD 其实就是分布式的元素集合。在 Spark 中，对数据的所有操作主要是创 建 RDD、转化已有 RDD 以及调用 RDD 操作进行求值。 Spark 中的 RDD 就是一个不可变的分布式对象集合。每个 RDD 都被分为多个分区，这些 分区运行在集群中的不同节点上。 创建 RDDSpark 提供了两种创建 RDD 的方式： 读取一个外部数据集 sc.textFile(&quot;README.md&quot;) 驱动器程序中对一个集合进 行并行化 val in =sc.parallelize(List(&quot;pandas&quot;, &quot;i like pandas&quot;),3) 操作RDDRDD 支持两种类型的操作：转化操作（transformation）和行动操作（action）。转化操作会由一个 RDD 生成一个新的 RDD，行动操作会对 RDD 计算出一个结果，并把结果返回到Driver程序中，或把结果存储到外部存储系统（如 HDFS）中。 惰性计算，转化操作和行动操作的区别在于 Spark 计算 RDD 的方式不同。 Spark只有第一次在一个行动操作中用到 时，才会真正计算。 转化操作：pythonLines = lines.filter(lambda line: &quot;Python&quot; in line) 行动操作：pythonLines.first(), pythonLines.persist() Spark程序过程总的来说，每个 Spark 程序或 shell 会话都按如下方式工作。 从外部数据创建出输入 RDD。 使用诸如 ﬁlter() 这样的转化操作对 RDD 进行转化，以定义新的 RDD。 告诉 Spark 对需要被重用的中间结果 RDD 执行 persist() 操作。 使用行动操作（例如 count() 和 ﬁrst() 等）来触发一次并行计算，Spark 会对计算进行优化后再执行。 RDD特点RDD是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有==依赖关系==的。基于RDDs之间的依赖，RDDs会形成一个==有向无环图DAG==，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过==血缘关系(Lineage)==一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。 RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过==持久化RDD来切断血缘关系==。 分区RDD以partition作为最小存储和计算单元，分布在cluster的不同nodes上，一个node可以有多个partitions，一个partition只能在一个node上 并行一个Task对应一个partition，Tasks之间相互独立可以并行计算 只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。 如下图所示，依赖包括两种 窄依赖（Narrow）是指父RDD的每个分区只被子RDD的一个分区所使用，子RDD分区通常对应常数个父RDD分区(O(1)，与数据规模无关) 宽依赖（shuffle）是指父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应多个的父RDD分区(O(n)，与数据规模有关) 宽依赖和窄依赖如下图所示： 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。 checkpoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 RDD操作RDDs support 两种类型的操作: transformations（转换）, 和 actions（动作） Spark 可以从 Hadoop 所支持的任何存储源中创建 distributed dataset（分布式数据集），包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3 等等。 Spark 支持文本文件，SequenceFiles，以及任何其它的 Hadoop InputFormat。 可以使用 SparkContext 的 textFile 方法来创建文本文件的 RDD。此方法需要一个文件的 URI（计算机上的本地路径 ，hdfs://，s3n:// 等等的 URI），并且读取它们作为一个 lines（行）的集合。 传递 Functions（函数）给 Spark当 driver 程序在集群上运行时，Spark 的 API 在很大程度上依赖于传递函数。有 2 种推荐的方式: Anonymous function syntax（匿名函数语法）, 它可以用于短的代码片断. 在全局单例对象中的静态方法. 例如, 您可以定义 object MyFunctions 然后传递 MyFunctions.func1, 如下: 12345object MyFunctions &#123; def func1(s: String): String = &#123; ... &#125;&#125;myRdd.map(MyFunctions.func1) 请注意，虽然也有可能传递一个类的实例（与单例对象相反）的方法的引用，这需要发送整个对象，包括类中其它方法。例如，考虑: 1234class MyClass &#123; def func1(s: String): String = &#123; ... &#125; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(func1) &#125;&#125; 这里，如果我们创建一个 MyClass 的实例，并调用 doStuff，在 map 内有 MyClass 实例的 func1 方法的引用，所以整个对象需要被发送到集群的。它类似于 rdd.map(x =&gt; this.func1(x)) 类似的方式，访问外部对象的字段将引用整个对象: 1234class MyClass &#123; val field = &quot;Hello&quot; def doStuff(rdd: RDD[String]): RDD[String] = &#123; rdd.map(x =&gt; field + x) &#125;&#125; 相当于写 rdd.map(x =&gt; this.field + x), 它引用 this 所有的东西. 为了避免这个问题, 最简单的方式是复制 field 到一个本地变量，而不是外部访问它: 1234def doStuff(rdd: RDD[String]): RDD[String] = &#123; val field_ = this.field rdd.map(x =&gt; field_ + x)&#125; 理解闭包一个关于 Spark 更难的事情是理解变量和方法的范围和生命周期. 修改其范围之外的变量 RDD 操作可以混淆的常见原因。在下面的例子中，我们将看一下使用的 foreach() 代码递增累加计数器，但类似的问题，也可能会出现其他操作上. 示例考虑一个简单的 RDD 元素求和，以下行为可能不同，具体取决于是否在同一个 JVM 中执行. 一个常见的例子是当 Spark 运行在 local 本地模式（--master = local[n]）时，与部署 Spark 应用到群集（例如，通过 spark-submit 到 YARN）: 1234var counter = 0var rdd = sc.parallelize(data)rdd.foreach(x =&gt; counter += x)println(&quot;Counter value: &quot; + counter) Local（本地）vs. cluster（集群）模式上面的代码行为是不确定的，并且可能无法按预期正常工作。执行作业时，Spark 会分解 RDD 操作到每个 executor 中的 task 里。在执行之前，Spark 计算任务的 closure（闭包）。闭包是指 executor 要在RDD上进行计算时必须对执行节点可见的那些变量和方法（在这里是foreach()）。闭包被序列化并被发送到每个 executor。 闭包的变量副本发给每个 executor ，==当 counter 被 foreach 函数引用的时候，它已经不再是 driver node 的 counter 了==。虽然在 driver node 仍然有一个 counter 在内存中，但是对 executors 已经不可见。executor 看到的只是序列化的闭包一个副本。所以 counter 最终的值还是 0，因为对 counter 所有的操作均引用序列化的 closure 内的值。 在 local 本地模式，在某些情况下的 foreach 功能实际上是同一 JVM 上的驱动程序中执行，并会引用同一个原始的 counter 计数器，实际上可能更新. 为了确保这些类型的场景明确的行为应该使用的 Accumulator 累加器。当一个执行的任务分配到集群中的各个 worker 结点时，Spark 的累加器是专门提供安全更新变量的机制。 在一般情况下，closures - constructs 像循环或本地定义的方法，==不应该被用于改动一些全局状态==。Spark 没有规定或保证突变的行为，以从封闭件的外侧引用的对象。一些代码，这可能以本地模式运行，但是这只是偶然和这样的代码如预期在分布式模式下不会表现。如果需要一些全局的聚合功能，应使用 Accumulator（累加器）。 打印 RDD 的 elements另一种常见的语法用于打印 RDD 的所有元素使用 rdd.foreach(println) 或 rdd.map(println)。在一台机器上，这将产生预期的输出和打印 RDD 的所有元素。然而，在集群 cluster 模式下，stdout 输出正在被执行写操作 executors 的 stdout 代替，而不是在一个驱动程序上，==因此stdout 的 driver 程序不会显示这些==！==要打印 driver 程序的所有元素，可以使用的 collect() 方法首先把 RDD 放到 driver 程序节点上: rdd.collect().foreach(println)。这可能会导致 driver 程序耗尽内存==，虽说，因为 collect() 获取整个 RDD 到一台机器; 如果你只需要打印 RDD 的几个元素，一个更安全的方法是使用 take(): rdd.take(100).foreach(println)。 Transformations（转换） Value数据类型的Transformation算子这种变换并不触发提交作业针对处理的数据项是Value型的数据。 Key-Value数据类型的Transfromation算子这种变换并不触发提交作业针对处理的数据项是Key-Value型的数据对。 Value型处理数据类型为Value型的Transformation算子可以根据RDD变换算子的输入分区与输出分区关系分为以下几种类型: 1）输入分区与输出分区一对一型2）输入分区与输出分区多对一型3）输入分区与输出分区多对多型4）输出分区为输入分区子集型5）还有一种特殊的输入与输出分区一对一的算子类型：Cache型。 Cache算子对RDD分区进行缓存 输入分区与输出分区一对一型 map：数据集中的每个元素经过用户自定义的函数转换形成一个新的RDD，新的RDD叫MappedRDD. flatMap，与map类似，但每个元素输入项都可以被映射到0个或多个的输出项，最终将结果”扁平化“后输出。 mapPartitions，mapPartitions函数获取到每个分区的迭代器，在函数中通过这个分区整体的迭代器对整个分区的元素进行操作。 内部实现是生成MapPartitionsRDD。 类似与map，map作用于每个分区的每个元素，但mapPartitions作用于每个分区中，调用参数不同 func的类型：Iterator[T] =&gt; Iterator[U] 下图中用户通过函数 f (iter)=&gt;iter.f ilter(_&gt;=3) 对分区中所有数据进行过滤大于和等于 3 的数据保留。 mapPartitionsWithIndex 与mapPartitions类似，不同的是函数多了个分区索引的参数 func类型：(Int, Iterator[T]) =&gt; Iterator[U] glom glom函数将每个分区形成一个数组，内部实现是返回的RDD[Array[T]]。 输入分区与输出分区多对一型 union，使用union函数时需要保证两个RDD元素的数据类型相同，返回的RDD数据类型和被合并的RDD元素数据类型相同，并不进行去重操作，保存所有元素。如果想去重，可以使用distinct（）。++符号相当于uion函数操作。 cartesian，笛 卡 尔 积 操 作。 操 作 后 内 部 实 现 返 回CartesianRDD 1234// 参数是另外一个rdd，所以是一对多def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope &#123; new CartesianRDD(sc, this, other)&#125; 输入分区与输出分区多对多型 groupBy，将元素通过函数生成相应的Key，数据就转化为Key-Value格式，之后将Key相同的元素分为一组。 1def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] 输出分区为输入分区子集型 filter，filter的功能是对元素进行过滤，对每个元素应用f函数，返回值为true的元素在RDD中保留，返回为false的将过滤掉。 内部实现相当于生成FilteredRDD(this，sc.clean(f))。 distinct，distinct将RDD中的元素进行去重操作。 subtract，subtract相当于进行集合的差操作，RDD 1去除RDD 1和RDD 2交集中的所有元素。 sample，sample将RDD这个集合内的元素进行采样，获取所有元素的子集。用户可以设定是否有放回的抽样、百分比、随机种子，进而决定采样方式。 1234def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T]) takeSample，takeSample()函数和上面的sample函数是一个原理，但是不使用相对比例采样，而是按设定的采样个数进行采样，同时返回结果不再是RDD，而是相当于对采样后的数据进行collect()，返回结果的集合为单机的数组。 cache型 cache，cache将RDD元素从磁盘缓存到内存，相当于persist（MEMORY_ONLY）函数的功能。 persist，persist函数对RDD进行缓存操作。数据缓存在哪里由StorageLevel枚举类型确定。有几种类型的组合，DISK代表磁盘，MEMORY代表内存，SER代表数据是否进行序列化存储。 | Storage Level（存储级别） | Meaning（含义） || ————————————– | ———————————————————— || MEMORY_ONLY | 将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中. 如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算. 这是默认的级别. || MEMORY_AND_DISK | 将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取. || MEMORY_ONLY_SER (Java and Scala) | 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer 时会节省更多的空间，但是在读取时会增加 CPU 的计算负担. || MEMORY_AND_DISK_SER (Java and Scala) | 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算. || DISK_ONLY | 只在磁盘上缓存 RDD. || MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. | 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本. || OFF_HEAP (experimental 实验性) | 类似于 MEMORY_ONLY_SER, 但是将数据存储在 off-heap memory 中. 这需要启用 off-heap 内存. | 在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据.这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法. Spark 的存储级别的选择，核心问题是在 memory 内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择: 如果您的 RDD 适合于默认存储级别 (MEMORY_ONLY), leave them that way. 这是CPU效率最高的选项，允许RDD上的操作尽可能快地运行. 如果不是, 试着使用 MEMORY_ONLY_SER 和 selecting a fast serialization library 以使对象更加节省空间，但仍然能够快速访问。 (Java和Scala) 不要溢出到磁盘，除非计算您的数据集的函数是昂贵的, 或者它们过滤大量的数据. 否则, 重新计算分区可能与从磁盘读取分区一样快. 如果需要快速故障恢复，请使用复制的存储级别 (e.g. 如果使用Spark来服务 来自网络应用程序的请求). All 存储级别通过重新计算丢失的数据来提供完整的容错能力，但复制的数据可让您继续在 RDD 上运行任务，而无需等待重新计算一个丢失的分区. key-value型输入分区与输出分区一对一 mapValues，mapValues：针对（Key，Value）型数据中的Value进行Map操作，而不对Key进行处理。 123456def mapValues[U](f: V =&gt; U): RDD[(K, U)] = &#123; val cleanF = self.context.clean(f) new MapPartitionsRDD[(K, U), (K, V)](self, (context, pid, iter) =&gt; iter.map &#123; case (k, v) =&gt; (k, cleanF(v)) &#125;, preservesPartitioning = true)&#125; 单个RDD或两个RDD聚集 combineByKey, 对单个Rdd的聚合。相当于将元素为（Int，Int）的RDD转变为了（Int，Seq[Int]）类型元素的RDD。定义combineByKey算子的说明如下： 12345678def combineByKey[C]( createCombiner: V =&gt; C, // 在C不存在的情况下，如通过V创建seq C。 mergeValue: (C, V) =&gt; C, // 当C已经存在的情况下，需要merge，如把item V加到seq mergeCombiners: (C, C) =&gt; C, // 合并两个C partitioner: Partitioner, // Partitioner（分区器），Shuffle时需要通过Partitioner的分区策略进行分区。 mapSideCombine: Boolean = true, // 为了减小传输量，很多combine可以在map端先做。例如， 叠加可以先在一个partition中把所有相同的Key的Value叠加， 再shuffle。 serializer: Serializer = null // 传输需要序列化，用户可以自定义序列化类。 ): RDD[(K, C)] 源码： 1234567891011121314151617181920212223242526272829303132333435363738394041def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] = &#123; require(mergeCombiners != null, "mergeCombiners must be defined") // required as of Spark 0.9.0 if (keyClass.isArray) &#123; if (mapSideCombine) &#123; throw new SparkException("Cannot use map-side combining with array keys.") &#125; if (partitioner.isInstanceOf[HashPartitioner]) &#123; throw new SparkException("Default partitioner cannot partition array keys.") &#125; &#125; val aggregator = new Aggregator[K, V, C]( self.context.clean(createCombiner), self.context.clean(mergeValue), self.context.clean(mergeCombiners)) if (self.partitioner == Some(partitioner)) &#123; self.mapPartitions(iter =&gt; &#123; val context = TaskContext.get() new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context)) &#125;, preservesPartitioning = true) &#125; else &#123; new ShuffledRDD[K, V, C](self, partitioner) .setSerializer(serializer) .setAggregator(aggregator) .setMapSideCombine(mapSideCombine) &#125;&#125;/** * Simplified version of combineByKey that hash-partitions the output RDD. */def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, numPartitions: Int): RDD[(K, C)] = &#123; combineByKey(createCombiner, mergeValue, mergeCombiners, new HashPartitioner(numPartitions))&#125; reduceByKey，reduceByKey是更简单的一种情况，只是两个值合并成一个值，所以createCombiner很简单，就是直接返回v，而mergeValue和mergeCombiners的逻辑相同，没有区别。 123456def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = &#123; combineByKey[V]((v: V) =&gt; v, func, func, partitioner)&#125;def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)] = &#123; reduceByKey(new HashPartitioner(numPartitions), func)&#125; partitionBy，partitionBy函数对RDD进行分区操作。 如果原有RDD的分区器和现有分区器（partitioner）一致，则不重分区，如果不一致，则相当于根据分区器生成一个新的ShuffledRDD。 cogroup，cogroup函数将两个RDD进行协同划分。对在两个RDD中的Key-Value类型的元素，每个RDD相同Key的元素分别聚合为一个集合，并且返回两个RDD中对应Key的元素集合的迭代器(K, (Iterable[V], Iterable[w]))。其中，Key和Value，Value是两个RDD下相同Key的两个数据集合的迭代器所构成的元组。 join join，join对两个需要连接的RDD进行cogroup函数操作。cogroup操作之后形成的新RDD，对每个key下的元素进行笛卡尔积操作，返回的结果再展平，对应Key下的所有元组形成一个集合，最后返回RDD[(K，(V，W))]。join的本质是通过cogroup算子先进行协同划分，再通过flatMapValues将合并的数据打散。 leftOuterJoin和rightOuterJoin，LeftOuterJoin（左外连接）和RightOuterJoin（右外连接）相当于在join的基础上先判断一侧的RDD元素是否为空，如果为空，则填充为空。 如果不为空，则将数据进行连接运算，并返回结果。 123456789def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))] = &#123; this.cogroup(other, partitioner).flatMapValues &#123; pair =&gt; if (pair._2.isEmpty) &#123; pair._1.iterator.map(v =&gt; (v, None)) &#125; else &#123; for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, Some(w)) &#125; &#125; &#125; Action算子本质上在Actions算子中通过SparkContext执行提交作业的runJob操作，触发了RDD DAG的执行。 根据Action算子的输出空间将Action算子进行分类：无输出、 HDFS、 Scala集合和数据类型。 无输出 foreach 对RDD中的每个元素都应用f函数操作，不返回RDD和Array，而是返回Uint。 1234def foreach(f: T =&gt; Unit) &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF)) &#125; HDFS saveAsTextFile, 函数将数据输出，存储到HDFS的指定目录。将RDD中的每个元素映射转变为（Null，x.toString），然后再将其写入HDFS。 saveAsObjectFile, saveAsObjectFile将分区中的每10个元素组成一个Array，然后将这个Array序列化，映射为（Null，BytesWritable（Y））的元素，写入HDFS为SequenceFile的格式。 Scala集合和数据类型 collect, collect将分布式的RDD返回为一个单机的scala Array数组。 在这个数组上运用scala的函数式操作。 1234567/** * Return an array that contains all of the elements in this RDD. */def collect(): Array[T] = &#123; val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray) Array.concat(results: _*)&#125; collectAsMap, collectAsMap对（K，V）型的RDD数据返回一个单机HashMap。对于重复K的RDD元素，后面的元素覆盖前面的元素。 1234567def collectAsMap(): Map[K, V] = &#123; val data = self.collect() val map = new mutable.HashMap[K, V] map.sizeHint(data.length) data.foreach &#123; pair =&gt; map.put(pair._1, pair._2) &#125; map&#125; reduceByKeyLocally, 实现的是先reduce再collectAsMap的功能，先对RDD的整体进行reduce操作，然后再收集所有结果返回为一个HashMap。 lookup, Lookup函数对（Key，Value）型的RDD操作，返回指定Key对应的元素形成的Seq。这个函数处理优化的部分在于，如果这个RDD包含分区器，则只会对应处理K所在的分区，然后返回由（K，V）形成的Seq。如果RDD不包含分区器，则需要对全RDD元素进行暴力扫描处理，搜索指定K对应的元素。 count, count返回整个RDD的元素个数。 top, top可返回最大的k个元素。 相近函数说明： top返回最大的k个元素。 take返回最小的k个元素。 takeOrdered返回最小的k个元素， 并且在返回的数组中保持元素的顺序。 first相当于top（ 1） 返回整个RDD中的前k个元素， 可以定义排序的方式Ordering[T]。返回的是一个含前k个元素的数组。 reduce, reduce函数相当于对RDD中的元素进行reduceLeft函数的操作。reduceLeft先对两个元素进行reduce函数操作然后将结果和迭代器取出的下一个元素进行reduce函数操作直到迭代器遍历完所有元素得到最后结果。在RDD中先对每个分区中的所有元素的集合分别进行reduceLeft。 每个分区形成的结果相当于一个元素再对这个结果集合进行reduceleft操作。==即先计算各个分区，最后再合并结果== flod， fold和reduce的原理相同但是与reduce不同相当于每个reduce时迭代器取的第一个元素是zeroValue。 def fold(zeroValue: T)(op: (T, T) =&gt; T) aggregate， aggregate先对每个分区的所有元素进行aggregate操作再对分区的结果进行fold操作。aggreagate与fold和reduce的不同之处在于aggregate相当于采用归并的方式进行数据聚集这种聚集是并行化的。 而在fold和reduce函数的运算过程中每个分区中需要进行串行处理每个分区串行计算完结果结果再按之前的方式进行聚集并返回最终聚集结果。 共享变量通常情况下，一个传递给 Spark 操作（例如 map 或 reduce）的函数 func 是在远程的集群节点上执行的。该函数 func 在多个节点执行过程中使用的变量，是同一个变量的多个副本。这些变量的以副本的方式拷贝到每个机器上，并且各个远程机器上变量的更新并不会传播回 driver program（驱动程序）。Spark 提供了两种特定类型的共享变量 : broadcast variables（广播变量）和 accumulators（累加器）。 广播变量Broadcast variables（广播变量）允许程序员将一个 ==read-only（只读的）==变量缓存到每台机器上，而不是给任务传递一个副本。它们是如何来使用呢，例如，广播变量可以用一种高效的方式给每个节点传递一份比较大的 input dataset（输入数据集）副本。在使用广播变量时，Spark 也尝试使用高效广播算法分发 broadcast variables（广播变量）以降低通信成本。 Spark 的 action（动作）操作是通过一系列的 stage（阶段）进行执行的，这些 stage（阶段）是通过分布式的 “shuffle” 操作进行拆分的。Spark 会自动广播出每个 stage（阶段）内任务所需要的公共数据。这种情况下广播的数据使用序列化的形式进行缓存，并在每个任务运行前进行反序列化。这也就意味着，只有在跨越多个 stage（阶段）的多个任务会使用相同的数据，或者在使用反序列化形式的数据特别重要的情况下，使用广播变量会有比较好的效果。 广播变量通过在一个变量 v 上调用 SparkContext.broadcast(v) 方法来进行创建。广播变量是 v 的一个 wrapper（包装器），可以通过调用 value方法来访问它的值。代码示例如下: 12345scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)scala&gt; broadcastVar.valueres0: Array[Int] = Array(1, 2, 3) 在创建广播变量之后，在集群上执行的所有的函数中，应该使用该广播变量代替原来的 v 值，所以节点上的 v 最多分发一次。另外，对象 v 在广播后不应该再被修改，以保证分发到所有的节点上的广播变量具有同样的值（例如，如果以后该变量会被运到一个新的节点）。 Accumulators（累加器）Accumulators（累加器）是一个仅可以执行 “added”（添加）的变量来通过一个关联和交换操作，因此可以高效地执行支持并行。累加器可以用于实现 counter（ 计数，类似在 MapReduce 中那样）或者 sums（求和）。原生 Spark 支持数值型的累加器，并且程序员可以添加新的支持类型。 作为一个用户，您可以创建 accumulators（累加器）并且重命名. 如下图所示, 一个命名的 accumulator 累加器（在这个例子中是 counter）将显示在 web UI 中，用于修改该累加器的阶段。 Spark 在 “Tasks” 任务表中显示由任务修改的每个累加器的值. 在 UI 中跟踪累加器可以有助于了解运行阶段的进度（注: 这在 Python 中尚不支持）. 分区RDD 内部，如何表示并行计算的一个计算单元。答案是使用分区（Partition） RDD 内部的数据集合在逻辑上和物理上被划分成多个小子集合，这样的每一个子集合我们将其称为分区，分区的个数会决定并行计算的粒度，而每一个分区数值的计算都是在一个单独的任务中进行，因此并行任务的个数，也是由 RDD分区的个数决定的 1234trait Partition extends Serializable &#123; def index: Int override def hashCode(): Int = index&#125; RDD 只是数据集的抽象，分区内部并不会存储具体的数据。Partition 类内包含一个 index 成员，表示该分区在 RDD 内的编号，通过 RDD 编号 + 分区编号可以唯一确定该分区对应的块编号，利用底层数据存储层提供的接口，就能从存储介质（如：HDFS、Memory）中提取出分区对应的数据。 分区个数RDD 分区的一个分配原则是：尽可能使得分区的个数，等于集群核心数目。 分区器数据倾斜数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 分区的作用在PairRDD即（key,value）这种格式的rdd中，很多操作都是基于key的，因此为了独立分割任务，会按照key对数据进行重组。比如groupbykey 重组肯定是需要一个规则的，最常见的就是基于Hash，Spark还提供了一种稍微复杂点的基于抽样的Range分区方法。 下面我们先看看分区器在Spark计算流程中是怎么使用的： Paritioner的使用1234abstract class Partitioner extends Serializable &#123; def numPartitions: Int def getPartition(key: Any): Int&#125; 就拿groupbykey来说： 12def groupByKey(): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey())) 它会调用PairRDDFunction的groupByKey()方法 123def groupByKey(): RDD[(K, Iterable[V])] = self.withScope &#123; groupByKey(defaultPartitioner(self)) &#125; 在这个方法里面创建了默认的分区器。默认的分区器是这样定义的： 123456789101112def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = &#123; val bySize = (Seq(rdd) ++ others).sortBy(_.partitions.size).reverse //获取最多分区数的RDD for (r &lt;- bySize if r.partitioner.isDefined &amp;&amp; r.partitioner.get.numPartitions &gt; 0) &#123; // 该RDD分区器定义有效 return r.partitioner.get &#125; // 返回hash RDD分区器 if (rdd.context.conf.contains("spark.default.parallelism")) &#123; new HashPartitioner(rdd.context.defaultParallelism) &#125; else &#123; new HashPartitioner(bySize.head.partitions.size) &#125; &#125; 首先获取当前分区的分区个数，如果没有设置spark.default.parallelism参数，则创建一个跟之前分区个数一样的Hash分区器。 当然，用户也可以自定义分区器，或者使用其他提供的分区器。API里面也是支持的： 123456// 传入分区器对象def groupByKey(partitioner: Partitioner): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey(partitioner)))// 传入分区的个数def groupByKey(numPartitions: Int): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey(numPartitions))) HashPatitionerHash分区器，是最简单也是默认提供的分区器 123456789101112131415161718192021class HashPartitioner(partitions: Int) extends Partitioner &#123; require(partitions &gt;= 0, s"Number of partitions ($partitions) cannot be negative.") def numPartitions: Int = partitions // 通过key计算其HashCode，并根据分区数取模。如果结果小于0，直接加上分区数。 def getPartition(key: Any): Int = key match &#123; case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) &#125; // 对比两个分区器是否相同，直接对比其分区个数就行 override def equals(other: Any): Boolean = other match &#123; case h: HashPartitioner =&gt; h.numPartitions == numPartitions case _ =&gt; false &#125; override def hashCode: Int = numPartitions&#125; 这里最重要的是这个Utils.nonNegativeMod(key.hashCode, numPartitions),它决定了数据进入到哪个分区。 1234def nonNegativeMod(x: Int, mod: Int): Int = &#123; val rawMod = x % mod rawMod + (if (rawMod &lt; 0) mod else 0) &#125; 说白了，就是基于这个key获取它的hashCode，然后对分区个数取模。由于HashCode可能为负，这里直接判断下，如果小于0，再加上分区个数即可。 因此，基于hash的分区，只要保证你的key是分散的，那么最终数据就不会出现数据倾斜的情况。 RangePartitioner从HashPartitioner分区的实现原理我们可以看出，其结果可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据，这显然不是我们需要的。而RangePartitioner分区则尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，也就是说一个分区中的元素肯定都是比另一个分区内的元素小或者大；==但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。== 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class RangePartitioner[K: Ordering : ClassTag, V]( partitions: Int, rdd: RDD[_ &lt;: Product2[K, V]], private var ascending: Boolean = true) extends Partitioner &#123; // We allow partitions = 0, which happens when sorting an empty RDD under the default settings. require(partitions &gt;= 0, s"Number of partitions cannot be negative but found $partitions.") // 获取RDD中key类型数据的排序器 private var ordering = implicitly[Ordering[K]] // An array of upper bounds for the first (partitions - 1) partitions private var rangeBounds: Array[K] = &#123; if (partitions &lt;= 1) &#123; // 如果给定的分区数是一个的情况下，直接返回一个空的集合，表示数据不进行分区 Array.empty &#125; else &#123; // This is the sample size we need to have roughly balanced output partitions, capped at 1M. // 给定总的数据抽样大小，最多1M的数据量(10^6)，最少20倍的RDD分区数量，也就是每个RDD分区至少抽取20条数据 val sampleSize = math.min(20.0 * partitions, 1e6) // Assume the input partitions are roughly balanced and over-sample a little bit. // 计算每个分区抽取的数据量大小， 假设输入数据每个分区分布的比较均匀 // 对于超大数据集(分区数超过5万的)乘以3会让数据稍微增大一点，对于分区数低于5万的数据集，每个分区抽取数据量为60条也不算多 val sampleSizePerPartition = math.ceil(3.0 * sampleSize / rdd.partitions.size).toInt // 从rdd中抽取数据，返回值:(总rdd数据量， Array[分区id，当前分区的数据量，当前分区抽取的数据]) val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition) if (numItems == 0L) &#123; // 如果总的数据量为0(RDD为空)，那么直接返回一个空的数组 Array.empty &#125; else &#123; // If a partition contains much more than the average number of items, we re-sample from it // to ensure that enough items are collected from that partition. // 计算总样本数量和总记录数的占比，占比最大为1.0 val fraction = math.min(sampleSize / math.max(numItems, 1L), 1.0) // 保存样本数据的集合buffer val candidates = ArrayBuffer.empty[(K, Float)] // 保存数据分布不均衡的分区id(数据量超过fraction比率的分区) val imbalancedPartitions = mutable.Set.empty[Int] // 计算抽取出来的样本数据 sketched.foreach &#123; case (idx, n, sample) =&gt; if (fraction * n &gt; sampleSizePerPartition) &#123; // 如果fraction乘以当前分区中的数据量大于之前计算的每个分区的抽象数据大小，那么表示当前分区抽取的数据太少了，该分区数据分布不均衡，需要重新抽取 imbalancedPartitions += idx &#125; else &#123; // 当前分区不属于数据分布不均衡的分区，计算占比权重，并添加到candidates集合中 // The weight is 1 over the sampling probability. val weight = (n.toDouble / sample.size).toFloat for (key &lt;- sample) &#123; candidates += ((key, weight)) &#125; &#125; &#125; // 对于数据分布不均衡的RDD分区，重新进行数据抽样 if (imbalancedPartitions.nonEmpty) &#123; // Re-sample imbalanced partitions with the desired sampling probability. // 获取数据分布不均衡的RDD分区，并构成RDD val imbalanced = new PartitionPruningRDD(rdd.map(_._1), imbalancedPartitions.contains) // 随机种子 val seed = byteswap32(-rdd.id - 1) // 利用rdd的sample抽样函数API进行数据抽样 val reSampled = imbalanced.sample(withReplacement = false, fraction, seed).collect() val weight = (1.0 / fraction).toFloat candidates ++= reSampled.map(x =&gt; (x, weight)) &#125; // 将最终的抽样数据计算出rangeBounds出来 RangePartitioner.determineBounds(candidates, partitions) &#125; &#125; &#125; // 下一个RDD的分区数量是rangeBounds数组中元素数量+ 1个 def numPartitions: Int = rangeBounds.length + 1 // 二分查找器，内部使用java中的Arrays类提供的二分查找方法 private var binarySearch: ((Array[K], K) =&gt; Int) = CollectionsUtils.makeBinarySearch[K] // 根据RDD的key值返回对应的分区id。从0开始 def getPartition(key: Any): Int = &#123; // 强制转换key类型为RDD中原本的数据类型 val k = key.asInstanceOf[K] var partition = 0 if (rangeBounds.length &lt;= 128) &#123; // If we have less than 128 partitions naive search // 如果分区数据小于等于128个，那么直接本地循环寻找当前k所属的分区下标 while (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123; partition += 1 &#125; &#125; else &#123; // Determine which binary search method to use only once. // 如果分区数量大于128个，那么使用二分查找方法寻找对应k所属的下标; // 但是如果k在rangeBounds中没有出现，实质上返回的是一个负数(范围)或者是一个超过rangeBounds大小的数(最后一个分区，比所有数据都大) partition = binarySearch(rangeBounds, k) // binarySearch either returns the match location or -[insertion point]-1 if (partition &lt; 0) &#123; partition = -partition - 1 &#125; if (partition &gt; rangeBounds.length) &#123; partition = rangeBounds.length &#125; &#125; // 根据数据排序是升序还是降序进行数据的排列，默认为升序 if (ascending) &#123; partition &#125; else &#123; rangeBounds.length - partition &#125; &#125; 采样总数在新的rangeBounds算法总，采样总数做了一个限制，也就是最大只采样1e6的样本（也就是1000000）： val sampleSize =math.min(20.0 * partitions, 1e6) 父RDD中每个分区采样样本数 按照我们的思路，正常情况下，父RDD每个分区需要采样的数据量应该是sampleSize/rdd.partitions.size，但是我们看代码的时候发现父RDD每个分区需要采样的数据量是正常数的3倍。 val sampleSizePerPartition = math.ceil(3.0* sampleSize / rdd.partitions.size).toInt 因为父RDD各分区中的数据量可能会出现倾斜的情况，乘于3的目的就是保证数据量小的分区能够采样到足够的数据，而对于数据量大的分区会进行第二次采样。 采样算法 这个地方就是RangePartitioner分区的核心了，其内部使用的就是水塘抽样，而这个抽样特别适合那种总数很大而且未知，并无法将所有的数据全部存放到主内存中的情况。也就是我们不需要事先知道RDD中元素的个数（不需要调用rdd.count()了！）。 水塘抽样 水塘抽样算法是用于解决，对于一个未知长度的数据流进行随机采样的问题的。 一个元素的时候：被抽中的概率为1两个元素的时候：第一个元素留下的概率为1-1/2，第二个元素留下的概率为 1/2三个元素的时候：第一个元素留下的概率为(1-1/2)(1-1/3)=1/3，第二个元素留下的概率为1/2(1-1/3)=1/3,第三个元素留下的概率为1/3以此类推，当判断到第i个元素时，第m（0&lt;m&lt;=i）个元素留下的概率为$1/m(1-1/(m+1))(1-1/(m+2))…(1-1/(i))=1/i$就可以保证所有元素的概率相等 真正的抽样算法在SamplingUtils中,由于在Spark中是需要一次性取多个值的，因此直接去前n个数值，然后依次概率替换即可 最后就可以通过获取的样本数据，确定边界了。 12// 从rdd中抽取数据，返回值:(总rdd数据量， Array[分区id，当前分区的数据量，当前分区抽取的数据]) val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition) 父RDD各分区中的数据量可能不均匀，在极端情况下，有些分区内的数据量会占有整个RDD的绝大多数的数据，如果按照水塘抽样进行采样，会导致该分区所采样的数据量不足，所以我们需要对该分区再一次进行采样，而这次采样使用的就是rdd的sample函数。 对于满足于fraction * n &gt; sampleSizePerPartition条件的分区，我们对其再一次采样。所有采样完的数据全部存放在candidates 中。 确认边界从上面的采样算法可以看出，对于不同的分区weight的值是不一样的（不同分区的元素数量不一样，但每个分区采样数是一样的），这个值对应的就是每个分区的采样间隔。 RangePartitioner的determineBounds函数的作用是根据样本数据记忆权重大小确定数据边界, 代码注释讲解如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243def determineBounds[K: Ordering : ClassTag]( candidates: ArrayBuffer[(K, Float)], partitions: Int): Array[K] = &#123; val ordering = implicitly[Ordering[K]] // 按照数据进行数据排序，默认升序排列 val ordered = candidates.sortBy(_._1) // 获取总的样本数量大小 val numCandidates = ordered.size // 计算总的权重大小 val sumWeights = ordered.map(_._2.toDouble).sum // 计算步长 val step = sumWeights / partitions var cumWeight = 0.0 var target = step val bounds = ArrayBuffer.empty[K] var i = 0 var j = 0 var previousBound = Option.empty[K] while ((i &lt; numCandidates) &amp;&amp; (j &lt; partitions - 1)) &#123; // 获取排序后的第i个数据及权重 val (key, weight) = ordered(i) // 累计权重 cumWeight += weight if (cumWeight &gt;= target) &#123; // Skip duplicate values. // 权重已经达到一个步长的范围，计算出一个分区id的值 if (previousBound.isEmpty || ordering.gt(key, previousBound.get)) &#123; // 上一个边界值为空，或者当前边界key数据大于上一个边界的值，那么当前key有效，进行计算 // 添加当前key到边界集合中 bounds += key // 累计target步长界限 target += step // 分区数量加1 j += 1 // 上一个边界的值重置为当前边界的值 previousBound = Some(key) &#125; &#125; i += 1 &#125; // 返回结果 bounds.toArray &#125; 自定义分区器自定义分区器，也是很简单的，只需要实现对应的两个方法就行： 12345678910111213141516171819202122232425public class MyPartioner extends Partitioner &#123; @Override public int numPartitions() &#123; return 1000; &#125; @Override public int getPartition(Object key) &#123; String k = (String) key; int code = k.hashCode() % 1000; System.out.println(k+":"+code); return code &lt; 0?code+1000:code; &#125; @Override public boolean equals(Object obj) &#123; if(obj instanceof MyPartioner)&#123; if(this.numPartitions()==((MyPartioner) obj).numPartitions())&#123; return true; &#125; return false; &#125; return super.equals(obj); &#125;&#125; 使用的时候，可以直接new一个对象即可。 1pairRdd.groupbykey(new MyPartitioner())]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark运行模式]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2Fspark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[spark运行模式Spark 有很多种模式，最简单就是单机本地模式，还有单机伪分布式模式，复杂的则运行在集群中，目前能很好的运行在 Yarn和 Mesos 中 local(本地模式)：常用于本地开发测试，本地还分为local单线程和local-cluster多线程; standalone(集群模式)：典型的Mater/slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HA on yarn(集群模式)： 运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算 on mesos(集群模式)： 运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算 on cloud(集群模式)：比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon的 S3;Spark 支持多种分布式存储系统：HDFS 和 S3 local本地单机模式本地单机模式下，所有的Spark进程均运行于同一个JVM中，并行处理则通过多线程来实现。在默认情况下，单机模式启动与本地系统的CPU核心数目相同的线程。如果要设置并行的级别，则以local[N]的格式来指定一个master变量，N表示要使用的线程数目。 运行该模式非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker守护进程( 只有集群的Standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS） 1spark-submit --class JavaWordCount --master local[10] JavaWordCount.jar file:///tmp/test.txt SparkSubmit进程既是客户提交任务的Client进程、又是Spark的driver程序、还充当着Spark执行Task的Executor角色。 因为driver程序在应用程序结束后就会终止，要想看webui可以Thread.sleep() 本地伪集群运行模式这种运行模式，和Local[N]很像，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景 用法是：提交应用程序时使用local-cluster[x,y,z]参数：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数。 1spark-submit --master local-cluster[2, 3, 1024] SparkSubmit依然充当全能角色，又是Client进程，又是driver程序 集群模式介绍为了运行在集群上，SparkContext 可以连接至几种类型的 Cluster Manager（既可以用 Spark 自己的 Standlone Cluster Manager，或者 Mesos，也可以使用 YARN），它们会分配应用的资源。一旦连接上，Spark 获得集群中节点上的 Executor，这些进程可以运行计算并且为您的应用存储数据。接下来，它将发送您的应用代码（通过 JAR 或者 Python 文件定义传递给 SparkContext）至 Executor。最终，SparkContext 将发送 Task 到 Executor 以运行。 每个应用获取到它自己的 Executor 进程，它们会保持在整个应用的生命周期中并且在多个线程中运行 Task（任务）。这样做的优点是把应用互相隔离，在调度方面（每个 driver 调度它自己的 task）和 Executor 方面（来自不同应用的 task 运行在不同的 JVM 中）。然而，这也意味着若是不把数据写到外部的存储系统中的话，数据就不能够被不同的 Spark 应用（SparkContext 的实例）之间共享。 Spark 是不知道底层的 Cluster Manager 到底是什么类型的。只要它能够获得 Executor 进程，并且它们可以和彼此之间通信，那么即使是在一个也支持其它应用的 Cluster Manager（例如，Mesos / YARN）上来运行它也是相对简单的。 Driver 程序必须在自己的生命周期内监听和接受来自它的 Executor 的连接请求。同样的，driver 程序必须可以从 worker 节点上网络寻址。 因为 driver 调度了集群上的 task（任务），更好的方式应该是在相同的局域网中靠近 worker 的节点上运行。 提交应用程序使用 spark-submit 脚本可以提交应用至任何类型的集群。 监控每个 driver 都有一个 Web UI，通常在端口 4040 上，可以显示有关正在运行的 task，executor，和存储使用情况的信息。 只需在 Web 浏览器中的http://&lt;driver-node&gt;:4040 中访问此 UI。 Job 调度Spark 即可以在应用间（Cluster Manager 级别），也可以在应用内（如果多个计算发生在相同的 SparkContext 上时）控制资源分配。 术语 Term（术语） Meaning（含义） Application 用户构建在 Spark 上的程序。由集群上的一个 driver 程序和多个 executor 组成。 Application jar 一个包含用户 Spark 应用的 Jar。有时候用户会想要去创建一个包含他们应用以及它的依赖的 “uber jar”。用户的 Jar 应该没有包括 Hadoop 或者 Spark 库，然而，它们将会在运行时被添加。 Driver program 该进程运行应用的 main() 方法并且创建了 SparkContext。 Cluster manager 一个外部的用于获取集群上资源的服务。（例如，Standlone Manager，Mesos，YARN） Deploy mode 根据 driver 程序运行的地方区别。在 “Cluster” 模式中，框架在群集内部启动 driver。在 “Client” 模式中，submitter（提交者）在 Custer 外部启动 driver。 Worker node 任何在集群中可以运行应用代码的节点。 Executor 一个为了在 worker 节点上的应用而启动的进程，它运行 task 并且将数据保持在内存中或者硬盘存储。每个应用有它自己的 Executor。 Task 一个将要被发送到 Executor 中的工作单元。 Job 一个由多个任务组成的并行计算，并且能从 Spark action 中获取响应（例如 save, collect）; 您将在 driver 的日志中看到这个术语。 Stage 每个 Job 被拆分成更小的被称作 stage（阶段） 的 task（任务） 组，stage 彼此之间是相互依赖的（与 MapReduce 中的 map 和 reduce stage 相似）。您将在 driver 的日志中看到这个术语。 standalone在执行应用程序前，先启动Spark的Master和Worker守护进程。 123spark-submit --master spark://wl1:7077或者 spark-submit --master spark://wl1:7077 --deploy-mode client 会在所有有Worker进程的节点上启动Executor来执行应用程序，此时产生的JVM进程如下：（非master节点，除了没有Master、SparkSubmit，其他进程都一样） Master进程做为cluster manager，用来对应用程序申请的资源进行管理； SparkSubmit 做为Client端和运行driver程序； CoarseGrainedExecutorBackend 用来并发执行应用程序； 注意，Worker进程生成几个Executor，每个Executor使用几个core，这些都可以在spark-env.sh里面配置 过程 SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）； Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend； StandaloneExecutorBackend向SparkContext注册； SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后以Stage（或者称为TaskSet）提交给Task Scheduler，Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行； StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成。 所有Task完成后，SparkContext向Master注销，释放资源。 YARNSpark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。 由于已经有了资源管理器，所以不需要启动Spark的Master、Worker守护进程。 123456789$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn \ # --deploy-mode client --deploy-mode cluster \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 1 \ --queue thequeue \ lib/spark-examples*.jar \ 10 YARN框架流程任何框架与YARN的结合，都必须遵循YARN的开发模式。 Yarn框架的基本运行流程图为： 其中，ResourceManager负责将集群的资源分配给各个应用使用，而资源分配和调度的基本单位是Container，其中封装了机器资源，如内存、CPU、磁盘和网络等，每个任务会被分配一个Container，该任务只能在该Container中执行，并使用该Container封装的资源。NodeManager是一个个的计算节点，主要负责启动Application所需的Container，监控资源（内存、CPU、磁盘和网络等）的使用情况并将之汇报给ResourceManager。ResourceManager与NodeManagers共同组成整个数据计算框架，ApplicationMaster与具体的Application相关，主要负责同ResourceManager协商以获取合适的Container，并跟踪这些Container的状态和监控其进度。 YARN-ClientYarn-Client模式中，Driver在客户端本地运行 YARN-client的工作流程分为以下几个步骤： Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend； ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派； Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）； 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task； Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 YARN-Cluster在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。 YARN-cluster的工作流程分为以下几个步骤： Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等； ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化； ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束； 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等； ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 YARN-Client 与 YARN-Cluster 区别理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。==在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器==。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。 YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业； YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。 MesosApache Mesos是一个通用的集群管理器 123456789./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master mesos://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ http://path/to/examples.jar \ 1000]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark简介]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2Fspark%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[spark简介Spark 是一个用来实现快速而通用的==集群计算的平台==，Spark简单说就是内存计算（包含迭代式计算，DAG计算,流式计算 ）框架 spark的优点有： 轻量级快速处理。Spark允许Hadoop集群中的应用程序在内存中以100倍的速度运行，即使在磁盘上运行也能快10倍。Spark通过减少磁盘IO来达到性能提升，它们将中间处理数据全部放到了内存中。 易于使用，Spark支持多语言。Spark允许Java、Scala及Python，自带了80多个高等级操作符，允许在shell中进行交互式查询。 支持复杂查询。在简单的“map”及“reduce”操作之外，Spark还支持SQL查询、流式查询及复杂查询，比如开箱即用的机器学习机图算法。 实时的流处理。 可以与Hadoop和已存Hadoop数据整合。 活跃和无限壮大的社区。 spark生态和组件 Spark Core: 包含spark的主要基本功能包括 任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core 中还包含了对弹性分布式数据集（resilient distributed dataset，简称RDD） Spark SQL: Spark中用于结构化数据处理的软件包. 用户可以在Spark环境下用SQL语言处理数据. Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。 Spark Streaming: Spark中用来处理流数据的部件 MLlib: Spark 中用来进行机器学习和数学建模的软件包 GraphX: Spark 中用来进行图计算的函数库 集群管理器 ，Spark 支持在各种集群管理器（cluster manager）上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度 器，叫作独立调度器。 Spark基本概念 RDD——Resillient Distributed Dataset A Fault-Tolerant Abstraction for In-Memory Cluster Computing弹性分布式数据集。 Operation——作用于RDD的各种操作分为transformation和action。 Transformations：转换(Transformations) (如：map, filter, groupBy, join等)，Transformations操作是Lazy的，也就是说从一个RDD转换生成另一个RDD的操作不是马上执行，Spark在遇到Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。 Actions：操作(Actions) (如：count, collect, save等)，Actions操作会返回结果或把RDD数据写到存储系统中。Actions是触发Spark启动计算的动因。 Job——作业，一个JOB包含多个RDD及作用于相应RDD上的各种operation。 Stage——一个作业分为多个阶段。 Partition——数据分区， 一个RDD中的数据可以分成多个不同的区。 DAG——Directed Acycle graph，有向无环图，反应RDD之间的依赖关系。 Narrow dependency——窄依赖，子RDD依赖于父RDD中固定的data partition。 Wide Dependency——宽依赖，子RDD对父RDD中的所有data partition都有依赖。 Caching Managenment——缓存管理，对RDD的中间计算结果进行缓存管理以加快整 体的处理速度。 spark架构Spark架构采用了分布式计算中的Master-Slave模型。Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行。具体如下图 在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。Driver程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Exucutor对相应数据分区的任务进行处理。 Spark的架构中的基本组件： ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。在YARN模式中为资源管理器。 Worker：从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制。 Driver :运行Application的main（）函数并创建SparkContext Executor：执行器，在worker node上执行任务的组件、用于启动线程池运行任务。每个Application拥有独立的一组Executors。 SparkContext：整个应用的上下文，控制应用的生命周期。 RDD:Spark的基本计算单元，一组RDD可形成执行的有向无环RDD Graph DAG Scheduler:根据作业(Job)构建基于Stage的DAG,并提交Stage给TaskScheduler TaskScheduler：将任务（Task）分发给Executor执行 SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。 SparkEnv内创建并包含如下的一些重要组件的引用。 MapOutPutTracker：负责Shuffle元信息的存储。 BroadcastManager：负责广播变量的控制与元信息的存储。 BlockManager：负责储存管理、创建和查找块。 MetricsSystem：监控运行时性能指标信息。 SparkConf：负责存储配置信息。 spark运行逻辑 在Spark应用中，整个执行流程在逻辑上会形成有向无环图（DAG）。Action算子触发之后，将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。Spark的调度方式与MapReduce有所不同。Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数执行流水线。图中的A、B、C、D、E、F分别代表不同的RDD，RDD内的方框代表分区。数据从HDFS输入Spark，形成RDD A和RDD C，RDD C上执行map操作，转换为RDD D， RDD B和 RDD E执行join操作，转换为F，而在B和E连接转化为F的过程中又会执行Shuffle，最后RDD F 通过函数saveAsSequenceFile输出并保存到HDFS或 Hbase中 wordcount示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.spark.appimport org.apache.spark.&#123;SparkContext, SparkConf&#125;object WordCount &#123; def main(args: Array[String]) &#123; /** * 第1步；创建Spark的配置对象SparkConf，设置Spark程序运行时的配置信息 * 例如 setAppName用来设置应用程序的名称，在程序运行的监控界面可以看到该名称， * setMaster设置程序运行在本地还是运行在集群中，运行在本地可是使用local参数，也可以使用local[K]/local[*], * 可以去spark官网查看它们不同的意义。 如果要运行在集群中，以Standalone模式运行的话，需要使用spark://HOST:PORT * 的形式指定master的IP和端口号，默认是7077 */ val conf = new SparkConf().setAppName("WordCount").setMaster("local")// val conf = new SparkConf().setAppName("WordCount").setMaster("spark://master:7077") // 运行在集群中 /** * 第2步：创建SparkContext 对象 * SparkContext是Spark程序所有功能的唯一入口 * SparkContext核心作用： 初始化Spark应用程序运行所需要的核心组件，包括DAGScheduler、TaskScheduler、SchedulerBackend * 同时还会负责Spark程序往Master注册程序 * * 通过传入SparkConf实例来定制Spark运行的具体参数和配置信息 */ val sc = new SparkContext(conf) /** * 第3步： 根据具体的数据来源(HDFS、 HBase、Local FS、DB、 S3等)通过SparkContext来创建RDD * RDD 的创建基本有三种方式： 根据外部的数据来源(例如HDFS)、根据Scala集合使用SparkContext的parallelize方法、 * 由其他的RDD操作产生 * 数据会被RDD划分成为一系列的Partitions，分配到每个Partition的数据属于一个Task的处理范畴 */ val lines = sc.textFile("D:/resources/README.md") // 读取本地文件// val lines = sc.textFile("/library/wordcount/input") // 读取HDFS文件，并切分成不同的Partition// val lines = sc.textFile("hdfs://master:9000/libarary/wordcount/input") // 或者明确指明是从HDFS上获取数据 /** * 第4步： 对初始的RDD进行Transformation级别的处理，例如 map、filter等高阶函数来进行具体的数据计算 */ val words = lines.flatMap(_.split(" ")).filter(word =&gt; word != " ") // 拆分单词，并过滤掉空格，当然还可以继续进行过滤，如去掉标点符号 val pairs = words.map(word =&gt; (word, 1)) // 在单词拆分的基础上对每个单词实例计数为1, 也就是 word =&gt; (word, 1) val wordscount = pairs.reduceByKey(_ + _) // 在每个单词实例计数为1的基础之上统计每个单词在文件中出现的总次数, 即key相同的value相加// val wordscount = pairs.reduceByKey((v1, v2) =&gt; v1 + v2) // 等同于 wordscount.collect.foreach(println) // 打印结果，使用collect会将集群中的数据收集到当前运行drive的机器上，需要保证单台机器能放得下所有数据 sc.stop() // 释放资源 &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark调度和任务分配]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2Fspark%E8%B0%83%E5%BA%A6%E5%92%8C%E4%BB%BB%E5%8A%A1%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[spark执行流程基本概念 Application：表示你的应用程序 Driver：表示main()函数，创建SparkContext。由SparkContext负责与ClusterManager通信，进行资源的申请，任务的分配和监控等。程序执行完毕后关闭SparkContext Executor：某个Application运行在Worker节点上的一个进程，该进程负责运行某些task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor对象，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task，这样，每个CoarseGrainedExecutorBackend能并行运行Task的数据就取决于分配给它的CPU的个数。 Worker：集群中可以运行Application代码的节点。在Standalone模式中指的是通过slave文件配置的worker节点，在Spark on Yarn模式中指的就是NodeManager节点。 Task：在Executor进程中执行任务的工作单元，多个Task组成一个Stage Job：包含多个Task组成的并行计算，是由Action行为触发的 Stage：每个Job会被拆分很多组Task，作为一个TaskSet，其名称为Stage DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系 TaskScheduler：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的。 Spark运行基本流程 构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； 资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上； SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。 Task在Executor上运行，运行完毕释放所有资源。 调度调度可以分为4个级别，Application调度、Job调度、Stage的调度、Task的调度与分发 应用调度一个executor在一个时间段内只能分配给一个应用使用。 Standalong，采用FIFO模式，每个应用会独占所有可用节点的资源 spark.cores.max：调整app可以在整个集群中申请的CPU core数量 spark.deploy.defaultCores：默认的CPU core数量 spark.executor.memory：限制每个Executor可用的内存 Mesos，用户可以通过参数设置所提交应用占用的资源 spark.mesos.coarse=true设置静态配置资源的策略 使用mesos://URL且不配置spark.mesos.coarse=true（每个app会有独立固定的内存分配，空闲时其他机器可以使用其资源） Yarn，可以指定为应用分配多少Executor，每个Executer的cpu和内存大小等，每个应用不会占用过多资源，提高yarn的吞吐量 通过–num-executors控制分配多少个Executor给app –executor-memory和–executor-cores分别控制Executor的内存和CPU core Job调度Spark应用程序内部，用户通过不同线程提交的Job可以并行运行，这里所说的Job就是Spark Action（如count、collect等）算子触发的整个RDD DAG为一个Job，在实现上，算子中的本质是调用SparkContext中的runJob提交了Job。 FIFO模式 第一个Job分配其所需的所有资源 第二个Job如果还有剩余资源的话就分配，否则等待 FAIR 使用轮询的方式调度Job 配置调度池默认新任务在 default pool中调度，可以在sc中设置： 1sc.setLocalProperty("spark.scheduler.pool","pool6") 设置该参数线程每次提交任务都放在这个池中调度，可以设置sc.setLocalProperty(&quot;spark.scheduler.pool&quot;,&quot;pool6&quot;)终止这个调度池的使用 默认每个调度池拥有相同优先级来共享整个集群的资源，可以通过一下配置 12345678910111213&lt;?xml version="1.0"?&gt;&lt;allocations&gt; &lt;pool name="production"&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;2&lt;/minShare&gt; &lt;/pool&gt; &lt;pool name="test"&gt; &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt;&lt;/allocations&gt; schedulingMode：FAIR或者FIFO。 weight： 权重，默认是1，设置为2的话，就会比其他调度池获得2x多的资源，如果设置为-1000，该调度池一有任务就会马上运行。 minShare： 最小共享核心数，默认是0，在权重相同的情况下，minShare大的，可以获得更多的资源。 我们可以通过spark.scheduler.allocation.file参数来设置这个文件的位置。 1System.setProperty(&quot;spark.scheduler.allocation.file&quot;, &quot;/path/to/file&quot;) Stage调度stage的生成由DAGScheduler完成。RDD的有向无环图DAG切分出了Stage的有向无环图DAG。Stage的DAG通过==最后执行的Stage为根==进行广度优先遍历，遍历到最开始执行的Stage执行，如果提交的Stage仍有未完成的父母Stage，则Stage需要等待其父Stage执行完才能执行。同时DAGScheduler中还维持了几个重要的Key-Value集合结构，用来记录Stage的状态，这样能够避免过早执行和重复提交Stage。waitingStages中记录仍有未执行的父母Stage，防止过早执行。runningStages中保存正在执行的Stage，防止重复执行。failedStages中保存执行失败的Stage，需要重新执行，这里的设计是出于容错的考虑。 依赖关系RDD的窄依赖是指父RDD的所有输出都会被指向的一个子RDD，即输出多对一；宽依赖是指父RDD的输出会指向不同的子RDD，即输出多对多。调度器会计算RDD之间的依赖关系，将拥有持续窄依赖的RDD归并到同一个Stage中，而宽依赖则作为划分不同Stage的判断标准。 导致窄依赖的Transformation操作：map、flatMap、filter、sample；导致宽依赖的Transformation操作：sortByKey、reduceByKey、groupByKey、cogroupByKey、join、cartensian。 Stage分为两种：ShuffleMapStagein which case its tasks’ results are input for another stage其实就是,非最终stage, 后面还有其他的stage, 所以它的输出一定是需要shuffle并作为后续的输入。 这种Stage是以Shuffle为输出边界，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出 其输出可以是另一个Stage的开始。 ShuffleMapStage的最后Task就是ShuffleMapTask。 在一个Job里可能有该类型的Stage，也可以能没有该类型Stage。 ResultStagein which case its tasks directly compute the action that initiated a job (e.g. count(), save(), etc)最终的stage, 没有输出, 而是直接产生结果或存储。 这种Stage是直接输出结果，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出。 ResultStage的最后Task就是ResultTask，在一个Job里必定有该类型Stage。 一个Job含有一个或多个Stage，但至少含有一个ResultStage。 Stage的划分RDD转换本身存在ShuffleDependency，像ShuffleRDD、CoGroupdRDD、SubtractedRDD会返回ShuffleDependency。 如果RDD中存在ShuffleDependency，就会创建一个新的Stage。Stage划分完毕就明确了以下内容： 产生的Stage需要从多少个Partition中读取数据 产生的Stage会生成多少Partition 产生的Stage是否属于ShuffleMap类型 确认Partition以决定需要产生多少不同的Task，ShuffleMap类型判断来决定生成的Task类型。Spark中有两种Task，分别是ShuffleMapTask和ResultTask。 Stage类stage的RDD参数只有一个RDD, final RDD, 而不是一系列的RDD。因为在==一个stage中的所有RDD都是map, partition不会有任何改变, 只是在data依次执行不同的map function==所以对于TaskScheduler而言, 一个RDD的状况就可以代表这个stage。 Stage参数说明：val id: Int //Stage的序号数值越大，优先级越高val rdd: RDD[_], //归属于本Stage的最后一个rddval numTasks: Int, //创建的Task数目，等于父RDD的输出Partition数目val shuffleDep: Option[ShuffleDependency[, , _]], //是否存在SuffleDependency，宽依赖val parents: List[Stage], //父Stage列表val jobId: Int //作业ID 处理Job，分割Job为Stage，封装Stage成TaskSet，最终提交给TaskScheduler的调用链dagScheduler.handleJobSubmitted–&gt;dagScheduler.submitStage–&gt;dagScheduler.submitMissingTasks–&gt;taskScheduler.submitTasks handleJobSubmitted函数函数handleJobSubmitted和submitStage主要负责依赖性分析，对其处理逻辑做进一步的分析。handleJobSubmitted最主要的工作是生成Stage，并根据finalStage来产生ActiveJob。 ==在创建一个Stage之前，需要知道该Stage需要从多少个Partition读入数据==，这个数值直接影响要创建多少个Task。也就是说，创建Stage时，已经清楚该Stage需要从多少不同的Partition读入数据，并写出到多少个不同的Partition中，输入和输出的个数均已明确。创建一个Stage的时候，通过不停的遍历它之前的rdd，如果碰到有依赖是ShuffleDependency类型的，就通过getShuffleMapStage方法计算出来它的Stage来。 ActiveJob类用户所提交的job在得到DAGScheduler的调度后，会被包装成ActiveJob，同时会启动JobWaiter阻塞监听job的完成状况。 同时依据job中RDD的dependency和dependency属性(NarrowDependency，ShufflerDependecy)，DAGScheduler会根据依赖关系的先后产生出不同的stage DAG(result stage, shuffle map stage)。 在每一个stage内部，根据stage产生出相应的task，包括ResultTask或是ShuffleMapTask，这些task会根据RDD中partition的数量和分布，产生出一组相应的task，并将其包装为TaskSet提交到TaskScheduler上去。 12345678910111213private[spark] class ActiveJob( val jobId: Int, val finalStage: Stage, val func: (TaskContext, Iterator[_]) =&gt; _, val partitions: Array[Int], val callSite: CallSite, val listener: JobListener, val properties: Properties) &#123; val numPartitions = partitions.length val finished = Array.fill[Boolean](numPartitions)(false) var numFinished = 0&#125; submitStage通过submitStage方法提交finalStage，方法会递归地将finalStage依赖的父stage先提交，最后提交finalStage，具体代码如下： 12345678910111213141516171819202122232425/** Submits stage, but first recursively submits any missing parents. */ private def submitStage(stage: Stage) &#123; val jobId = activeJobForStage(stage) if (jobId.isDefined) &#123; logDebug("submitStage(" + stage + ")") if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; //获取依赖的未提交的父stage val missing = getMissingParentStages(stage).sortBy(_.id) logDebug("missing: " + missing) if (missing.isEmpty) &#123; logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents") //如果父Stage都提交完成，则提交Stage submitMissingTasks(stage, jobId.get) &#125; else &#123; //如果有未提交的父Stage，则递归提交 for (parent &lt;- missing) &#123; submitStage(parent) &#125; waitingStages += stage &#125; &#125; &#125; else &#123; abortStage(stage, "No active job for stage " + stage.id, None) &#125; &#125; submitStage处理流程： 如果所有的依赖已经完成，则提交自身所处的Stage 最后会在submitMissingTasks函数中将stage封装成TaskSet通过taskScheduler.submitTasks函数提交给TaskScheduler处理。 dagScheduler.submitMissingTasks无论是哪种stage，都是对于每个stage中的每个partitions创建task，并最终封装成TaskSet，将该stage提交给taskscheduler。 taskScheduler.submitTasks见下文 Task调度stage划分完之后会以TaskSet的形式提交给我们的TaskScheduler。 TaskSetManager每个Stage对应的一个TaskSetManager通过Stage回溯到最源头缺失的Stage提交到==调度池pool==中，在调度池中，这些TaskSetMananger又会根据Job ID排序，先提交的Job的TaskSetManager优先调度，然后一个Job内的TaskSetManager ID小的先调度，并且==如果有未执行完的父母Stage的TaskSetManager，则是不会提交到调度池中。== 整体的Task分发由TaskSchedulerImpl来实现，但是Task的调度（本质上是Task在哪个分区执行）逻辑由TaskSetManager完成。这个类监控整个任务的生命周期，当任务失败时（如执行时间超过一定的阈值），重新调度，也会通过delay scheduling进行基于位置感知（locality-aware）的任务调度。 TaskSet的生成ShuffleMapStage 转化成 ShuffleMapTask ResultStage 转化成为 ResultTask TaskSet类定义如下： 1234567891011121314/** * A set of tasks submitted together to the low-level TaskScheduler, usually representing * missing partitions of a particular stage. */private[spark] class TaskSet( val tasks: Array[Task[_]], val stageId: Int, val stageAttemptId: Int, val priority: Int, val properties: Properties) &#123; val id: String = stageId + "." + stageAttemptId override def toString: String = "TaskSet " + id&#125; 生成方法是上面stage调度中的submitMissingTasks(stage: Stage, jobId: Int)，会设置每个任务累加变量，广播变量，任务序列化的二进制文件，任务执行位置等信息设置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** Called when stage's parents are available and we can now do its task. */ private def submitMissingTasks(stage: Stage, jobId: Int) &#123; // ... // First figure out the indexes of partition ids to compute. val (allPartitions: Seq[Int], partitionsToCompute: Seq[Int]) = &#123; stage match &#123; //在DAG Stage依赖关系中，除之后的Stage 外，全部为ShuffleMapStage //allPartitions为所有partion的ID //filteredPartitions为不在缓存中的partion ID case stage: ShuffleMapStage =&gt; val allPartitions = 0 until stage.numPartitions val filteredPartitions = allPartitions.filter &#123; id =&gt; stage.outputLocs(id).isEmpty &#125; (allPartitions, filteredPartitions) //在DAG Stage依赖关系中，最后的Stage为ResultStage case stage: ResultStage =&gt; val job = stage.resultOfJob.get val allPartitions = 0 until job.numPartitions val filteredPartitions = allPartitions.filter &#123; id =&gt; !job.finished(id) &#125; (allPartitions, filteredPartitions) &#125; &#125; // ... 设置累加变量 if (stage.internalAccumulators.isEmpty || allPartitions == partitionsToCompute) &#123; stage.resetInternalAccumulators() &#125; val properties = jobIdToActiveJob.get(stage.firstJobId).map(_.properties).orNull runningStages += stage //根据partitionsToCompute获取其优先位置PreferredLocations，使计算离数据最近 val taskIdToLocations = try &#123; stage match &#123; case s: ShuffleMapStage =&gt; partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap case s: ResultStage =&gt; val job = s.resultOfJob.get partitionsToCompute.map &#123; id =&gt; val p = job.partitions(id) (id, getPreferredLocs(stage.rdd, p)) &#125;.toMap &#125; &#125; catch &#123; //... return &#125; stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq) listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties)) // 原子广播变量设置 var taskBinary: Broadcast[Array[Byte]] = null try &#123; // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep). // For ResultTask, serialize and broadcast (rdd, func). val taskBinaryBytes: Array[Byte] = stage match &#123; case stage: ShuffleMapStage =&gt; closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef).array() case stage: ResultStage =&gt; closureSerializer.serialize((stage.rdd, stage.resultOfJob.get.func): AnyRef).array() &#125; taskBinary = sc.broadcast(taskBinaryBytes) &#125; catch &#123; // In the case of a failure during serialization, abort the stage. // ... &#125; //根据ShuffleMapTask或ResultTask，用于后期创建TaskSet val tasks: Seq[Task[_]] = try &#123; stage match &#123; case stage: ShuffleMapStage =&gt; partitionsToCompute.map &#123; id =&gt; val locs = taskIdToLocations(id) val part = stage.rdd.partitions(id) //创建ShuffleMapTask new ShuffleMapTask(stage.id, stage.latestInfo.attemptId, taskBinary, part, locs, stage.internalAccumulators) &#125; case stage: ResultStage =&gt; val job = stage.resultOfJob.get partitionsToCompute.map &#123; id =&gt; val p: Int = job.partitions(id) val part = stage.rdd.partitions(p) val locs = taskIdToLocations(id) //创建ResultTask new ResultTask(stage.id, stage.latestInfo.attemptId, taskBinary, part, locs, id, stage.internalAccumulators) &#125; &#125; &#125; catch &#123; case NonFatal(e) =&gt; abortStage(stage, s"Task creation failed: $e\n$&#123;e.getStackTraceString&#125;", Some(e)) runningStages -= stage return &#125; if (tasks.size &gt; 0) &#123; stage.pendingTasks ++= tasks //重要！创建TaskSet并使用taskScheduler的submitTasks方法提交Stage taskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptId, stage.firstJobId, properties)) stage.latestInfo.submissionTime = Some(clock.getTimeMillis()) &#125; else &#123; //提交完毕 markStageAsFinished(stage, None) &#125; &#125; Task提交上一节中TaskSet的生成说到最终stage被封装成TaskSet，使用taskScheduler.submitTasks提交，具体代码如下： 12taskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptId, stage.firstJobId, properties)) submitTasks方法定义在TaskScheduler Trait当中，目前TaskScheduler 只有一个子类TaskSchedulerImpl，其submitTasks方法源码如下： 123456789101112131415161718192021222324252627//TaskSchedulerImpl类中的submitTasks方法override def submitTasks(taskSet: TaskSet) &#123; val tasks = taskSet.tasks this.synchronized &#123; //创建TaskSetManager，TaskSetManager用于对TaskSet中的Task进行调度，包括跟踪Task的运行、Task失败重试等 val manager = createTaskSetManager(taskSet, maxTaskFailures) val stage = taskSet.stageId //schedulableBuilder中添加TaskSetManager，用于完成所有TaskSet的调度，即整个Spark程序生成的DAG图对应Stage的TaskSet调度,schedulableBuilder中包含调度池等信息 schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties) // 定时判断任务是否执行 if (!isLocal &amp;&amp; !hasReceivedTask) &#123; starvationTimer.scheduleAtFixedRate(new TimerTask() &#123; override def run() &#123; if (!hasLaunchedTask) &#123; logWarning("Initial job has not accepted any resources......") &#125; else &#123; this.cancel() &#125; &#125; &#125;, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS) &#125; hasReceivedTask = true &#125; //为Task分配运行资源 backend.reviveOffers() &#125; 将TaskSet 封装成TaskSetManager，通过schedulableBuilder去添加TaskSetManager到队列中，在Spark中，有两种形态 FIFOSchedulableBuilder: 单一pool FairSchedulableBuilder: 多个pool 在TaskSchedulerImpl在submitTasks添加TaskSetManager到pool后，调用了backend.reviveOffers(); ==向driver的endpoint发送了reviveoffers的消息，Spark中的许多操作都是通过消息来传递的，哪怕DAGScheduler的线程和endpoint的线程都是同一个Driver进程== Task的分配Netty 的dispatcher线程接受到revievoffers的消息后，CoarseGrainedSchedulerBackend 12case ReviveOffers =&gt; makeOffers() 调用了makeoffers函数 12345678private def makeOffers() &#123; // Filter out executors under killing val activeExecutors = executorDataMap.filterKeys(executorIsAlive) val workOffers = activeExecutors.map &#123; case (id, executorData) =&gt; new WorkerOffer(id, executorData.executorHost, executorData.freeCores) &#125;.toIndexedSeq launchTasks(scheduler.resourceOffers(workOffers)) &#125; makeOffers里进行了资源的调度，netty中接收所有的信息，同时也在CoarseGrainedSchedulerBackend中维护着executor的状态map:executorDataMap，executor的状态是executor主动汇报的。 通过scheduler.resourceOffers来进行task的资源分配到executor中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized &#123; // Mark each slave as alive and remember its hostname // Also track if new executor is added var newExecAvail = false for (o &lt;- offers) &#123; if (!hostToExecutors.contains(o.host)) &#123; hostToExecutors(o.host) = new HashSet[String]() &#125; if (!executorIdToRunningTaskIds.contains(o.executorId)) &#123; hostToExecutors(o.host) += o.executorId executorAdded(o.executorId, o.host) executorIdToHost(o.executorId) = o.host executorIdToRunningTaskIds(o.executorId) = HashSet[Long]() newExecAvail = true &#125; for (rack &lt;- getRackForHost(o.host)) &#123; hostsByRack.getOrElseUpdate(rack, new HashSet[String]()) += o.host &#125; &#125; // Randomly shuffle offers to avoid always placing tasks on the same set of workers. val shuffledOffers = Random.shuffle(offers) // Build a list of tasks to assign to each worker. val tasks = shuffledOffers.map(o =&gt; new ArrayBuffer[TaskDescription](o.cores)) val availableCpus = shuffledOffers.map(o =&gt; o.cores).toArray val sortedTaskSets = rootPool.getSortedTaskSetQueue for (taskSet &lt;- sortedTaskSets) &#123; logDebug("parentName: %s, name: %s, runningTasks: %s".format( taskSet.parent.name, taskSet.name, taskSet.runningTasks)) if (newExecAvail) &#123; taskSet.executorAdded() &#125; &#125; // Take each TaskSet in our scheduling order, and then offer it each node in increasing order // of locality levels so that it gets a chance to launch local tasks on all of them. // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY for (taskSet &lt;- sortedTaskSets) &#123; var launchedAnyTask = false var launchedTaskAtCurrentMaxLocality = false for (currentMaxLocality &lt;- taskSet.myLocalityLevels) &#123; do &#123; launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet( taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks) launchedAnyTask |= launchedTaskAtCurrentMaxLocality &#125; while (launchedTaskAtCurrentMaxLocality) &#125; if (!launchedAnyTask) &#123; taskSet.abortIfCompletelyBlacklisted(hostToExecutors) &#125; &#125; if (tasks.size &gt; 0) &#123; hasLaunchedTask = true &#125; return tasks &#125; 随机化了有效的executor的列表，为了均匀的分配 获取池里（前面已经提过油两种池）的排号序的taskSetManager的队列 对taskSetManager里面的task集合进行调度分配 taskSetManager队列的排序这里的排序是对单个Pool里的taskSetManager进行排序，Spark有两种排序算法 1234567891011var taskSetSchedulingAlgorithm: SchedulingAlgorithm = &#123; schedulingMode match &#123; case SchedulingMode.FAIR =&gt; new FairSchedulingAlgorithm() case SchedulingMode.FIFO =&gt; new FIFOSchedulingAlgorithm() case _ =&gt; val msg = "Unsupported scheduling mode: $schedulingMode. Use FAIR or FIFO instead." throw new IllegalArgumentException(msg) &#125; &#125; 单个TaskSetManager的task调度TaskSetManager 里保存了TaskSet，也就是DAGScheduler里生成的tasks的集合，在TaskSchedulerImpl.scala中进行了单个的TaskSetManager进行调度 在spark里，我们可以设置task所使用的cpu的数量，默认是1个，一个task任务在executor中是启动一个线程来执行的 通过计算每个executor的剩余资源，决定是否需要从tasksetmanager里分配出task. 在Spark中为了尽量分配任务到task所需的资源的本地,依据task里的preferredLocations所保存的需要资源的位置进行分配 尽量分配到task到task所需资源相同的executor里执行，比如ExecutorCacheTaskLocation，HDFSCacheTaskLocation 尽量分配到task里task所需资源相通的host里执行 task的数组从最后向前开始分配 分配完生成TaskDescription，里面记录着taskId, execId, task在数组的位置，和task的整个序列化的内容 driver启动Tasks1234567891011121314151617181920private def launchTasks(tasks: Seq[Seq[TaskDescription]]) &#123; for (task &lt;- tasks.flatten) &#123; val serializedTask = ser.serialize(task) if (serializedTask.limit &gt;= maxRpcMessageSize) &#123; scheduler.taskIdToTaskSetManager.get(task.taskId).foreach &#123; taskSetMgr =&gt; try &#123; var msg = "..." taskSetMgr.abort(msg) &#125; catch &#123; &#125; &#125; &#125; else &#123; val executorData = executorDataMap(task.executorId) executorData.freeCores -= scheduler.CPUS_PER_TASK executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))) &#125; &#125; &#125; TaskDescription里面包含着executorId，而CoarseGrainedSchedulerBackend里有executor的信息，根据executorId获取到executor的通讯端口，发送LunchTask的信息。 这里有个RPC的消息的大小控制，如果序列化的task的内容超过了最大RPC的消息，这个任务会被丢弃 Executor launch task启动任务Driver向Executor发送了LaunchTask的消息，Executor接收到了LaunchTask的消息后，进行了任务的启动，在CoarseGrainedExecutorBackend.scala 123456789case LaunchTask(data) =&gt; if (executor == null) &#123; exitExecutor(1, "Received LaunchTask command but executor was null") &#125; else &#123; val taskDesc = ser.deserialize[TaskDescription](data.value) logInfo("Got assigned task " + taskDesc.taskId) executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber, taskDesc.name, taskDesc.serializedTask) &#125; 接收消息，反序列化了TaskDescription的对象 在TaskDescription反序列化了taskId, executeId, name，index, attemptNumber, serializedTask属性，其中serializedTask是ByteBuffer。 Executor的launchTask方法 1234567891011def launchTask( context: ExecutorBackend, taskId: Long, attemptNumber: Int, taskName: String, serializedTask: ByteBuffer): Unit = &#123; val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName, serializedTask) runningTasks.put(taskId, tr) threadPool.execute(tr) &#125; 方法中通过线程池中启动了线程运行TaskRunner的任务 private val threadPool = ThreadUtils.newDaemonCachedThreadPool(&quot;Executor task launch worker&quot;) 关于线程池，在executor启动的是一个无固定大小线程数量限制的线程池，也就是说在executor的设计中，启动的任务数量是完全由Driver来管控 任务的运行TaskDescription中的serializedTask是个bytebuffer, 里面的结构如下图所示： 分别是task所依赖的文件的数量，文件的名字，时间戳，Jar的数量，Jar的名字，Jar的时间戳，属性，subBuffer是个bytebuffer 加载Jars文件Driver所运行的class等包括依赖的Jar文件在Executor上并不存在，Executor首先要fetch所依赖的jars，也就是TaskDescription中serializedTask中的jar部分 在上面的结构描述中，jar相关的只是numJars,jarName,timestamp并没有jar的内容，也就是在LaunchTask里的消息中并不携带Jar的内容，原因也很容易理解，rpc的消息体必须简单高效 timestamp:这是用于判断文件的时间戳，在相同文件名的情况下只有新的才需要重新fetch jarName: 这里的JarName是网络文件名：spark://192.168.121.101:37684/jars/spark-examples_2.11-2.1.0.jar 运行task 前面所提到的subBuffer实际上就是Task的序列化对象，通过反序列化可以获取到Driver生成的Task 在Executor.scala里的run方法中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Called by [[Executor]] to run this task. * 被Executor调用以执行Task * * @param taskAttemptId an identifier for this task attempt that is unique within a SparkContext. * @param attemptNumber how many times this task has been attempted (0 for the first attempt) * @return the result of the task along with updates of Accumulators. */ final def run( taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem) : (T, AccumulatorUpdates) = &#123; // 创建一个Task上下文实例：TaskContextImpl类型的context context = new TaskContextImpl( stageId, partitionId, taskAttemptId, attemptNumber, taskMemoryManager, metricsSystem, internalAccumulators, runningLocally = false) // 将context放入TaskContext的taskContext变量中 // taskContext变量为ThreadLocal[TaskContext] TaskContext.setTaskContext(context) // 设置主机名localHostName、内部累加器internalAccumulators等Metrics信息 context.taskMetrics.setHostname(Utils.localHostName()) context.taskMetrics.setAccumulatorsUpdater(context.collectInternalAccumulators) // task线程为当前线程 taskThread = Thread.currentThread() if (_killed) &#123;// 如果需要杀死task，调用kill()方法，且调用的方式为不中断线程 kill(interruptThread = false) &#125; try &#123; // 调用runTask()方法，传入Task上下文信息context，执行Task，并调用Task上下文的collectAccumulators()方法，收集累加器 (runTask(context), context.collectAccumulators()) &#125; finally &#123; // 上下文标记Task完成 context.markTaskCompleted() try &#123; Utils.tryLogNonFatalError &#123; // Release memory used by this thread for unrolling blocks // 为unrolling块释放当前线程使用的内存 SparkEnv.get.blockManager.memoryStore.releaseUnrollMemoryForThisTask() // Notify any tasks waiting for execution memory to be freed to wake up and try to // acquire memory again. This makes impossible the scenario where a task sleeps forever // because there are no other tasks left to notify it. Since this is safe to do but may // not be strictly necessary, we should revisit whether we can remove this in the future. val memoryManager = SparkEnv.get.memoryManager memoryManager.synchronized &#123; memoryManager.notifyAll() &#125; &#125; &#125; finally &#123; // 释放TaskContext TaskContext.unset() &#125; &#125; &#125; ​ 1、需要创建一个Task上下文实例，即TaskContextImpl类型的context，这个TaskContextImpl主要包括以下内容：Task所属Stage的stageId、Task对应数据分区的partitionId、Task执行的taskAttemptId、Task执行的序号attemptNumber、Task内存管理器taskMemoryManager、指标度量系统metricsSystem、内部累加器internalAccumulators、是否本地运行的标志位runningLocally（为false）； ​ 2、将context放入TaskContext的taskContext变量中，这个taskContext变量为ThreadLocal[TaskContext]； ​ 3、在任务上下文context中设置主机名localHostName、内部累加器internalAccumulators等Metrics信息； ​ 4、设置task线程为当前线程； ​ 5、如果需要杀死task，调用kill()方法，且调用的方式为不中断线程； ​ 6、调用runTask()方法，传入Task上下文信息context，执行Task，并调用Task上下文的collectAccumulators()方法，收集累加器； ​ 7、最后，任务上下文context标记Task完成，为unrolling块释放当前线程使用的内存，清楚任务上下文等。 runTask()Task共分为两种类型，一个是最后一个Stage产生的ResultTask，另外一个是其parent Stage产生的ShuffleMapTask ShuffleMapTask中的runTask()方法，定义如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445override def runTask(context: TaskContext): MapStatus = &#123; // Deserialize the RDD using the broadcast variable. // 使用广播变量反序列化RDD // 反序列化的起始时间 val deserializeStartTime = System.currentTimeMillis() // 获得反序列化器closureSerializer val ser = SparkEnv.get.closureSerializer.newInstance() // 调用反序列化器closureSerializer的deserialize()进行RDD和ShuffleDependency的反序列化，数据来源于taskBinary val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader) // 计算Executor进行反序列化的时间 _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime metrics = Some(context.taskMetrics) var writer: ShuffleWriter[Any, Any] = try &#123; // 获得shuffleManager val manager = SparkEnv.get.shuffleManager // 通过shuffleManager的getWriter()方法，获得shuffle的writer // 启动的partitionId表示的是当前RDD的某个partition,也就是说write操作作用于partition之上 writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context) // 针对RDD中的分区&lt;span style="font-family: Arial, Helvetica, sans-serif;"&gt;partition&lt;/span&gt;&lt;span style="font-family: Arial, Helvetica, sans-serif;"&gt;，调用rdd的iterator()方法后，再调用writer的write()方法，写数据&lt;/span&gt; writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &lt;: Product2[Any, Any]]]) // 停止writer，并返回标志位 writer.stop(success = true).get &#125; catch &#123; case e: Exception =&gt; try &#123; if (writer != ) &#123; writer.stop(success = false) &#125; &#125; catch &#123; case e: Exception =&gt; log.debug("Could not stop writer", e) &#125; throw e &#125; &#125; 通过使用广播变量反序列化得到RDD和ShuffleDependency： 获得反序列化的起始时间deserializeStartTime； 通过SparkEnv获得反序列化器ser； 调用反序列化器ser的deserialize()进行RDD和ShuffleDependency的反序列化，数据来源于taskBinary，得到rdd、dep； 计算Executor进行反序列化的时间_executorDeserializeTime； 利用shuffleManager的writer进行数据的写入： 通过SparkEnv获得shuffleManager； 通过shuffleManager的getWriter()方法，获得shuffle的writer，其中的partitionId表示的是当前RDD的某个partition,也就是说write操作作用于partition之上； 针对RDD中的分区partition，调用rdd的iterator()方法后，再调用writer的write()方法，写数据； 停止writer，并返回标志位。 ​ ResultTask，其runTask()方法更简单 12345678910111213141516171819202122override def runTask(context: TaskContext): U = &#123; // Deserialize the RDD and the func using the broadcast variables. // 获取反序列化的起始时间 val deserializeStartTime = System.currentTimeMillis() // 获取反序列化器 val ser = SparkEnv.get.closureSerializer.newInstance() // 调用反序列化器ser的deserialize()方法，得到RDD和FUNC，数据来自taskBinary val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader) // 计算反序列化时间_executorDeserializeTime _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime metrics = Some(context.taskMetrics) // 调针对RDD中的每个分区，迭代执行func方法，执行Task func(context, rdd.iterator(partition, context)) &#125; 通过SparkEnv获取反序列化器ser； 调用反序列化器ser的deserialize()方法，得到RDD和FUNC，数据来自taskBinary； 计算反序列化时间_executorDeserializeTime； 调针对RDD中的每个分区，迭代执行func方法，执行Task。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark shuffle]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2Fspark%20shuffle%2F</url>
    <content type="text"><![CDATA[spark shuffle对于大数据计算框架而言，Shuffle阶段的设计优劣是决定性能好坏的关键因素之一。 shuffle基本概念shuffle是一个算子，表达的是多对多的依赖关系，在MapReduce计算框架中，是连接Map阶段和Reduce阶段的纽带，即每个Reduce Task从每个Map Task产生数的据中读取一片数据。通常shuffle分为两部分：Map阶段的数据准备和Reduce阶段的数据拷贝。 Map阶段的数据准备Map阶段需根据Reduce阶段的Task数量决定每个Map Task输出的数据分片数目，有多种方式存放这些数据分片，不同的数据存放方式各有优缺点和适用场景。 一般而言，shuffle在Map端的数据要存储到磁盘上，以防止容错触发重算带来的庞大开销（如果保存到Reduce端内存中，一旦Reduce Task挂掉了，所有Map Task需要重算）。 数据在磁盘上存放方式有多种可选方案，在MapReduce前期设计中，每个Map Task为每个Reduce Task产生一个文件，该文件只保存特定Reduce Task需处理的数据，这样会产生M*R个文件，如果M和R非常庞大，比如均为1000，则会产生100w个文件，产生和读取这些文件会产生大量的随机IO，效率非常低下。解决这个问题的一种直观方法是减少文件数目，常用的方法有： 将一个节点上所有Map产生的文件合并成一个大文件（MapReduce现在采用的方案）， 每个节点产生{(slot数目)*R}个文件（Spark优化后的方案）。 不管是MapReduce 1.0还是Spark，每个节点的资源会被抽象成若干个slot，由于一个Task占用一个slot，因此slot数目可看成是最多同时运行的Task数目。如果一个Job的Task数目非常多，限于slot数目有限，可能需要运行若干轮。这样，只需要由第一轮产生{(slot数目)*R}个文件，后续几轮产生的数据追加到这些文件末尾即可。因此，后一种方案可减少大作业产生的文件数目。 Reduce阶段的数据拷贝在Reduce端，各个Task会并发启动多个线程同时从多个Map Task端拉取数据。在Reduce阶段的主要任务是对数据进行按组规约。也就是说，需要将数据分成若干组，以便以组为单位进行处理。大家知道，分组的方式非常多，常见的有： Map/HashTable（key相同的，放到同一个value list中）； Sort（按key进行排序，key相同的一组，经排序后会挨在一起）。 这两种方式各有优缺点：第一种复杂度低，效率高，但是需要将数据全部放到内存中；第二种方案复杂度高，但能够借助磁盘（外部排序）处理庞大的数据集。 Spark前期采用了第一种方案，而在最新的版本中加入了第二种方案，Hadoop MapReduce则从一开始就选用了基于sort的方案。下面将对其进行详细分析。 什么是Spark的Shuffle Spark有很多算子，比如：groupByKey、join等等都会产生shuffle。产生shuffle的时候，首先会产生Stage划分。 上一个Stage会把 计算结果放在LocalSystemFile中，并汇报给Driver；下一个Stage的运行由Driver触发，Executor向Driver请求，把上一个Stage的计算结果抓取过来。 Hadoop MapReduce Shuffle发展史 该图表达了Hadoop的map和reduce两个阶段，通过Shuffle怎样把map task的输出结果有效地传送到reduce端，描述着数据从map task输出到reduce task输入的这段过程。map的计算为reduce产生不同的文件，在Hadoop集群环境中，大部分map task与reduce task的执行是在不同的节点上，reduce执行时需要跨节点去拉取其它节点上的map task结果，那么对集群内部的网络资源消耗会很严重。我们希望最大化地减少不必要的消耗, 于是对Shuffle过程的期望有： 完整地从map task端拉取数据到reduce 端。 在跨节点拉取数据时，尽可能地减少对带宽的不必要消耗。 减少磁盘IO对task执行的影响。可优化的地方主要在于减少拉取数据的量及尽量使用内存而不是磁盘。 map端的Shuffle细节：整个map流程，简单些可以这样说：1）input, 根据split输入数据，运行map任务；2）patition, 每个map task都有一个内存缓冲区，存储着map的输出结果；3）spill, 当缓冲区快满的时候需要将缓冲区的数据以临时文件的方式存放到磁盘；4）merge, 当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。 map流程的细节：输入数据：在Map Reduce中，map task只读取split，Split与block的对应关系可能是多对一，默认是一对一； mapper运行后，通过Partitioner接口，根据key或value及reduce的数量来决定当前map的输出数据最终应该交由哪个reduce task处理。然后将数据写入内存缓冲区中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组； 内存缓冲区有大小限制，默认是100MB。需要在一定条件下将缓冲区中的数据临时写入磁盘，从内存往磁盘写数据的过程被称为Spill（溢写）； splill是由单独线程来完成，不影响往缓冲区写map结果的线程,splill的过程会涉及到Sort和Combiner，当splill线程启动后，需要对锁定内存块空间内的key做排序，是对序列化的字节做排序。 如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录，非正式地合并数据叫做combine了， Combiner会优化MapReduce的中间结果。 每次溢写会在磁盘上生成一个溢写文件，如果map的输出结果很大，就会有多个溢写文件存在。当map task完成时，内存缓冲区中的数据也全部溢写到磁盘中形成一个溢写文件。最终磁盘中会至少有一个这样的溢写文件存在(如果map的输出结果很少，当map执行完成时，只会产生一个溢写文件)，因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge。 至此，map端的所有工作都已结束，最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内。每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息，如果获知TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。 reduce 端的Shuffle细节：reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。 Copy过程，简单地拉取数据。 Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 Merge阶段。 这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。merge有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。 Reducer的输入文件。 不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，hadoop是把这个文件是存放于磁盘中的。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。 Hadoop的MapReduce Shuffle数据流动过程 图上map阶段，有4个map；Reduce端，有3个reduce。4个map 也就是4个JVM，每个JVM处理一个数据分片（split1~split4），每个map产生一个map输出文件，但是每个map都为后面的reduce产生了3部分数据（分别用红1、绿2、蓝3标识），也就是说每个输出的map文件都包含了3部分数据。正如前面第二节所述：mapper运行后，通过Partitioner接口，根据key或value及reduce的数量来决定当前map的输出数据最终应该交由哪个reduce task处理Reduce端一共有3个reduce，去前面的4个map的输出结果中抓取属于自己的数据。 Spark Shuffle 该图描述了最简单的Spark 0.X版本的Spark Shuffle过程。与Hadoop Map Reduce的区别在于输出文件个数的变化。 每个ShuffleMapTask产生与Ruducer个数相同的Shuffle blockFile文件，图中有3个reducer，那么每个ShuffleMapTask就产生3个Shuffle blockFile文件，4个ShuffleMapTask，那么一共产生12个Shuffle blockFile文件。 在内存中每个Shuffle blockFile文件都会存在一个句柄从而消耗一定内存，又因为物理内存的限制，就不能有很多并发，这样就限制了Spark集群的规模。该图描绘的只是Spark 0.X版本而已，让人误以为Spark不支持大规模的集群计算，当时这只是Hash Based Shuffle。Spark后来做了改进，引入了Sort Based Shuffle之后，就再也没有人说Spark只支持小规模的集群运算了。 Shuffle Read在几种模式中都差不多 Shuffle Read回忆一下，每个Stage的上边界，要么需要从外部存储读取数据，要么需要读取上一个Stage的输出；而下边界，要么是需要写入本地文件系统（需要Shuffle），以供childStage读取，要么是最后一个Stage，需要输出结果。这里的Stage，在运行时的时候就是可以以pipeline的方式运行的一组Task，除了最后一个Stage对应的是ResultTask，其余的Stage对应的都是ShuffleMap Task。 而除了需要从外部存储读取数据和RDD已经做过cache或者checkpoint的Task，一般Task的开始都是从ShuffledRDD的ShuffleRead开始的。 如果在本地有，那么可以直接从BlockManager中获取数据；如果需要从其他的节点上获取，那么需要走网络。 读取数据的主要流程： 获取待拉取数据的iterator； 使用AppendOnlyMap/ExternalAppendOnlyMap 做combine，这个过程和shuffle write一样； 如果需要对key排序，则使用ExternalSorter。 Hash based shuffleHash based shuffle的==每个mapper都需要为每个reducer写一个文件，供reducer读取，即需要产生M*R个数量的文件==，如果mapper和reducer的数量比较大，产生的文件数会非常多。Hadoop Map Reduce被人诟病的地方，很多不需要sort的地方的sort导致了不必要的开销，于是Spark的==Hash based shuffle设计的目标之一就是避免不需要的排序，但是它在处理超大规模数据集的时候，产生了大量的磁盘IO和内存的消耗，很影响性能。==Hash based shuffle不断优化，Spark 0.8.1引入的file consolidation在一定程度上解决了这个问题。 不需要排序的Hash Shuffle是否一定比需要排序的Sorted Shuffle速度更快？ ​ 不一定！如果数据规模比较小的情形下，Hash Shuffle会比Sorted Shuffle速度快（很多）！但是如果数据量大，此时Sorted Shuffle一般都会比Hash Shuffle快（很多），因为如果数据规模比较大，Hash Shuffle甚至无法处理，因为Hash Shuffle会产生很多的句柄，小文件，这时候磁盘和内存会变成瓶颈，而Sorted Shuffle就会极大的节省内存和磁盘的访问，所以更有利于更大规模的计算。Hash Shuffle适合中小型规模的数据计算。 每个ShuffleMapTask会根据key的哈希值计算出当前的key需要写入的Partition，然后把决定后的结果写入单独的文件，此时会导致==每个Task产生R（指下一个Stage的Task并行度）个文件==，如果当前的Stage中有M个ShuffleMapTask，则会产生M*R个文件！！！此时数据已经分好类了，==下一个Stage就会通过网络根据Driver端的注册信息，因为上一个Stage写过的内容会注册给Driver，然后向Driver获取上一个Stage的输出位置==，就会通过网络去读取数据，数据分成几种类型只跟下一个阶段分成多少个任务有关系，因为下一个阶段的任务数表示数据被分成多少类。跟并行度没有关系，也就是说跟实际并行运行多少任务没有关系。注意：Shuffle操作绝大多数情况下都要通过网络，如果Mapper和Reducer在同一台机器上，此时只需要读取本地磁盘即可。Spark中的Executor是线程池中的线程复用的，这个线程有可能运行上一个Stage的Task，也有可能运行下一个Stage的Task。 Hash Shuffle Writer实现解析Shuffle Map Task计算是调用ShuffleMapTask.runTask执行的。 核心代码如下： 获得ShuffleManager为 Hash Shuffle。 获得Hash Shuffle的Writer方法：HashShuffleWriter. 调用HashShuffleWriter的write方法。其中调用RDD的iterator方法计算，然后将结果传入给write。 12345val manager = SparkEnv.get.shuffleManager//获得ShuffleManager//获得Hash Shuffle的Writer方法writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)//调用HashShuffleWriter的write方法，writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &lt;: Product2[Any, Any]]]) 下面具体看write方法。 123456789101112131415/** Write a bunch of records to this task's output */override def write(records: Iterator[Product2[K, V]]): Unit = &#123;//判断aggregator是否被定义 val iter = if (dep.aggregator.isDefined) &#123;//判断数据是否需要聚合如果需要，聚合records if (dep.mapSideCombine) &#123; dep.aggregator.get.combineValuesByKey(records, context)//中间代码省略//elem是(K,V)形式的，通过K计算出bucketId for (elem &lt;- iter) &#123; val bucketId = dep.partitioner.getPartition(elem._1)//然后再通过bucketId具体写入那个partition//此时Shuffle是FileShuffleBlockResolver shuffle.writers(bucketId).write(elem._1, elem._2) &#125; 具体看一下FileShuffleBlockResolver.writers： 12345678val writers: Array[DiskBlockObjectWriter] = &#123; Array.tabulate[DiskBlockObjectWriter](numReducers) &#123; bucketId =&gt; val blockId = ShuffleBlockId(shuffleId, mapId, bucketId) val blockFile = blockManager.diskBlockManager.getFile(blockId) val tmp = Utils.tempFileWith(blockFile)//tmp也就是blockFile如果已经存在则，在后面追加数据 blockManager.getDiskWriter(blockId, tmp, serializerInstance, bufferSize, writeMetrics) &#125; blockManager.getDiskWriter就会为每个文件创建一个DiskBlockObjectWriter DiskBlockObjectWriter可以直接向一个在磁盘上的文件写数据，并且允许在后面追加数据 Hash Shuffle的两大死穴： Shuffle前会产生海量的小文件于磁盘之上，此时会产生大量耗时低效的IO操作； 内存不够用！！！由于内存中需要保存海量的文件操作句柄和临时缓存信息，如果数据处理规模比较庞大的话，内存不可承受，出现OOM等问题！ 进行HashShuffle的时候会根据后面的Task数，生成对应数量的小文件，而每个小文件也就是一种类型，在数据处理的时候，Task就从前面的小文件抓取需要的数据即可，它会导致同时打开过多的文件，这样就会占用过多的内存，写文件通过Write Handler默认是50KB。 Consalidate为了改善上述的问题（同时打开过多文件导致Writer Handler内存使用过大以及产生过度文件导致大量的随机读写带来的效率极为低下的磁盘IO操作），Spark后来推出了Consalidate机制，来把小文件(指的是每个Map都要为所有的Reducer产生Reducer个数的小文件)合并，此时Shuffle时文件产生的数量为cores*R，对于ShuffleMapTask的数量明显多于同时可用的并行Cores的数量的情况下，Shuffle产生的文件会大幅度减少，会极大降低OOM的可能； Consalidate机制：把同一个Task的输出变成一个文件进行合并，根据CPU的个数来决定具体产生多少文件，对于运行在同一个core的Shuffle Map Task，第一个Shuffle Map Task会创建一个，之后的就会将数据追加到这个文件上而不是新建一个文件。 但是在生成环境下，并行度特别大的话，还是会产生原来的问题。为此Spark推出了Shuffle Pluggable开放框架，方便系统升级的时候定制Shuffle功能模块，也方便第三方系统改造人员根据实际的业务场景来开发具体最佳的Shuffle模块；核心接口ShuffleManager，具体默认实现有HashShuffleManager、SortShuffleManager等 Sort based shuffle为了解决hash based shuffle性能差的问题，Spark 1.1 引入了Sort based shuffle，完全借鉴map reduce实现，每个Shuffle Map Task只产生一个文件，不再为每个Reducer生成一个单独的文件，将所有的结果只写到一个Data文件里，同时生成一个index文件，index文件存储了Data中的数据是如何进行分类的。Reducer可以通过这个index文件取得它需要处理的数据。 下一个Stage中的Task就是根据这个Index文件来获取自己所要抓取的上一个Stage中的Shuffle Map Task的输出数据。 Shuffle Map Task产生的结果只写到一个Data文件里, 避免产生大量的文件，从而节省了内存的使用和顺序Disk IO带来的低延时。节省内存的使用可以减少GC的风险和频率。 而减少文件的数量可以避免同时写多个文件对系统带来的压力。Sort based shuffle在速度和内存使用方面也优于Hash based shuffle。以上逻辑可以使用下图来描述： Sort based Shuffle包含两阶段的任务： 产生Shuffle数据的阶段(Map阶段) Write 对应的是ShuffleMapTask,具体的写操作ExternalSorter来负责，数据可以通过BlockManager写在内存、磁盘以及Tachyon等，例如想非常快的Shuffle，此时考虑可以把数据写在内存中，但是内存不稳定，建议采用内存+磁盘。 使用Shuffle数据的阶段(Reduce阶段) 需要实现ShuffleManager的getReader，Reader会向Driver去获取上一个Stage产生的Shuffle数据)。Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的 具体实现 ShuffleMapTask会根据Key，相应的Partition进行Sort，==如果属于同一个Partition本身不会进行Sort。== 在进行Sort的时候如果内存不够用的话，它会将哪些已经排序的数据写入到外部磁盘，结束的时候再进行归并排序，这时候基本上就不受内存限制了，同时由于在一个文件中，在一个File中，分为File不同的segment，为了高效的读取不同的segment，它就有一个index文件，会记录不同的Partition信息，BlockManager也会对它的寻址算法进行优化。 Sort Shuffle Writer对于每个Partition会创建一个数组，存储他所包含的Key,Value.每个需要处理的Key,Value,都会被插入到相应的数组中，如果数组的大小超过了具体规定大小的限定值的时候，就需要将内容写入到外部存储。 文件开始的部分记录我们这个Partition的ID，以及这个文件具体保存了有哪些数量的数据。 最后将所有写入到外部文件进行归并排序，归并排序的时候文件不能过多，如果过多的话会消耗很多的内存，可能会有OOM的风险，或者垃圾回收过多的风险，过少性能不好，会有延迟，最优一般同时打开10~100个文件。 最后生成文件的时候需要生成index索引文件，由于它已经排序好了索引文件，所以Reducer去读取索引文件的时候就会非常方便。 排序过程详解==Spark基于Sorted-Based Shuffle 它产出的结果是有序的。是错误的== Sorted-Based Shuffle 的核心是借助于 ExternalSorter 把每个 ShuffleMapTask 的输出，排序到一个文件中 (FileSegmentGroup)，为了区分下一个阶段 Reducer Task 不同的内容，它还需要有一个索引文件 (Index) 来告诉下游 Stage 的并行任务，那一部份是属于你的。 Shuffle Map Task 在ExternalSorter 溢出到磁盘的时候，产生一组 File （File Group是hashShuffle中的概念，理解为一个file文件池，这里为区分，使用File的概念，==FileSegment根据PartionID排序）==和 一个索引文件，File 里的 FileSegement 会进行排序，在 Reducer 端有4个Reducer Task，下游的 Task 可以很容易跟据索引 (index) 定位到这个 Fie 中的哪部份 FileSegement 是属于下游的，它相当于一个指针，下游的 Task 要向 Driver 去碓定文件在那里，然后到了这个 File 文件所在的地方，实际上会跟 BlockManager 进行沟通，BlockManager 首先会读一个 Index 文件，根据它的命名则进行解析，比如说下一个阶段的第一个 Task，一般就是抓取第一个 Segment，这是一个指针定位的过程。 Sort-Based Shuffle 最大的意义是减少临时文件的输出数量，且只会产生两个文件：一个是包含不同内容划分成不同 FileSegment 构成的单一文件 File，另外一个是索引文件 Index。 Sort-Based Shuffle Mapper端的 Sort and Spill 的过程 (ApendOnlyMap时不进行排序，Spill 到磁盘的时候再进行排序的) ExternalSorter.scala中有2个很重要的数据结构： 12@volatile private var map = new PartitionedAppendOnlyMap[K, C]@volatile private var buffer = new PartitionedPairBuffer[K, C] 在map端进行combine：PartitionedAppendOnlyMap 是map类型的数据结构，map是key-value ，在本地进行聚合，在本地key值不变，Value不断进行更新 在map端没有combine：使用PartitionedPairBuffer 排序：以partition ID进行排序，实现快速的写，方便的读操作；关键的一点对KEY进行操作。 12345678910111213private[spark] class PartitionedAppendOnlyMap[K, V] extends SizeTrackingAppendOnlyMap[(Int, K), V] with WritablePartitionedPairCollection[K, V] &#123; def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]]) : Iterator[((Int, K), V)] = &#123; val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator) destructiveSortedIterator(comparator) &#125; def insert(partition: Int, key: K, value: V): Unit = &#123; update((partition, key), value) &#125;&#125; Tungsten-sort Based ShuffleTungsten-sort 在特定场景下基于现有的Sort Based Shuffle处理流程，对内存/CPU/Cache使用做了非常大的优化。带来高效的同时，也就限定了自己的使用场景，所以Spark 默认开启的还是Sort Based Shuffle。 Tungsten 是钨丝的意思。 Tungsten Project 是 Databricks 公司提出的对Spark优化内存和CPU使用的计划，该计划初期对Spark SQL优化的最多，不过部分RDD API 还有Shuffle也因此受益。 Tungsten-sort是对普通sort的一种优化，排序的不是内容本身，而是内容序列化后字节数组的指针(元数据)，把数据的排序转变为了指针数组的排序，实现了直接对序列化后的二进制数据进行排序。由于直接基于二进制数据进行操作，所以在这里面没有序列化和反序列化的过程。内存的消耗降低，相应的也会减少gc的开销。 Tungsten-sort优化点主要在三个方面: 直接在serialized binary data上进行sort而不是java objects，减少了memory的开销和GC的overhead。 提供cache-efficient sorter，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序。 spill的merge过程也无需反序列化即可完成。 这些优化的实现导致引入了一个新的内存管理模型，类似OS的Page，Page是由MemoryBlock组成的, 支持off-heap(用NIO或者Tachyon管理) 以及 on-heap 两种模式。为了能够对Record 在这些MemoryBlock进行定位，又引入了Pointer的概念。 Sort Based Shuffle里存储数据的对象是PartitionedAppendOnlyMap,这只是一个放在JVM heap里普通对象。在Tungsten-sort中，它被替换成了类似操作系统内存页的对象。如果无法申请新的Page,这个时候就要执行spill溢写操作，将数据写到磁盘。具体触发条件和Sort Based Shuffle 类似。 开启条件 Spark 默认开启的是Sort Based Shuffle,想要打开Tungsten-sort ,请设置 1spark.shuffle.manager=tungsten-sort 对应的实现类是： 1org.apache.spark.shuffle.unsafe.UnsafeShuffleManager 数据一旦进来，就使用shuffle write进行序列化，在序列化的二进制基础上进行排序，这样就可以减少内存的GC。这种优化需要序列化器可以在不反序列化的情况下重新排序。 当且仅当下面条件都满足时，才能使用Tungsten-sort Shuffle: Shuffle 文件的数量不能大于 16777216 Shuffle 的序列化器需要是 KryoSerializer 或者 Spark SQL’s 自定义的一些序列化方式. 因为整个过程是追求不反序列化的，所以不能做aggregation Shuffle dependency 不能带有aggregation 或者输出不需要排序 序列化时，单条记录不能大于 128 MB Tungsten内存模型： 这张图其实画的是 on-heap 的内存逻辑图，#Page 部分为13bit, Offset 为51bit,你会发现 2^51 &gt;&gt;128M的。但是在Shuffle的过程中，对51bit 做了压缩，使用了27bit 具体如下： 12&gt; [24 bit partition number][13 bit memory page number][27 bit offset in page]&gt; 这里预留出的24bit给了partition number,为了后面的排序用。上面的好几个限制其实都是因为这个指针引起的： 一个是partition 的限制，前面的数字 16777216 就是来源于partition number 使用24bit 表示的。 第二个是page number 第三个是偏移量，最大能表示到2^27=128M。那一个task 能管理到的内存是受限于这个指针的，最多是 2^13 * 128M 也就是1TB左右。 对于第一个限制，那是因为后续Shuffle Write的sort 部分，只对前面24bit的partiton number 进行排序，key的值没有被编码到这个指针，所以没办法进行ordering 同时，因为整个过程是追求不反序列化的，所以不能做aggregation。 Shuffle Write数据会通过 UnsafeShuffleExternalSorter.insertRecordIntoSorter 一条一条写入到 serOutputStream 序列化输出流。 类似于Sort Based Shuffle 中的ExternalSorter，在Tungsten Sort 对应的为UnsafeShuffleExternalSorter,记录序列化后就通过sorter.insertRecord方法放到sorter里去了。 这里sorter 负责申请Page,释放Page,判断是否要进行spill都这个类里完成。代码的架子其实和Sort Based 是一样的。 算子示例combineByKey因为combineByKey是Spark中一个比较核心的高级函数，其他一些高阶键值对函数底层都是用它实现的。诸如 groupByKey,reduceByKey等等 1234567def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = ) createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值 mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并 mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更 多的分区都有对应同一个键的累加器， 就需要使用用户提供的mergeCombiners() 方法将各 个分区的结果进行合并。 partitioner：Partitioner（分区器），Shuffle时需要通过Partitioner的分区策略进行分区。 执行过程 shuffle write操作中，遇到新&lt;K,V&gt;，通过createCombiner生成累加器，遇到已经出现过的&lt;K,V&gt;，通过mergeValue进行合并，在这个过程中使用AppendOnlyMap/ExternalAppendOnlyMap 做combine，即相同的key合并 shuffle read操作中，通过mergeCombiners将不同分区mergeValue后的结果进行合并，获取数据后也会使用AppendOnlyMap/ExternalAppendOnlyMap 做combine，即相同的key合并 sortByKeysortByKey函数作用于Key-Value形式的RDD，并对Key进行排序。 1234567def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size) : RDD[(K, V)] =&#123; val part = new RangePartitioner(numPartitions, self, ascending) new ShuffledRDD[K, V, V](self, part) .setKeyOrdering(if (ascending) ordering else ordering.reverse)&#125; 该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，它可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark应用执行机制]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2Fspark%E5%BA%94%E7%94%A8%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[spark应用执行机制执行机制总览action算子触发job提交，提交到spark的job生成RDD DAG，经过DAGScheduler转化为stage DAG，每个stage中产生相应的task集合，taskscheduler讲任务分发到executor执行。每个任务对应相应的一个数据块，使用用户定义的函数处理数据。 spark为了系统的内存不会快速用完，使用延迟执行，只有操作累积到action，算子才会触发整个操作序列的执行，中间结果不会单独 重新分配内存，而是在同一个数据块上进行流水线操作。 对RDD的块管理通过BlockManager完成，BlockManager将数据抽象为数据块，在内存或者磁盘进行存储，如果数据不在本节点，则还可以通过远端节点复制到本机进行计算。 在计算节点的执行器Exector中会创建线程池，这个执行器将需要执行的任务通过线程池进行并发执行。 Spark应用概念 Application：Spark的应用程序，用户提交后，Spark为App分配资源，将程序转换并执行，其中Application包含一个Drive Program和若干个Executor。 Driver Progream： 运行Application的main()函数，并创建SparkContext。 SparkContext：Spark程序的入口，负责调度各个运算资源，协调各个worker Node上的Executor。 RDD Graph：RDD是Spark的核心结构，可以通过一系列算子进行操作（主要有Transformation和Action操作）。当RDD遇到Action算子时，将之前所有的算子形成一个有向无形环（DAG），再在Spark中转化为Job，提交到集群执行。一个App中可以包含多个Job。 Job：一个RDD Graph触发的作业，往往由Spark Action触发算子，在SparkContext中通过runJob方法想Spark提交Job。 Stage：每个Job会根据RDD的依赖关系被切分为很多歌Stage，每个Stage中会包含一组相同的Task，这一组Task也叫TaskSet Task：一个分区对应一个Task，Task执行RDD中对应的Stage包含的算子，Task被封装好后放入Exector的线程池中执行 DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskSheduler。 TaskScheduler：将TaskSet提交给worker Node集群并返回结果。 应用提交与执行方式以standalone为例 driver进程运行在客户端，对应用进行管理监控 启动集群，Master和Worker，Worker向Master注册 客户端启动后直接运行用户程序，启动Driver相关的工作：DAGScheduler和BlockManagerMaster等。客户端的Driver向Master注册。 Master让Worker启动Exeuctor。Worker创建一个ExecutorRunner线程，ExecutorRunner会启动ExecutorBackend进程。 ExecutorBackend启动后会向Driver的SchedulerBackend注册，Driver就可以找到计算资源。Driver的DAGScheduler解析作业并生成相应的Stage，每个Stage包含的Task通过TaskScheduler分配给Executor执行。所有stage都完成后作业结束 Driver在worker运行 启动集群，Master和Worker，Worker向Master注册 客户端提交作业给Master，Master让一个Worker启动Driver，即SchedulerBackend。Worker创建一个DriverRunner线程，DriverRunner启动SchedulerBackend进程。 其余同上 ​ Spark运行基本流程 构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； 资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上； SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。 Task在Executor上运行，运行完毕释放所有资源。 Spark运行架构特点： 每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。这种Application隔离机制有其优势的，无论是从调度角度看（每个Driver调度它自己的任务），还是从运行角度看（来自不同Application的Task运行在不同的JVM中）。当然，这也意味着Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。 Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了。 提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。 Task采用了数据本地性和推测执行的优化机制。 DAGSchedulerDAGScheduler把一个Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据RDD和Stage之间的关系找出开销最小的调度方法，然后把Stage以TaskSet的形式提交给TaskScheduler，下图展示了DAGScheduler的作用： TaskSchedulerDAGScheduler决定了运行Task的理想位置，并把这些信息传递给下层的TaskScheduler。此外，DAGScheduler还处理由于Shuffle数据丢失导致的失败，这有可能需要重新提交运行之前的Stage（非Shuffle数据丢失导致的Task失败由TaskScheduler处理） TaskScheduler维护所有TaskSet，当Executor向Driver发送心跳时，TaskScheduler会根据其资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行状态，重试失败的Task。下图展示了TaskScheduler的作用： 在不同运行模式中任务调度器具体为： Spark on Standalone模式为TaskScheduler； YARN-Client模式为YarnClientClusterScheduler YARN-Cluster模式为YarnClusterScheduler RDD运行原理高层次来看，主要分为三步： 创建 RDD 对象 DAGScheduler模块介入运算，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG 每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Storage]]></title>
    <url>%2F2018%2F05%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2FSpark%20Storage%2F</url>
    <content type="text"><![CDATA[Spark Storage在Spark中提及最多的是RDD，而RDD所交互的数据是通过Storage来实现和管理 Storage模块整体架构存储层在Spark里，单节点的Storage的管理是通过block来管理的，每个Block的存储可以在内存里或者在磁盘中，在BlockManager里既可以管理内存的存储，同时也管理硬盘的存储，存储的标识是通过块的ID来区分的。 集群下的架构在集群下Spark的Block的管理架构使用Master-Slave模式 Master : 拥有所有block的具体信息（本地和Slave节点） Slave ： 通过master获取block的信息，并且汇报自己的信息 这里的Master并不是Spark集群中分配任务的Master，而是==提交task的客户端Driver==，这里并没有主备设计，因为Driver client是单点的，通常Driver client crash了，计算也没有结果了，在Storage 的集群管理中Master是由driver承担。 在Executor在运行task的时候，通过blockManager获取本地的block块，如果本地找不到，尝试通过master去获取远端的块 Executor获取块内容的位置 唯一的blockID: broadcast_0_piece0请求Master获取该BlockID所在的 Location，也就是BlockManagerId的集合 1234/** Get locations of the blockId from the driver */ def getLocations(blockId: BlockId): Seq[BlockManagerId] = &#123; driverEndpoint.askWithRetrySeq[BlockManagerId] &#125; BlockManagerId BlockManagerId(driver, 192.168.121.101, 55153, None) Host： executor/driver IP Port: executor/driver Port 每一个executor, 和driver 都生成唯一的BlockManagerId Executor获取块的内容通过获取的BlockManagerId的集合列表，顺序的从列表中取出一个拥有该Block的服务器，通过 1blockTransferService.fetchBlockSync(loc.host, loc.port, loc.executorId, blockId.toString).nioByteBuffer() 同步的获取块的内容，如果该块不存在，则换下一个拥有该Block的服务器 BlockManager注册Driver 初始化SparkContext.init 的时候，会初始化BlockManager.initialize 1234val idFromMaster = master.registerBlockManager( id, maxMemory, slaveEndpoint) 会通过master 注册BlockManager 12345678910def registerBlockManager( blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId = &#123; logInfo(s"Registering BlockManager $blockManagerId") val updatedId = driverEndpoint.askWithRetry[BlockManagerId]( RegisterBlockManager(blockManagerId, maxMemSize, slaveEndpoint)) logInfo(s"Registered BlockManager $updatedId") updatedId &#125; 在BlockManagerMaster里，我们看到了endpoint是强制的driver，也就是默认是driver 是master 无论driver,还是executor都是初始化后BlockManager，发消息给driver master进行注册，唯一不同的是driver标识自己的ID是driver，而executor是按照executor id来标识自己的 Driver Master的endpoint无论driver还是executor 都会发送消息到Driver的Master，在Driver 和Executor里SparkEnv.create的时候会初始化BlockManagerMaster 1234val blockManagerMaster = new BlockManagerMaster(registerOrLookupEndpoint( BlockManagerMaster.DRIVER_ENDPOINT_NAME, new BlockManagerMasterEndpoint(rpcEnv, isLocal, conf, listenerBus)), conf, isDriver) 注册一个lookup的endpoint 12345678910def registerOrLookupEndpoint( name: String, endpointCreator: =&gt; RpcEndpoint): RpcEndpointRef = &#123; if (isDriver) &#123; logInfo("Registering " + name) rpcEnv.setupEndpoint(name, endpointCreator) &#125; else &#123; RpcUtils.makeDriverRef(name, conf, rpcEnv) &#125; &#125; 只有isDriver的时候才会setup一个rpc的endpoint，默认是netty的rpc环境，命名为：BlockManagerMaster spark://BlockManagerMaster@192.168.121.101:40978 所有的driver, executor都会向master 40978发消息 Master和Executor消息格式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = &#123; case RegisterBlockManager(blockManagerId, maxMemSize, slaveEndpoint) =&gt; context.reply(register(blockManagerId, maxMemSize, slaveEndpoint)) case _updateBlockInfo @ UpdateBlockInfo(blockManagerId, blockId, storageLevel, deserializedSize, size) =&gt; context.reply(updateBlockInfo(blockManagerId, blockId, storageLevel, deserializedSize, size)) listenerBus.post(SparkListenerBlockUpdated(BlockUpdatedInfo(_updateBlockInfo))) case GetLocations(blockId) =&gt; context.reply(getLocations(blockId)) case GetLocationsMultipleBlockIds(blockIds) =&gt; context.reply(getLocationsMultipleBlockIds(blockIds)) case GetPeers(blockManagerId) =&gt; context.reply(getPeers(blockManagerId)) case GetExecutorEndpointRef(executorId) =&gt; context.reply(getExecutorEndpointRef(executorId)) case GetMemoryStatus =&gt; context.reply(memoryStatus) case GetStorageStatus =&gt; context.reply(storageStatus) case GetBlockStatus(blockId, askSlaves) =&gt; context.reply(blockStatus(blockId, askSlaves)) case GetMatchingBlockIds(filter, askSlaves) =&gt; context.reply(getMatchingBlockIds(filter, askSlaves)) case RemoveRdd(rddId) =&gt; context.reply(removeRdd(rddId)) case RemoveShuffle(shuffleId) =&gt; context.reply(removeShuffle(shuffleId)) case RemoveBroadcast(broadcastId, removeFromDriver) =&gt; context.reply(removeBroadcast(broadcastId, removeFromDriver)) case RemoveBlock(blockId) =&gt; removeBlockFromWorkers(blockId) context.reply(true) case RemoveExecutor(execId) =&gt; removeExecutor(execId) context.reply(true) case StopBlockManagerMaster =&gt; context.reply(true) stop() case BlockManagerHeartbeat(blockManagerId) =&gt; context.reply(heartbeatReceived(blockManagerId)) case HasCachedBlocks(executorId) =&gt; blockManagerIdByExecutor.get(executorId) match &#123; case Some(bm) =&gt; if (blockManagerInfo.contains(bm)) &#123; val bmInfo = blockManagerInfo(bm) context.reply(bmInfo.cachedBlocks.nonEmpty) &#125; else &#123; context.reply(false) &#125; case None =&gt; context.reply(false) &#125; &#125; Master结构关系 在Master上会保存每一个executor所对应的BlockManagerID和BlockManagerInfo，而在BlockManagerInfo中保存了每个block的状态 Executor通过心跳主动汇报自己的状态，Master更新EndPoint中Executor的状态 Executor 中的block的状态更新也会汇报给Master，只是跟新Master状态，但不会通知其他的Executor 在Executor和Master交互中是Executor主动推和获取数据的，Master只是管理executor的状态，以及Block的所在的Driver、Executor的位置及其状态，负载较小，Master没有考虑可用性，通常Master节点就是提交任务的Driver的节点。 broadcastSpark BroadCastBroadcast 简单来说就是将数据从一个节点复制到其他各个节点，常见用于数据复制到节点本地用于计算，Block既可以保存在内存中，也可以保存在磁盘中，当Executor节点本地没有数据，通过Driver去获取数据 在Broadcast中，Spark只是传递只读变量的内容，通常如果一个变量更新会涉及到多个节点的该变量的数据同步更新，为了保证数据一致性，Spark在broadcast 中只传递不可修改的数据。 Broadcast 只是细粒度化到executor? 在storage前面的文章中讨论过BlockID 是以executor和实际的block块组合的，executor 是执行submit的任务的子worker进程，随着任务的结束而结束，对executor里执行的子任务是同一进程运行，数据可以进程内直接共享（内存），所以BroadCast只需要细粒度化到executor就足够了 TorrentBroadCastSpark在老的版本1.2中有HttpBroadCast，但在2.1版本中就移除了，HttpBroadCast 中实现的原理是每个executor都是通过Driver来获取Data数据，这样很明显的加大了Driver的网络负载和压力，无法解决Driver的单点性能问题。 为了解决Driver的单点问题，Spark使用了Block Torrent的方式。 \ Driver 初始化的时候，会知道有几个executor，以及Block, 最后在Driver端会生成block所对应的节点位置，初始化的时候因为executor没有数据，所有块的location都是Driver Executor 进行运算的时候，从BlockManager里的获取本地数据，如果本地数据不存在，然后从driver获取数据的位置 Driver里保存的块的位置只有Driver自己有，所以返回executer的位置列表只有driver 通过块的传输通道从Driver里获取到数据 获取数据后，使用BlockManager.putBytes保存数据 在保存数据后同时汇报该Block的状态到Driver Driver跟新executor 的BlockManager的状态，并且把Executor的地址加入到该BlockID的地址集合中 如何实现Torrent? 为了避免Driver的单点问题，在上面的分析中每个executor如果本地不存在数据的时候，通过Driver获取了该BlockId的位置的集合，executor获取到BlockId的地址集合随机化后，优先找同主机的地址（这样可以走回环），然后从随机的地址集合按顺序取地址一个一个尝试去获取数据，因为随机化了地址，那么executor不只会从Driver去获取数据 BlockID 的随机化 通常数据会被分为多个BlockID，取决于你设置的每个Block的大小 在获取完整的BlockID块的时候，在Torrent的算法中，随机化了BlockID 在任务启动的时候，新启的executor都会同时从driver去获取数据，大家如果都是以相同的Block的顺序，基本上的每个Block数据对executor还是会从Driver去获取， 而BlockID的简单随机化就可以保证每个executor从driver获取到不同的块，当不同的executor在取获取其他块的时候就有机会从其他的executor上获取到，从而分散了对Driver的负载压力。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop的io]]></title>
    <url>%2F2018%2F05%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fmapreduce%2Fhadoop%E7%9A%84io%2F</url>
    <content type="text"><![CDATA[hadoop的IO操作数据完整性检测数据是否损坏的常用措施是：在数据第一次引入系统时计算校验和并在数据通过一个不可靠的同道进行传输时再一次计算校验和，这样就能发现数据是否损坏。 HDFS的数据完整性 数据节点负责在存储数据及其校验和之前验证它们收到的数据。 从客户端和其它数据节点复制过来的数据。客户端写入数据并且将它发送到一个数据节点管线中，在==管线的最后一个数据节点验证校验和==。 客户端读取数据节点上的数据时，会验证校验和，将其与数据节点上存储的校验和进行对比。每个数据节点维护一个连续的校验和验证日志，因此它知道每个数据块最后验证的时间。 每个数据节点还会在后台线程运行一个DataBlockScanner（数据块检测程序），定期验证存储在数据节点上的所有块，为了防止物理存储介质中位衰减锁造成的数据损坏。 压缩文件压缩两大好处：减少存储文件所需要的空间且加快了数据在网络上或从磁盘上或到磁盘上的传输速度。 所有的要锁算法都要权衡时间/空间：压缩和解压缩的速度更快，其代价通常只能节省少量的时间，我们有9个不同的选项来控制压缩时必须考虑的权衡，-1为优化压缩速度，-9优化压缩空间。 当使用MapReduce处理压缩文件时，需要考虑压缩文件的可分割性。mapreduce一般用可分割的算法比较好 编码和解码编码和解码器用以执行压缩解压算法。在Hadoop中，编码和解码是通过一个压缩解码器接口实现的。 CompressionCodec有两个方法轻松地压缩和解压数据。使用createOutputStream(OutputStream out)创建一个CompressionOutputStream，将其以压缩格式写入底层的流。使用createInputStream(InputStream in) 获取一个CompressionInputStream，从底层的流读取未压缩的数据。 12345678910111213141516171819package com.laos.hadoop; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.CompressionCodec; import org.apache.hadoop.io.compress.CompressionOutputStream; import org.apache.hadoop.util.ReflectionUtils; public class StreamCompressor &#123; public static void main(String[] args) throws Exception &#123; String codecClassname = "org.apache.hadoop.io.compress.GzipCodec"; Class&lt;?&gt; codecClass = Class.forName(codecClassname); Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils .newInstance(codecClass, conf); //将读入数据压缩至System.out CompressionOutputStream out = codec.createOutputStream(System.out); IOUtils.copyBytes(System.in, out, 4096, false); out.finish(); &#125; &#125; 在阅读一个压缩文件时，我们可以从扩展名来推断出它的编码/解码器。以.gz结尾的文件可以用GzipCodec来阅读。CompressionCodecFactory提供了getCodec()方法，从而将文件扩展名映射到相应的CompressionCodec。 压缩和输入分隔考虑如何压缩哪些将由MapReduce处理的数据时，考虑压缩格式是否支持分隔很重要。 如果要压缩MapReduce作业的输出，设置mapred.output.compress为true，mapred.output.compression.codec属性指定编码解码器。 如果输入的文件时压缩过的，MapReduce读取时，它们会自动解压，根据文件扩展名来决定使用那一个压缩解码器。 1234567891011121314151617181920212223242526272829303132package com.laos.hadoop; import java.io.IOException; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.io.compress.CompressionCodec; import org.apache.hadoop.io.compress.GzipCodec; import org.apache.hadoop.mapred.FileInputFormat; import org.apache.hadoop.mapred.FileOutputFormat; import org.apache.hadoop.mapred.JobClient; import org.apache.hadoop.mapred.JobConf; public class MaxTemperatureWithCompression &#123; public static void main(String[] args) throws IOException &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperatureWithCompression &lt;input path&gt; " + "&lt;output path&gt;"); System.exit(-1); &#125; JobConf conf = new JobConf(MaxTemperatureWithCompression.class); conf.setJobName("Max temperature with output compression"); FileInputFormat.addInputPath(conf, new Path(args[0])); FileOutputFormat.setOutputPath(conf, new Path(args[1])); conf.setOutputKeyClass(Text.class); conf.setOutputValueClass(IntWritable.class); conf.setBoolean("mapred.output.compress", true); conf.setClass("mapred.output.compression.codec", GzipCodec.class, CompressionCodec.class); JobClient.runJob(conf); &#125; &#125; 序列化序列化就是把内存中的对象，转换成 字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。反序列化就是将收到 字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成 内存中的对象。 RPC序列化格式要求在Hadoop中，系统中多个节点上进程间的通信是通过“远程过程调用（RPC）”实现的。RPC协议将消息序列化成 二进制流后发送到远程节点，远程节点将二进制流反序列化为原始信息。通常情况下，RPC序列化格式如下： 紧凑（compact） 紧凑格式能充分利用网络带宽。 快速（Fast） 进程间通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本..最基本的。 可扩展（Extensible） 为了满足新的需求，协议不断变化。所以控制客户端和服务器的过程中，需要直接引进相应的协议。 支持互操作（Interoperable） 对于某些系统来说，希望能支持以不同语言写的客户端与服务器交互，所以需要设计需要一种特定的格式来满足这一需求。 JDK序列化和反序列化Serialization（序列化）是一种将对象转换为字节流；反序列化deserialization是一种将这些字节流生成一个对象。 a）当你想把的内存中的对象保存到一个文件中或者数据库中时候；b）当你想用套接字在网络上传送对象的时候；c）当你想通过RMI传输对象的时候； 将需要序列化的类实现Serializable接口就可以了，Serializable接口中没有任何方法，可以理解为一个标记，即表明这个类可以序列化。 Hadoop序列化和反序列化在hadoop中，hadoop实现了一套自己的序列化框架，相对于JDK比较简洁，在集群信息的传递上速度更快，容量更小。 在Java中将一个类写为可以序列化的类是实现Serializable接口，在Hadoop中将一个类写为可以序列化的类是实现Writable接口，它是一个最顶级的接口。 Hadoop对基本数据类型的包装所有Java基本类型的可写包装器，除了char（可以是存储在IntWritable中）。所有的都有一个get（）和set（）方法来检索和存储包装值。 Writable接口Writable接口定义了两个方法：一个将其状态写到DataOutput二进制流，另一个从DataInput二进制流读取状态。 123456789101112131415161718192021public class MyWritable implements Writable &#123; // Some data private int counter; private long timestamp; public void write(DataOutput out) throws IOException &#123; out.writeInt(counter); out.writeLong(timestamp); &#125; public void readFields(DataInput in) throws IOException &#123; counter = in.readInt(); timestamp = in.readLong(); &#125; public static MyWritable read(DataInput in) throws IOException &#123; MyWritable w = new MyWritable(); w.readFields(in); return w; &#125;&#125; Writable的继承关系 WritableComparable接口RawComparator接口除了Comparator中继承的两个方法，它自己也定义了一个方法有6个参数，这是在字节流的层面上去做比较。（第一个参数：指定字节数组，第二个参数：从哪里开始比较，第三个参数：比较多长） 基于文件的数据结构SequenceFileSequenceFile是一个由二进制序列化过的key/value的字节流组成的文本存储文件，它可以在map/reduce过程中的input/output 的format时被使用。在map/reduce过程中，map处理文件的临时输出就是使用SequenceFile处理过的。 所以一般的SequenceFile均是在FileSystem中生成，供map调用的原始文件。 特点SequenceFile是append only的，于是你不能对已存在的key进行写操作。 SequenceFile可以作为小文件的容器，可以通过它将小文件包装起来传递给MapReduce进行处理。 SequenceFile提供了两种定位文件位置的方法 seek(long poisitiuion):poisition必须是记录的边界，否则调用next()方法时会报错 sync(long poisition):Poisition可以不是记录的边界，如果不是边界，会定位到下一个同步点，如果Poisition之后没有同步点了，会跳转到文件的结尾位置 状态SequenceFile 有三种压缩态： Uncompressed – 未进行压缩的状态 record compressed - 对每一条记录的value值进行了压缩（文件头中包含上使用哪种压缩算法的信息） block compressed – 当数据量达到一定大小后，将停止写入进行整体压缩，整体压缩的方法是把所有的keylength,key,vlength,value 分别合在一起进行整体压缩，块的压缩效率要比记录的压缩效率高 格式每一个SequenceFile都包含一个“头”（header)。Header包含了以下几部分。 SEQ三个字母的byte数组 Version number的byte，目前为数字3的byte Key和Value的类名 压缩相关的信息 其他用户定义的元数据 同步标记，sync marker 对于每一条记录（K-V），其内部格式根据是否压缩而不同。SequenceFile的压缩方式有两种，“记录压缩”（record compression）和“块压缩”（block compression）。如果是记录压缩，则只压缩Value的值。如果是块压缩，则将多条记录一并压缩，包括Key和Value。具体格式如下面两图所示： 操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class SequenceFileDemo &#123; private static final String OPERA_FILE = "./output.seq"; /** * 随便从网上截取的一段文本 */ private static String[] testArray = &#123; "&lt;plugin&gt; ", " &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; ", " &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; ", " &lt;version&gt;1.7.7&lt;/version&gt; ", " &lt;executions&gt; ", " &lt;execution&gt; ", " &lt;phase&gt;generate-sources&lt;/phase&gt; ", " &lt;goals&gt; ", " &lt;goal&gt;schema&lt;/goal&gt; ", " &lt;/goals&gt; ", " &lt;configuration&gt; ", " &lt;sourceDirectory&gt;$&#123;project.basedir&#125;/src/main/avro/&lt;/sourceDirectory&gt; ", " &lt;outputDirectory&gt;$&#123;project.basedir&#125;/src/main/java/&lt;/outputDirectory&gt; ", " &lt;/configuration&gt; ", " &lt;/execution&gt; ", " &lt;/executions&gt; ", "&lt;/plugin&gt; ", "&lt;plugin&gt; ", " &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; ", " &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; ", " &lt;configuration&gt; ", " &lt;source&gt;1.6&lt;/source&gt; ", " &lt;target&gt;1.6&lt;/target&gt; ", " &lt;/configuration&gt; ", "&lt;/plugin&gt;"&#125;; public static void main(String[] args) throws IOException &#123; writeSequenceFile(OPERA_FILE); readSequenceFile(OPERA_FILE); &#125; private static void readSequenceFile(String inputFile) throws IOException &#123; Configuration config = new Configuration(); Path path = new Path(inputFile); SequenceFile.Reader reader = null; try &#123; FileSystem fs = FileSystem.get(URI.create(inputFile), config); reader = new SequenceFile.Reader(fs, path, config); IntWritable key = new IntWritable(); Text value = new Text(); long posion = reader.getPosition(); // reader.next()返回非空的话表示正在读，如果返回null表示已经读到文件结尾的地方 while (reader.next(key, value)) &#123; //打印同步点的位置信息 String syncMark = reader.syncSeen() ? "*" : ""; System.out.printf("[%s\t%s]\t%s\t%s\n", posion, syncMark, key, value); posion = reader.getPosition(); &#125; &#125; finally &#123; IOUtils.closeStream(reader); &#125; &#125; /** * 写Sequence File 文件 * * @param outputFile * @throws IOException */ private static void writeSequenceFile(String outputFile) throws IOException &#123; Configuration config = new Configuration(); Path path = new Path(outputFile); IntWritable key = new IntWritable(); Text value = new Text(); SequenceFile.Writer writer = null; try &#123; FileSystem fs = FileSystem.get(URI.create(outputFile), config); writer = SequenceFile.createWriter(fs, config, path, key.getClass(), value.getClass()); for (int i = 1; i &lt; 2000; i++) &#123; key.set(2000 - i); value.set(testArray[i % testArray.length]); System.out.printf("[%s]\t%s\t%s\n", writer.getLength() + "", key, value); // 通过Append方法进行写操作 writer.append(key, value); &#125; &#125; finally &#123; IOUtils.closeStream(writer); &#125; &#125;&#125; map fileMapFile是排序后的SequenceFile，MapFile由两部分组成分别是data和index。 index作为文件的数据索引，主要记录了每个Record的Key值，以及该Record在文件中的偏移位置。在MapFile被访问的时候，索引文件会被加载到内存，通过索引映射关系可以迅速定位到指定Record所在文件位置，因此，相对SequenceFile而言，MapFile的检索效率是最高的，缺点是会消耗一部分内存来存储index数据。 面向列Hadoop中的文件格式大致上分为面向行和面向列两类： 面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。 面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn基础]]></title>
    <url>%2F2018%2F05%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fyarn%2Fyarn%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[yarn基础yarn是hadoop集群资源管理系统。yarn提供请求和使用集群资源的API，但这些api很少直接用于用户代码，用户用的是分布式计算框架更高层的API，这些API建立在YARN上并隐藏了细节。]]></content>
      <categories>
        <category>大数据</category>
        <category>yarn</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn实现一个简单的Application框架]]></title>
    <url>%2F2018%2F05%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fyarn%2F%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84Application%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[主要接口涉及到的核心接口如下： ClientRM:YarnClient,封装了ApplicationClientProtocol协议。 AMRM:AMRMClientAsync,AMRMClientAsync.CallbackHandler，封装了ApplicationMasterProtocol协议。 AMNM:NMClientAsync,NMClientAsync.CallbackHandler，封装了ContainerManagementProtocol协议。 实现一个简单Application首先使用配置conf初始化一个YARN客户端： 123YarnClient yarnClient = YarnClient.createYarnClient();yarnClient.init(conf);yarnClient.start(); 初始化后，创建一个application： 12YarnClientApplication app = yarnClient.createApplication();GetNewApplicationResponse appResponse = app.getNewApplicationResponse(); 返回的Response中包含了一些集群信息，例如资源的最大最小容量，你要根据这些信息确保你的应用在申请资源的时候设置合理的值。 提交作业的核心是定义好ApplicationSubmissionContext： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();ApplicationId appId = appContext.getApplicationId();appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);appContext.setApplicationName(appName);//为Application Master设置本地资源 例如jar包，本地文件等Map&lt;String, LocalResource&gt; localResources = new HashMap&lt;String, LocalResource&gt;();LOG.info("Copy App Master jar from local filesystem and add to local environment");// 复制application master的jar包到本地环境（运行AM的container）// Create a local resource to point to the destination jar pathFileSystem fs = FileSystem.get(conf);addToLocalResources(fs, appMasterJar, appMasterJarPath, appId.toString(), localResources, null);// Set the log4j properties if neededif (!log4jPropFile.isEmpty()) &#123; addToLocalResources(fs, log4jPropFile, log4jPath, appId.toString(), localResources, null);&#125;// The shell script has to be made available on the final container(s)// where it will be executed.// To do this, we need to first copy into the filesystem that is visible// to the yarn framework.// We do not need to set this as a local resource for the application// master as the application master does not need it.String hdfsShellScriptLocation = "";long hdfsShellScriptLen = 0;long hdfsShellScriptTimestamp = 0;if (!shellScriptPath.isEmpty()) &#123; Path shellSrc = new Path(shellScriptPath); String shellPathSuffix = appName + "/" + appId.toString() + "/" + SCRIPT_PATH; Path shellDst = new Path(fs.getHomeDirectory(), shellPathSuffix); fs.copyFromLocalFile(false, true, shellSrc, shellDst); hdfsShellScriptLocation = shellDst.toUri().toString(); FileStatus shellFileStatus = fs.getFileStatus(shellDst); hdfsShellScriptLen = shellFileStatus.getLen(); hdfsShellScriptTimestamp = shellFileStatus.getModificationTime();&#125;if (!shellCommand.isEmpty()) &#123; addToLocalResources(fs, null, shellCommandPath, appId.toString(), localResources, shellCommand);&#125;if (shellArgs.length &gt; 0) &#123; addToLocalResources(fs, null, shellArgsPath, appId.toString(), localResources, StringUtils.join(shellArgs, " "));&#125;// 设置AM运行的环境变量LOG.info("Set the environment for the application master");Map&lt;String, String&gt; env = new HashMap&lt;String, String&gt;();// put location of shell script into env// using the env info, the application master will create the correct local resource for the// eventual containers that will be launched to execute the shell scriptsenv.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION, hdfsShellScriptLocation);env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP, Long.toString(hdfsShellScriptTimestamp));env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN, Long.toString(hdfsShellScriptLen));// Add AppMaster.jar location to classpath// At some point we should not be required to add// the hadoop specific classpaths to the env.// It should be provided out of the box.// For now setting all required classpaths including// the classpath to "." for the application jarStringBuilder classPathEnv = new StringBuilder(Environment.CLASSPATH.$$()) .append(ApplicationConstants.CLASS_PATH_SEPARATOR).append("./*");for (String c : conf.getStrings( YarnConfiguration.YARN_APPLICATION_CLASSPATH, YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH)) &#123; classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR); classPathEnv.append(c.trim());&#125;classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR).append( "./log4j.properties");// Set the necessary command to execute the application masterVector&lt;CharSequence&gt; vargs = new Vector&lt;CharSequence&gt;(30);// Set java executable commandLOG.info("Setting up app master command");vargs.add(Environment.JAVA_HOME.$$() + "/bin/java");// Set Xmx based on am memory sizevargs.add("-Xmx" + amMemory + "m");// 设置application maser的Main类vargs.add(appMasterMainClass);// Set params for Application Mastervargs.add("--container_memory " + String.valueOf(containerMemory));vargs.add("--container_vcores " + String.valueOf(containerVirtualCores));vargs.add("--num_containers " + String.valueOf(numContainers));vargs.add("--priority " + String.valueOf(shellCmdPriority));for (Map.Entry&lt;String, String&gt; entry : shellEnv.entrySet()) &#123; vargs.add("--shell_env " + entry.getKey() + "=" + entry.getValue());&#125;if (debugFlag) &#123; vargs.add("--debug");&#125;vargs.add("1&gt;" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/AppMaster.stdout");vargs.add("2&gt;" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/AppMaster.stderr");// Get final commmandStringBuilder command = new StringBuilder();for (CharSequence str : vargs) &#123; command.append(str).append(" ");&#125;LOG.info("Completed setting up app master command " + command.toString());List&lt;String&gt; commands = new ArrayList&lt;String&gt;();commands.add(command.toString());// Set up the container launch context for the application masterContainerLaunchContext amContainer = ContainerLaunchContext.newInstance( localResources, env, commands, null, null, null);// Set up resource type requirements// For now, both memory and vcores are supported, so we set memory and// vcores requirementsResource capability = Resource.newInstance(amMemory, amVCores);appContext.setResource(capability);// Service data is a binary blob that can be passed to the application// Not needed in this scenario// amContainer.setServiceData(serviceData);// Setup security tokensif (UserGroupInformation.isSecurityEnabled()) &#123; // Note: Credentials class is marked as LimitedPrivate for HDFS and MapReduce Credentials credentials = new Credentials(); String tokenRenewer = conf.get(YarnConfiguration.RM_PRINCIPAL); if (tokenRenewer == null | | tokenRenewer.length() == 0) &#123; throw new IOException( "Can't get Master Kerberos principal for the RM to use as renewer"); &#125; // For now, only getting tokens for the default file-system. final Token&lt;?&gt; tokens[] = fs.addDelegationTokens(tokenRenewer, credentials); if (tokens != null) &#123; for (Token&lt;?&gt; token : tokens) &#123; LOG.info("Got dt for " + fs.getUri() + "; " + token); &#125; &#125; DataOutputBuffer dob = new DataOutputBuffer(); credentials.writeTokenStorageToStream(dob); ByteBuffer fsTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength()); amContainer.setTokens(fsTokens);&#125;appContext.setAMContainerSpec(amContainer); 设置好Context之后，提交作业： 12345678// 设置application master的优先级Priority pri = Priority.newInstance(amPriority);appContext.setPriority(pri);// 设置应用程序要提交到RM的哪个队列queueappContext.setQueue(amQueue);yarnClient.submitApplication(appContext); 提交之后，可以有多种方式与RM保持通信： 12// 获取appId对应的状态报告ApplicationReport report = yarnClient.getApplicationReport(appId); 也可以杀死应用程序： 1yarnClient.killApplication(appId); 实现ApplicationMaster的关键步骤实现AM，其实就是实现应用程序运行中的主要步骤，就是上面介绍的8个步骤。 当RM分配一个Container并且启动了AM之后，AM可以获取到一些参数，例如container id，作业提交信息以及container的宿主信息等。与RM的所有交互都必须使用ApplicationAttempId，这个值可以从ContainerId获取： 12345678910Map&lt;String, String&gt; envs = System.getenv();String containerIdString = envs.get(ApplicationConstants.AM_CONTAINER_ID_ENV);if (containerIdString == null) &#123; // container id should always be set in the env by the framework throw new IllegalArgumentException( &quot;ContainerId not set in the environment&quot;);&#125;ContainerId containerId = ConverterUtils.toContainerId(containerIdString);ApplicationAttemptId appAttemptID = containerId.getApplicationAttemptId(); AM初始化之后，我们可以启动两个客户端，一个与RM通信，一个与NM通信，并且为客户端设置自己的回调处理方法： 123456789AMRMClientAsync.CallbackHandler allocListener = new RMCallbackHandler();amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);amRMClient.init(conf);amRMClient.start();containerListener = createNMCallbackHandler();nmClientAsync = new NMClientAsyncImpl(containerListener);nmClientAsync.init(conf);nmClientAsync.start(); AM需要向ResourceManager发送心跳以免RM认为我们挂掉，超期时间通过 YarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS设置，默认为YarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS。为了发送心跳，首先得向RM注册自己： 12345// Register self with ResourceManager// This will start heartbeating to the RMappMasterHostname = NetUtils.getHostname();RegisterApplicationMasterResponse response = amRMClient .registerApplicationMaster(appMasterHostname, appMasterRpcPort, appMasterTrackingUrl); 返回结果中包含资源的最大容量，可以用来check我们的资源请求是否合理。 1234567891011121314151617181920212223242526// Dump out information about cluster capability as seen by the// resource managerint maxMem = response.getMaximumResourceCapability().getMemory();LOG.info(&quot;Max mem capabililty of resources in this cluster &quot; + maxMem);int maxVCores = response.getMaximumResourceCapability().getVirtualCores();LOG.info(&quot;Max vcores capabililty of resources in this cluster &quot; + maxVCores);// A resource ask cannot exceed the max.if (containerMemory &gt; maxMem) &#123; LOG.info(&quot;Container memory specified above max threshold of cluster.&quot; + &quot; Using max value.&quot; + &quot;, specified=&quot; + containerMemory + &quot;, max=&quot; + maxMem); containerMemory = maxMem;&#125;if (containerVirtualCores &gt; maxVCores) &#123; LOG.info(&quot;Container virtual cores specified above max threshold of cluster.&quot; + &quot; Using max value.&quot; + &quot;, specified=&quot; + containerVirtualCores + &quot;, max=&quot; + maxVCores); containerVirtualCores = maxVCores;&#125;List&lt;Container&gt; previousAMRunningContainers = response.getContainersFromPreviousAttempts();LOG.info(&quot;Received &quot; + previousAMRunningContainers.size() + &quot; previous AM&apos;s running containers on AM registration.&quot;); 接着我们可以根据需要向RM申请资源了： 12345678910111213141516171819List&lt;Container&gt; previousAMRunningContainers = response.getContainersFromPreviousAttempts();List&lt;Container&gt; previousAMRunningContainers = response.getContainersFromPreviousAttempts();LOG.info(&quot;Received &quot; + previousAMRunningContainers.size() + &quot; previous AM&apos;s running containers on AM registration.&quot;);int numTotalContainersToRequest = numTotalContainers - previousAMRunningContainers.size();// Setup ask for containers from RM// Send request for containers to RM// Until we get our fully allocated quota, we keep on polling RM for// containers// Keep looping until all the containers are launched and shell script// executed on them ( regardless of success/failure).for (int i = 0; i &lt; numTotalContainersToRequest; ++i) &#123; ContainerRequest containerAsk = setupContainerAskForRM(); amRMClient.addContainerRequest(containerAsk);&#125; setupContainerAskForRM方法中，需要设置资源容量以及优先级： 12345678910111213141516private ContainerRequest setupContainerAskForRM() &#123; // setup requirements for hosts // using * as any host will do for the distributed shell app // set the priority for the request Priority pri = Priority.newInstance(requestPriority); // Set up resource type requirements // For now, memory and CPU are supported so we set memory and cpu requirements Resource capability = Resource.newInstance(containerMemory, containerVirtualCores); ContainerRequest request = new ContainerRequest(capability, null, null, pri); LOG.info(&quot;Requested container ask: &quot; + request.toString()); return request;&#125; 资源申请提交后，由于分配时异步的，因此需要设置好回调，通过实现AMRMClientAsync.CallbackHandler来完成： 1234567891011121314151617@Overridepublic void onContainersAllocated(List&lt;Container&gt; allocatedContainers) &#123; LOG.info(&quot;Got response from RM for container ask, allocatedCnt=&quot; + allocatedContainers.size()); numAllocatedContainers.addAndGet(allocatedContainers.size()); for (Container allocatedContainer : allocatedContainers) &#123; LaunchContainerRunnable runnableLaunchContainer = new LaunchContainerRunnable(allocatedContainer, containerListener); Thread launchThread = new Thread(runnableLaunchContainer); // launch and start the container on a separate thread to keep // the main thread unblocked // as all containers may not be allocated at one go. launchThreads.add(launchThread); launchThread.start(); &#125;&#125; 同时也要通过心跳发送进度： 1234567@Overridepublic float getProgress() &#123; // set progress to deliver to RM on next heartbeat float progress = (float) numCompletedContainers.get() / numTotalContainers; return progress;&#125; 上面的Container启动线程在NM上启动container，在启动之前，需要准备好ContainerLaunchContext： 1234567891011121314151617181920212223242526272829303132333435363738// Set the necessary command to execute on the allocated containerVector&lt;CharSequence&gt; vargs = new Vector&lt;CharSequence&gt;(5);// Set executable commandvargs.add(shellCommand);// Set shell script pathif (!scriptPath.isEmpty()) &#123; vargs.add(Shell.WINDOWS ? ExecBatScripStringtPath : ExecShellStringPath);&#125;// Set args for the shell command if anyvargs.add(shellArgs);// Add log redirect paramsvargs.add(&quot;1&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR + &quot;/stdout&quot;);vargs.add(&quot;2&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR + &quot;/stderr&quot;);// Get final commmandStringBuilder command = new StringBuilder();for (CharSequence str : vargs) &#123; command.append(str).append(&quot; &quot;);&#125;List&lt;String&gt; commands = new ArrayList&lt;String&gt;();commands.add(command.toString());// Set up ContainerLaunchContext, setting local resource, environment,// command and token for constructor.// Note for tokens: Set up tokens for the container too. Today, for normal// shell commands, the container in distribute-shell doesn&apos;t need any// tokens. We are populating them mainly for NodeManagers to be able to// download anyfiles in the distributed file-system. The tokens are// otherwise also useful in cases, for e.g., when one is running a// &quot;hadoop dfs&quot; command inside the distributed shell.ContainerLaunchContext ctx = ContainerLaunchContext.newInstance( localResources, shellEnv, commands, null, allTokens.duplicate(), null);containerListener.addContainer(container.getId(), container); 准备好之后，通过NMClientAsync启动： 1nmClientAsync.startContainerAsync(container, ctx); NMClientAsync及其相应的Callback，会处理容器发布的各种事件，例如启动、停止，状态更新等。 当ApplicationMaster确认自己的任务完成后，向RM注销并关闭客户端： 123456789try &#123; amRMClient.unregisterApplicationMaster(appStatus, appMessage, null);&#125; catch (YarnException ex) &#123; LOG.error(&quot;Failed to unregister application&quot;, ex);&#125; catch (IOException e) &#123; LOG.error(&quot;Failed to unregister application&quot;, e);&#125;amRMClient.stop();]]></content>
      <categories>
        <category>大数据</category>
        <category>yarn</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS HA]]></title>
    <url>%2F2018%2F05%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhdfs%2FHDFS%20HA%2F</url>
    <content type="text"><![CDATA[5 Hdfs HA背景：hdfs集群SPOF，主要在两种情况下： 突发事件如断电等 系统升级 在同一个集群中运行两个(以及两个以上)冗余的NameNodes。这样可以在机器崩溃或者为计划维护的情况下快速转移到新的NameNode 可通过Quorum Journal Manager或NFS 实现 架构只有一个活动namenode负责处理客户端请求，其他namenode仅用来快速恢复 所有的节点通过JournalNodes(jns)通信，active namenode会把所有日志记录到JNs上，Standby node会不断监控并更新editlog，active namenode故障时，Standby node会确保读取了所有日志，才切换到active namenode 为了提供快速故障转移，还需要备用节点具有关于集群中块位置的最新信息。datanode配置了所有的NameNodes的位置，并发送块位置信息和心跳。 为防止“split-brain scenario”，JNs只允许有一个writer]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn架构]]></title>
    <url>%2F2018%2F05%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fyarn%2Fyarn%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[yarn架构YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。其核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。 YARN可以将多种计算框架(如离线处理MapReduce、在线处理的Storm、迭代式计算框架Spark、流式处理框架S4等) 部署到一个公共集群中，共享集群的资源。并提供如下功能： 资源的统一管理和调度：集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。 资源隔离：YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。 YARN由一个ResourceManager和多个NodeManager组成， ResourceManager负责管理所有NodeManger上多维度资源， 并以Container(启动一个Container相当于启动一个进程)方式分配给应用程序启动ApplicationMaster(相当于主进程中运行逻辑) 或运行ApplicationMaster切分的各Task(相当于子进程中运行逻辑)。 YARN体系架构 YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。 ResourceManager(RM)负责对各NM上的资源进行统一管理和调度。将AM分配空闲的Container运行并监控其运行状态。对AM申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器和应用程序管理器： 调度器(Scheduler)：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Shceduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。调度器是可插拔的，例如CapacityScheduler、FairScheduler。具体看下文的调度算法。 应用程序管理器(Applications Manager)：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动AM、监控AM运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。 NodeManager (NM)NM是每个节点上的资源和任务管理器。它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM的Container 启动/停止等请求。 ApplicationMaster (AM)：用户提交的应用程序均包含一个AM，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。如果需要，我们也可以自己写一个符合规范的YARN application。 Container：Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。 YARN应用工作流程如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 启动AM ，如下步骤1~3； 由AM创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。 YARN应用工作流程图 用户向YARN中提交应用程序，其中包括AM程序、启动AM的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。 RM为该应用程序分配第一个Container，并与对应的NM通信，要求它在这个Container中启动AM； AM首先向RM注册，这样用户可以直接通过RM査看应用程序的运行状态，运行状态通过 AMRMClientAsync.CallbackHandler的getProgress() 方法来传递给RM。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7； AM采用轮询的方式通过RPC协议向RM申请和领取资源；资源的协调通过 AMRMClientAsync异步完成,相应的处理方法封装在AMRMClientAsync.CallbackHandler中。 —旦AM申请到资源后，便与对应的NM通信，要求它启动任务；通常需要指定一个ContainerLaunchContext，提供Container启动时需要的信息。 NM为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务； 各个任务通过某个RPC协议向AM汇报自己的状态和进度，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NM的通信通过NMClientAsync object来完成，容器的所有事件通过NMClientAsync.CallbackHandler来处理。例如启动、状态更新、停止等。 应用程序运行完成后，AM向RM注销并关闭自己。 YARN资源调度模型YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container。 YARN采用了双层资源调度模型： RM中的资源调度器将资源分配给各个AM：资源分配过程是异步的。资源调度器将资源分配给一个应用程序后，它不会立刻push给对应的AM，而是暂时放到一个缓冲区中，等待AM通过周期性的心跳主动来取； AM领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。 也就是说，ResourceManager分配集群资源的时候，以抽象的Container形式分配给各应用程序，至于应用程序的子任务如何使用这些资源，由应用程序自行决定。 Capacity Scheduler：该调度器用于在共享、多租户（multi-tenant）的集群环境中运行Hadoop应用，对运营尽可能友好的同时最大化吞吐量和效用。 该调度器保证共享集群的各个组织能够得到容量的保证，同时可以超额使用集群中暂时没有人使用的资源。Capacity Scheduler为了实现这些目标，抽象了queue的概念，queue通常由管理员配置。为了进一步细分容量的使用，调度器支持层级化的queue（hierarchical queues），使得在特定组织内部，可以进一步有效利用集群资源。 Capacity调度器支持的一些特性如下： 层级队列（Hierarchical Queues） 容量保证 安全性：每个队列都有队列的访问权限控制（ACL） 弹性： 空闲资源可以额外分配给任何需要的队列 多租户 基于资源的调度（resouce-based scheduling): 对资源敏感的应用程序，可以有效地控制资源情况 支持用户（组）到queue的映射：基于用户组提交作业到对应queue。 运营支持：支持运行时配置队列的容量，ACL等。也可以在运行时停止queue阻止进一步往queue提交作业。 要使用该调度器，在conf/yarn-site.xml配置如下： 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 队列配置 etc/hadoop/capacity-scheduler.xml，所有queue都是root的子队列 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt; &lt;value&gt;a,b,c&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.a.queues&lt;/name&gt; &lt;value&gt;a1,a2&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.b.queues&lt;/name&gt; &lt;value&gt;b1,b2,b3&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt; 具体参考官方文档 Fair Scheduler公平调度FAIR，该算法的思想是尽可能地公平调度，即已分配资源量少的优先级高。也就是说，在考虑如何分配资源时，调度器尽可能使得每个应用程序都能够得到大致相当的资源。默认情况下，公平性只通过内存来衡量，但是可以配置成内存和CPU。 这种策略使得运行时间短的应用能够尽快结束，而不至于在等待资源时被饿死。另外，也可以为应用程序配置优先级，优先级用于决定资源使用量的占比。 配置要使用Fair Scheduler，在conf/yarn-site.xml中如下配置： 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; 队列配置 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version="1.0"?&gt;&lt;allocations&gt; &lt;queue name="sample_queue"&gt; &lt;minResources&gt;10000 mb,0vcores&lt;/minResources&gt; &lt;maxResources&gt;90000 mb,0vcores&lt;/maxResources&gt; &lt;maxRunningApps&gt;50&lt;/maxRunningApps&gt; &lt;maxAMShare&gt;0.1&lt;/maxAMShare&gt; &lt;weight&gt;2.0&lt;/weight&gt; &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt; &lt;queue name="sample_sub_queue"&gt; &lt;aclSubmitApps&gt;charlie&lt;/aclSubmitApps&gt; &lt;minResources&gt;5000 mb,0vcores&lt;/minResources&gt; &lt;/queue&gt; &lt;queue name="sample_reservable_queue"&gt; &lt;reservation&gt;&lt;/reservation&gt; &lt;/queue&gt; &lt;/queue&gt; &lt;queueMaxAMShareDefault&gt;0.5&lt;/queueMaxAMShareDefault&gt; &lt;queueMaxResourcesDefault&gt;40000 mb,0vcores&lt;/queueMaxResourcesDefault&gt; &lt;!-- Queue 'secondary_group_queue' is a parent queue and may have user queues under it --&gt; &lt;queue name="secondary_group_queue" type="parent"&gt; &lt;weight&gt;3.0&lt;/weight&gt; &lt;maxChildResources&gt;4096 mb,4vcores&lt;/maxChildResources&gt; &lt;/queue&gt; &lt;user name="sample_user"&gt; &lt;maxRunningApps&gt;30&lt;/maxRunningApps&gt; &lt;/user&gt; &lt;userMaxAppsDefault&gt;5&lt;/userMaxAppsDefault&gt; &lt;!-- Queue放置规则 --&gt; &lt;queuePlacementPolicy&gt; &lt;rule name="specified" /&gt; &lt;rule name="primaryGroup" create="false" /&gt; &lt;rule name="nestedUserQueue"&gt; &lt;rule name="secondaryGroupExistingQueue" create="false" /&gt; &lt;/rule&gt; &lt;rule name="default" queue="sample_queue"/&gt; &lt;/queuePlacementPolicy&gt;&lt;/allocations&gt; 其他特性抢占允许队列终止超过公平份额队列的容器，释放后资源分配给资源数量低于应得份额的队列中的任务 抢占会降低整个集群利用率，因为失容器会重启 延时调度等待指定时间获得本地运行是值得的，等待配置可通过： 允许错过调度机会的数量 允许等待的集群规模比例 主导资源公平性RDF：针对不同的资源类型的调度算法。同时考虑内存和cpu。]]></content>
      <categories>
        <category>大数据</category>
        <category>yarn</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper原理]]></title>
    <url>%2F2018%2F04%2F30%2F%E5%88%86%E5%B8%83%E5%BC%8F%2Fzookeeper%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[概述ZooKeeper是一种分布式协调服务，用于管理大型主机。在分布式环境中协调和管理服务是一个复杂的过程。ZooKeeper通过其简单的架构和API解决了这个问题。ZooKeeper允许开发人员专注于核心应用程序逻辑，而不必担心应用程序的分布式特性。 分布式应用分布式应用可以在给定时间（同时）在网络中的多个系统上运行，通过协调它们以快速有效的方式完成特定任务。分布式应用正在运行的一组系统称为集群，而在集群中运行的每台机器被称为节点。 分布式应用有两部分， Server（服务器） 和 Client（客户端） 应用程序。服务器应用程序实际上是分布式的，并具有通用接口，以便客户端可以连接到集群中的任何服务器并获得相同的结果。 客户端应用程序是与分布式应用进行交互的工具。 分布式应用的优点 可靠性 - 单个或几个系统的故障不会使整个系统出现故障。 可扩展性 - 可以在需要时增加性能，通过添加更多机器，在应用程序配置中进行微小的更改，而不会有停机时间。 透明性 - 隐藏系统的复杂性，并将其显示为单个实体/应用程序。 分布式应用的挑战 竞争条件 - 两个或多个机器尝试执行特定任务，实际上只需在任意给定时间由单个机器完成。例如，共享资源只能在任意给定时间由单个机器修改。 死锁 - 两个或多个操作等待彼此无限期完成。 不一致 - 数据的部分失败。 什么是Apache ZooKeeper？Apache ZooKeeper是由集群（节点组）使用的一种服务，用于在自身之间协调，并通过稳健的同步技术维护共享数据。ZooKeeper本身是一个分布式应用程序，为写入分布式应用程序提供服务。 ZooKeeper提供的常见服务如下 : 命名服务 - 按名称标识集群中的节点。它类似于DNS，但仅对于节点。 配置管理 - 加入节点的最近的和最新的系统配置信息。 集群管理 - 实时地在集群和节点状态中加入/离开节点。 选举算法 - 选举一个节点作为协调目的的leader。 锁定和同步服务 - 在修改数据的同时锁定数据。此机制可帮助你在连接其他分布式应用程序（如Apache HBase）时进行自动故障恢复。 高度可靠的数据注册表 - 即使在一个或几个节点关闭时也可以获得数据。 ZooKeeper的好处以下是使用ZooKeeper的好处： 简单的分布式协调过程 同步 - 服务器进程之间的相互排斥和协作。此过程有助于Apache HBase进行配置管理。 有序的消息 序列化 - 根据特定规则对数据进行编码。确保应用程序运行一致。这种方法可以在MapReduce中用来协调队列以执行运行的线程。 可靠性 原子性 - 数据转移完全成功或完全失败，但没有事务是部分的。 Zookeeper 基础ZooKeeper的架构 部分 描述 Client（客户端） 客户端，我们的分布式应用集群中的一个节点，从服务器访问信息。对于特定的时间间隔，每个客户端向服务器发送消息以使服务器知道客户端是活跃的。类似地，当客户端连接时，服务器发送确认码。如果连接的服务器没有响应，客户端会自动将消息重定向到另一个服务器。 Server（服务器） 服务器，我们的ZooKeeper总体中的一个节点，为客户端提供所有的服务。向客户端发送确认码以告知服务器是活跃的。 Ensemble ZooKeeper服务器组。形成ensemble所需的最小节点数为3。 Leader 服务器节点，如果任何连接的节点失败，则执行自动恢复。Leader在服务启动时被选举。 Follower 跟随leader指令的服务器节点。 层次命名空间ZooKeeper节点称为 znode 。每个znode由一个名称标识 ZooKeeper数据模型中的每个znode都维护着一个 stat 结构。由以下组成 版本号 - 每个znode都有版本号，这意味着每当与znode相关联的数据发生变化时，其对应的版本号也会增加。当多个zookeeper客户端尝试在同一znode上执行操作时，版本号的使用就很重要。 操作控制列表(ACL) - ACL基本上是访问znode的认证机制。它管理所有znode读取和写入操作。 时间戳 - 时间戳表示创建和修改znode所经过的时间。它通常以毫秒为单位。ZooKeeper从“事务ID”(zxid)标识znode的每个更改。Zxid 是唯一的，并且为每个事务保留时间，以便你可以轻松地确定从一个请求到另一个请求所经过的时间。 数据长度 - 存储在znode中的数据总量是数据长度。你最多可以存储1MB的数据。 Znode的类型 持久节点 - 即使在创建该特定znode的客户端断开连接后，持久节点仍然存在。默认znode都是持久的。 临时节点 - 客户端活跃时，临时节点就是有效的。当客户端与ZooKeeper集合断开连接时，临时节点会自动删除。临时节点不允许有子节点。 顺序节点 - 顺序节点可以是持久的或临时的。当一个新的znode被创建为一个顺序节点时，ZooKeeper通过将10位的序列号附加到原始名称来设置znode的路径。例如，如果将具有路径 /myapp 的znode创建为顺序节点，则ZooKeeper会将路径更改为 /myapp0000000001 ，并将下一个序列号设置为0000000002。如果两个顺序节点是同时创建的，那么ZooKeeper不会对每个znode使用相同的数字。顺序节点在锁定和同步中起重要作用。 Sessions（会话）会话对于ZooKeeper的操作非常重要。会话中的请求按FIFO顺序执行。一旦客户端连接到服务器，将建立会话并向客户端分配会话ID 。 客户端以特定的时间间隔发送心跳以保持会话有效。如果ZooKeeper集合在超过服务器开启时指定的期间（会话超时）都没有从客户端接收到心跳，则它会判定客户端死机。 Watches（监视）监视使客户端收到关于ZooKeeper中的更改的通知。客户端可以在读取特定znode时设置Watches。Watches会向注册的客户端发送任何znode（客户端注册表）更改的通知。 Znode更改是与znode相关的数据的修改或znode的子项中的更改。只触发一次watches。如果客户端想要再次通知，则必须通过另一个读取操作来完成。当连接会话过期时，客户端将与服务器断开连接，相关的watches也将被删除。 Zookeeper 工作流一旦ZooKeeper集合启动，它将等待客户端连接。客户端将连接到ZooKeeper集合中的一个节点。它可以是leader或follower节点。一旦客户端被连接，节点将向特定客户端分配会话ID并向该客户端发送确认。如果客户端没有收到确认，它将尝试连接ZooKeeper集合中的另一个节点。 一旦连接到节点，客户端将以有规律的间隔向节点发送心跳，以确保连接不会丢失。 读数据：向具有znode路径的节点发送读取请求，并且节点通过从其自己的数据库获取来返回所请求的znode 写数据：将znode路径和数据发送到服务器。连接的服务器将该请求转发给leader，然后leader将向所有的follower重新发出写入请求。如果只有大部分节点成功响应，而写入请求成功 工作原理Zookeeper 的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid Zookeeper选主流程(basic paxos)当leader崩溃或者系统启动时，zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。 Zookeeper选主流程（fast paxos）fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。 Zookeeper同步流程选完Leader以后，zk就进入状态同步过程。 Leader等待server连接； Follower连接leader，将最大的zxid发送给leader； Leader根据follower的zxid确定同步点并发送同步消息 follower 完成同步后通知leader已经成为uptodate状态； Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。]]></content>
      <categories>
        <category>分布式</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[综述设计模式分为三种类型，共23种。 创建型模式：单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。 结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。 行为型模式：模版方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式（Interpreter模式）、状态模式、策略模式、职责链模式、访问者模式。设计模式的六大原则 1、开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。 2、里氏代换原则（Liskov Substitution Principle） 里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。 3、依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：真对接口编程，依赖于抽象而不依赖于具体。 4、接口隔离原则（Interface Segregation Principle） 这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 5、迪米特法则（最少知道原则）（Demeter Principle） 为什么叫最少知道原则，就是说：一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。 6、合成复用原则（Composite Reuse Principle） 原则是尽量使用合成/聚合的方式，而不是使用继承。 按字典序排列简介如下。 Abstract Factory（抽象工厂模式）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 Adapter（适配器模式）：将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 Bridge（桥接模式）：将抽象部分与它的实现部分分离，使它们都可以独立地变化。 Builder（建造者模式）：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 Chain of Responsibility（责任链模式）：为解除请求的发送者和接收者之间耦合，而使多个对象都有机会处理这个请求。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它。 Command（命令模式）：将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化；对请求排队或记录请求日志，以及支持可取消的操作。 Composite（组合模式）：将对象组合成树形结构以表示“部分-整体”的层次结构。它使得客户对单个对象和复合对象的使用具有一致性。 Decorator（装饰模式）：动态地给一个对象添加一些额外的职责。就扩展功能而言， 它比生成子类方式更为灵活。 Facade（外观模式）：为子系统中的一组接口提供一个一致的界面，Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 Factory Method（工厂模式）：定义一个用于创建对象的接口，让子类决定将哪一个类实例化。Factory Method使一个类的实例化延迟到其子类。 Flyweight（享元模式）：运用共享技术有效地支持大量细粒度的对象。 Interpreter（解析器模式）：给定一个语言, 定义它的文法的一种表示，并定义一个解释器, 该解释器使用该表示来解释语言中的句子。 Iterator（迭代器模式）：提供一种方法顺序访问一个聚合对象中各个元素，而又不需暴露该对象的内部表示。 Mediator（中介模式）：用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 Memento（备忘录模式）：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可将该对象恢复到保存的状态。 Observer（观察者模式）：定义对象间的一种一对多的依赖关系,以便当一个对象的状态发生改变时,所有依赖于它的对象都得到通知并自动刷新。 Prototype（原型模式）：用原型实例指定创建对象的种类，并且通过拷贝这个原型来创建新的对象。 Proxy（代理模式）：为其他对象提供一个代理以控制对这个对象的访问。 Singleton（单例模式）：保证一个类仅有一个实例，并提供一个访问它的全局访问点。 单例模式是最简单的设计模式之一，但是对于Java的开发者来说，它却有很多缺陷。在九月的专栏中，David Geary探讨了单例模式以及在面对多线程（multi-threading）、类装载器（class loaders）和序列化（serialization）时如何处理这些缺陷。 State（状态模式）：允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它所属的类。 Strategy（策略模式）：定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。本模式使得算法的变化可独立于使用它的客户。 Template Method（模板方法模式）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 Visitor（访问者模式）：表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。 工厂模式 一个抽象产品类派生出多个具体产品类；一抽象工厂类派生出多个具体工厂类；每个具体工厂类只能创建一个具体产品类的实例。即定义一个创建对象的接口（即抽象工厂类），让其子类（具体工厂类）决定实例化哪一个类（具体产品类）。“一对一”的关系。 抽象工厂模式 抽象工厂模式是工厂方法模式的升级版本，他用来创建一组相关或者相互依赖的对象。他与工厂方法模式的区别就在于，工厂方法模式针对的是一个产品等级结构；而抽象工厂模式则是针对的多个产品等级结构。在编程中，通常一个产品结构，表现为一个接口或者抽象类，也就是说，工厂方法模式提供的所有产品都是衍生自同一个接口或抽象类，而抽象工厂模式所提供的产品则是衍生自不同的接口或抽象类。 在抽象工厂模式中，有一个产品族的概念：所谓的产品族，是指位于不同产品等级结构中功能相关联的产品组成的家族。抽象工厂模式所提供的一系列产品就组成一个产品族；而工厂方法提供的一系列产品称为一个等级结构。 举例 优缺点增加产品族：Abstract Factory很好的支持了”开放－封闭”原则。 增加新产品的等级结构：需要修改所有的工厂角色，没有很好支持”开放－封闭”原则。 综合起来，抽象工厂模式以一种倾斜的方式支持增加新的产品，它为新产品族的增加提供方便，而不能为新的产品等级结构的增加提供这样的方便。 单例模式一个类有且仅有一个实例，并且自行实例化向整个系统提供 public class Singleton { private volatile static Singleton instance=null; public static Singleton getInstance(){ if(instance==null){ synchronized(Singleton.class){ if(instance==null){ instance=new Singleton(); } } } return instance; } private Singleton(){ System.out.println(&quot;i have been creaded!&quot;); } } 建造者模式将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 建造者模式与工厂模式的区别我们可以看到，建造者模式与工厂模式是极为相似的，总体上，建造者模式仅仅只比工厂模式多了一个“导演类”的角色。在建造者模式的类图中，假如把这个导演类看做是最终调用的客户端，那么图中剩余的部分就可以看作是一个简单的工厂模式了。 与工厂模式相比，建造者模式一般用来创建更为复杂的对象，因为对象的创建过程更为复杂，因此将对象的创建过程独立出来组成一个新的类——导演类。也就是说，工厂模式是将对象的全部创建过程封装在工厂类中，由工厂类向客户端提供最终的产品；而建造者模式中，建造者类一般只提供产品类中各个组件的建造，而将具体建造过程交付给导演类。由导演类负责将各个组件按照特定的规则组建为产品，然后将组建好的产品交付给客户端。 原型模式用原型实例指定创建对象的种类，并且通过拷贝这个原型来创建新的对象。 原型模式主要用于对象的复制，它的核心是就是类图中的原型类Prototype。Prototype类需要具备以下两个条件： 实现Cloneable接口。在java语言有一个Cloneable接口，它的作用只有一个，就是在运行时通知虚拟机可以安全地在实现了此接口的类上使用clone方法。在java虚拟机中，只有实现了这个接口的类才可以被拷贝，否则在运行时会抛出CloneNotSupportedException异常。 重写Object类中的clone方法。Java中，所有类的父类都是Object类，Object类中有一个clone方法，作用是返回对象的一个拷贝，但是其作用域protected类型的，一般的类无法调用，因此，Prototype类需要将clone方法的作用域修改为public类型。 适配器模式将一个类的接口适配成用户所期待的。一个适配允许通常因为接口不兼容而不能在一起工作的类工作在一起，做法是将类自己的接口包裹在一个已存在的类中。 对象适配器 类适配器 桥接模式将抽象部分与它的实现部分分离，使它们都可以独立地变化 在软件系统中，某些类型由于自身的逻辑，它具有两个或多个维度的变化，那么如何应对这种“多维度的变化”？这就要使用桥接模式 就拿汽车在路上行驶的来说。即有小汽车又有公共汽车，它们都不但能在市区中的公路上行驶，也能在高速公路上行驶。这你会发现，对于交通工具（汽车）有不同的类型，然而它们所行驶的环境（路）也在变化，在软件系统中就要适应两个方面的变化？怎样实现才能应对这种变化呢？ 抽象化(Abstraction)角色：抽象化给出的定义，并保存一个对实现化对象的引用。 修正抽象化(Refined Abstraction)角色：扩展抽象化角色，改变和修正父类对抽象化的定义。 实现化(Implementor)角色：这个角色给出实现化角色的接口，但不给出具体的实现。必须指出的是，这个接口不一定和抽象化角色的接口定义相同，实际上，这两个接口可以非常不一样。实现化角色应当只给出底层操作，而抽象化角色应当只给出基于底层操作的更高一层的操作。 具体实现化(Concrete Implementor)角色：这个角色给出实现化角色接口的具体实现。 装饰模式动态地给一个对象添加一些额外的职责。就扩展功能而言， 它比生成子类方式更为灵活。 特点 装饰对象和真实对象有相同的接口。这样客户端对象就能以和真实对象相同的方式和装饰对象交互。 装饰对象包含一个真实对象的引用（reference） 装饰对象接受所有来自客户端的请求。它把这些请求转发给真实的对象。 装饰对象可以在转发这些请求以前或以后增加一些附加功能。这样就确保了在运行时，不用修改给定对象的结构就可以在外部增加附加的功能。在面向对象的设计中，通常是通过继承来实现对给定类的功能扩展。 角色 抽象构件（Component）角色：给出一个抽象接口，以规范准备接收附加责任的对象。 具体构件（Concrete Component）角色：定义一个将要接收附加责任的类。 装饰（Decorator）角色：持有一个构件（Component）对象的实例，并实现一个与抽象构件接口一致的接口。 具体装饰（Concrete Decorator）角色：负责给构件对象添加上附加的责任。 组合模式组合模式，将对象组合成树形结构以表示“部分-整体”的层次结构，组合模式使得用户对单个对象和组合对象的使用具有一致性。 适用性 你想表示对象的部分-整体层次结构 你希望用户忽略组合对象与单个对象的不同，用户将统一地使用组合结构中的所有对象。 角色 Component 是组合中的对象声明接口，在适当的情况下，实现所有类共有接口的默认行为。声明一个接口用于访问和管理Component子部件。 Leaf 在组合中表示叶子结点对象，叶子结点没有子结点。 Composite 定义有枝节点行为，用来存储子部件，在Component接口中实现与子部件有关操作，如增加(add)和删除(remove)等。 外观模式为子系统中的一组接口提供一个统一接口。Facade模式定义了一个高层接口，这个接口使得这子系统更容易使用。 优点 实现了子系统与客户端之间的松耦合关系。 客户端屏蔽了子系统组件，减少了客户端所需处理的对象数目，并使得子系统使用起来更加容易。示例 享元模式运用共享技术有效的支持大量细粒度的对象。 主要解决：在有大量对象时，有可能会造成内存溢出，我们把其中共同的部分抽象出来，如果有相同的业务请求，直接返回在内存中已有的对象，避免重新创建。 示例数据库连接池 代理模式为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介的作用。 角色 抽象角色：声明真实对象和代理对象的共同接口。 代理角色：代理对象角色内部含有对真实对象的引用，从而可以操作真实对象，同时代理对象提供与真实对象相同的接口以便在任何时刻都能代替真实对象。同时，代理对象 可以在执行真实对象操作时，附加其他的操作，相当于对真实对象进行封装。 真实角色：代理角色所代表的真实对象，是我们最终要引用的对象。 与装饰模式区别对装饰器模式来说，装饰者（decorator）和被装饰者（decoratee）都实现同一个 接口。对代理模式来说，代理类（proxy class）和真实处理的类（real class）都实现同一个接口 装饰器模式关注于在一个对象上动态的添加方法，然而代理模式关注于控制对对象的访问。换句话 说，用代理模式，代理类（proxy class）可以对它的客户隐藏一个对象的具体信息。因此，当使用代理模式的时候，我们常常在一个代理类中创建一个对象的实例。并且，当我们使用装饰器模 式的时候，我们通常的做法是将原始对象作为一个参数传给装饰者的构造器。 观察者模式观察者设计模式定义了对象间的一种一对多的依赖关系，以便一个对象的状态发生变化时，所有依赖于它的对象都得到通知并自动刷新。 模版方法模式将部分逻辑以具体方法以及具体构造函数的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。 角色抽象模板 定义了一个或多个抽象操作，以便让子类实现。这些抽象操作叫做基本操作，它们是一个顶级逻辑的组成步骤。 定义并实现了一个模板方法。这个模板方法一般是一个具体方法，它给出了一个顶级逻辑的骨架，而逻辑的组成步骤在相应的抽象操作中，推迟到子类实现。顶级逻辑也有可能调用一些具体方法。 具体模板： 实现父类所定义的一个或多个抽象方法，它们是一个顶级逻辑的组成步骤。 每一个抽象模板角色都可以有任意多个具体模板角色与之对应，而每一个具体模板角色都可以给出这些抽象方法（也就是顶级逻辑的组成步骤）的不同实现，从而使得顶级逻辑的实现各不相同。 基本方法 抽象方法：一个抽象方法由抽象类声明，由具体子类实现。在Java语言里抽象方法以abstract关键字标示。 具体方法：一个具体方法由抽象类声明并实现，而子类并不实现或置换。 钩子方法：一个钩子方法由抽象类声明并实现，而子类会加以扩展。通常抽象类给出的实现是一个空实现，作为方法的默认实现。 示例 命令模式在软件系统中，行为请求者与行为实现者通常是一种紧耦合的关系，但某些场合，比如需要对行为进行记录、撤销或重做、事务等处理时，这种无法抵御变化的紧耦合的设计就不太合适。将一组行为抽象为对象，实现二者之间的松耦合。这就是命令模式（Command Pattern）。 角色 抽象命令（Command）：定义命令的接口，声明执行的方法。 具体命令（ConcreteCommand）：具体命令，实现要执行的方法，它通常是“虚”的实现；通常会有接收者，并调用接收者的功能来完成命令要执行的操作。 接收者（Receiver）：真正执行命令的对象。任何类都可能成为一个接收者，只要能实现命令要求实现的相应功能。 调用者（Invoker）：要求命令对象执行请求，通常会持有命令对象，可以持有很多的命令对象。这个是客户端真正触发命令并要求命令执行相应操作的地方，也就是说相当于使用命令对象的入口。 客户端（Client）：命令由客户端来创建，并设置命令的接收者。 示例 迭代器模式迭代器模式（Iterator），提供一种方法顺序访问一个聚合对象中的各种元素，而又不暴露该对象的内部表示 适用性 访问一个聚合对象的内容而无需暴露它的内部表示 支持对聚合对象的多种遍历 为遍历不同的聚合结构提供一个统一的接口 角色 抽象容器：一般是一个接口，提供一个iterator()方法，例如java中的Collection接口，List接口，Set接口等。 具体容器：就是抽象容器的具体实现类，比如List接口的有序列表实现ArrayList，List接口的链表实现LinkList，Set接口的哈希列表的实现HashSet等。 抽象迭代器：定义遍历元素所需要的方法，一般来说会有这么三个方法：取得第一个元素的方法first()，取得下一个元素的方法next()，判断是否遍历结束的方法isDone()（或者叫hasNext()），移出当前对象的方法remove(), 迭代器实现：实现迭代器接口中定义的方法，完成集合的迭代。 中介者模式用一个中介者对象封装一系列的对象交互，中介者使各对象不需要显示地相互作用，从而使耦合松散，而且可以独立地改变它们之间的交互。 在软件的开发过程中，势必会碰到这样一种情况，多个类或多个子系统相互交互，而且交互很繁琐，导致每个类都必须知道他需要交互的类，这样它们的耦合会显得异常厉害。牵一发而动全身。 角色 Mediator：中介者接口。在里面定义了各个同事之间相互交互所需要的方法 ConcreteMediator：具体的中介者实现对象。它需要了解并为维护每个同事对象，并负责具体的协调各个同事对象的交互关系。 Colleague：同事类的定义，通常实现成为抽象类，主要负责约束同事对象的类型，并实现一些具体同事类之间的公共功能，比如，每个具体同事类都应该知道中介者对象，也就是每个同事对象都会持有中介者对象的引用，这个功能可定义在这个类中。 ConcreteColleague：具体的同事类，实现自己的业务，需要与其他同事对象交互时，就通知中介对象，中介对象会负责后续的交互。 备忘录模式在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样就可以将该对象恢复到原先保存的状态 角色 发起人：记录当前时刻的内部状态，负责定义哪些属于备份范围的状态，负责创建和恢复备忘录数据。 备忘录：负责存储发起人对象的内部状态，在需要的时候提供发起人需要的内部状态。 管理角色：对备忘录进行管理，保存和提供备忘录。 解释器模式给定一种语言，定义他的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中句子。 角色 抽象解释器：声明一个所有具体表达式都要实现的抽象接口（或者抽象类），接口中主要是一个interpret()方法，称为解释操作。具体解释任务由它的各个实现类来完成，具体的解释器分别由终结符解释器TerminalExpression和非终结符解释器NonterminalExpression完成。 终结符表达式：实现与文法中的元素相关联的解释操作，通常一个解释器模式中只有一个终结符表达式，但有多个实例，对应不同的终结符。终结符一半是文法中的运算单元，比如有一个简单的公式R=R1+R2，在里面R1和R2就是终结符，对应的解析R1和R2的解释器就是终结符表达式。 非终结符表达式：文法中的每条规则对应于一个非终结符表达式，非终结符表达式一般是文法中的运算符或者其他关键字，比如公式R=R1+R2中，+就是非终结符，解析+的解释器就是一个非终结符表达式。非终结符表达式根据逻辑的复杂程度而增加，原则上每个文法规则都对应一个非终结符表达式。 环境角色：这个角色的任务一般是用来存放文法中各个终结符所对应的具体值，比如R=R1+R2，我们给R1赋值100，给R2赋值200。这些信息需要存放到环境角色中，很多情况下我们使用Map来充当环境角色就足够了。 适用场景 有一个简单的语法规则，比如一个sql语句，如果我们需要根据sql语句进行rm转换，就可以使用解释器模式来对语句进行解释。 一些重复发生的问题，比如加减乘除四则运算，但是公式每次都不同，有时是a+b-cd，有时是ab+c-d，等等等等个，公式千变万化，但是都是由加减乘除四个非终结符来连接的，这时我们就可以使用解释器模式。 状态模式状态模式允许一个对象在其内部状态改变的时候改变其行为。这个对象看上去就像是改变了它的类一样。 用一句话来表述，状态模式把所研究的对象的行为包装在不同的状态对象里，每一个状态对象都属于一个抽象状态类的一个子类。状态模式的意图是让一个对象在其内部状态改变的时候，其行为也随之改变。 解决的问题主要解决的是当控制一个对象状态转换的条件表达式过于复杂时的情况。把状态的判断逻辑转移到表示不同的一系列类当中，可以把复杂的逻辑判断简单化。 角色 上下文环境（Context）：它定义了客户程序需要的接口并维护一个具体状态角色的实例，将与状态相关的操作委托给当前的Concrete State对象来处理。 抽象状态（State）：定义一个接口以封装使用上下文环境的的一个特定状态相关的行为。 具体状态（Concrete State）：实现抽象状态定义的接口。 优点 状态模式将与特定状态相关的行为局部化，并且将不同状态的行为分割开来。 所有状态相关的代码都存在于某个ConcereteState中，所以通过定义新的子类很容易地增加新的状态和转换。 状态模式通过把各种状态转移逻辑分不到State的子类之间，来减少相互间的依赖。 缺点 导致较多的ConcreteState子类适用场景 当一个对象的行为取决于它的状态，并且它必须在运行时刻根据状态改变它的行为时，就可以考虑使用状态模式来。 一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态。 示例 策略模式策略模式定义了一系列的算法，并将每一个算法封装起来，而且使它们还可以相互替换。策略模式让算法独立于使用它的客户而独立变化。 角色 封装类：也叫上下文，对策略进行二次封装，目的是避免高层模块对策略的直接调用。 抽象策略： 定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，Context使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略：实现了Strategy定义的接口，提供具体的算法实现。 应用场景 多个类只区别在表现行为不同，可以使用Strategy模式，在运行时动态选择具体要执行的行为。 需要在不同情况下使用不同的策略(算法)，或者策略还可能在未来用其它方式来实现。 对客户隐藏具体策略(算法)的实现细节，彼此完全独立。 优缺点优点: 策略类之间可以自由切换，由于策略类实现自同一个抽象，所以他们之间可以自由切换。 易于扩展，增加一个新的策略对策略模式来说非常容易 避免使用多重条件，如果不使用策略模式，对于所有的算法，必须使用条件语句进行连接，通过条件判断来决定使用哪一种算法 缺点: 维护各个策略类会给开发带来额外开销 必须对客户端（调用者）暴露所有的策略类，因为使用哪种策略是由客户端来决定的,就这一点来说是有悖于迪米特法则的。 职责链模式使多个对象都有机会处理请求，从而避免了请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有对象处理它为止。 传统public void test(int i, Request request){ if(i==1){ Handler1.response(request); }else if(i == 2){ Handler2.response(request); }else if(i == 3){ Handler3.response(request); }else if(i == 4){ Handler4.response(request); }else{ Handler5.response(request); } } 代码臃肿：实际应用中的判定条件通常不是这么简单地判断是否为1或者是否为2，如果判断条件再比较多，那么这个if…else…语句基本上就没法看了。 耦合度高：如果我们想继续添加处理请求的类，那么就要继续添加else if判定条件 角色 抽象处理类：抽象处理类中主要包含一个指向下一处理类的成员变量nextHandler和一个处理请求的方法handRequest，handRequest方法的主要主要思想是，如果满足处理的条件，则有本处理类来进行处理，否则由nextHandler来处理。 具体处理类：具体处理类主要是对具体的处理逻辑和处理的适用条件进行实现。 访问者模式封装某些作用于某种数据结构中各元素的操作，它可以在不改变数据结构的前提下定义作用于这些元素的新的操作。 角色 抽象访问者：抽象类或者接口，声明访问者可以访问哪些元素，具体到程序中就是visit方法中的参数定义哪些对象是可以被访问的。 访问者：实现抽象访问者所声明的方法，它影响到访问者访问到一个类后该干什么，要做什么事情。 抽象元素类：接口或者抽象类，声明接受哪一类访问者访问，程序上是通过accept方法中的参数来定义的。抽象元素一般有两类方法，一部分是本身的业务逻辑，另外就是允许接收哪类访问者来访问。 元素类：实现抽象元素类所声明的accept方法，通常都是visitor.visit(this)，基本上已经形成一种定式了。 结构对象：一个元素的容器，一般包含一个容纳多个不同类、不同接口的容器，如List、Set、Map等，在项目中一般很少抽象出这个角色。 优点 符合单一职责原则：凡是适用访问者模式的场景中，元素类中需要封装在访问者中的操作必定是与元素类本身关系不大且是易变的操作，使用访问者模式一方面符合单一职责原则，另一方面，因为被封装的操作通常来说都是易变的，所以当发生变化时，就可以在不改变元素类本身的前提下，实现对变化部分的扩展。 扩展性良好：元素类可以通过接受不同的访问者来实现对不同操作的扩展。 适用场景 假如一个对象中存在着一些与本对象不相干（或者关系较弱）的操作，为了避免这些操作污染这个对象，则可以使用访问者模式来把这些操作封装到访问者中去。 假如一组对象中，存在着相似的操作，为了避免出现大量重复的代码，也可以将这些重复的操作封装到访问者中去。 但是，访问者模式并不是那么完美，它也有着致命的缺陷：增加新的元素类比较困难。]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql面试题]]></title>
    <url>%2F2018%2F03%2F18%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2Fmysql%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 主键 超键 候选键 外键 超键(super key) ：在关系中能唯一标识元组的属性集称为关系模式的超键 候选键(candidate key)：不含有多余属性的超键称为候选键 主键(primary key)：用户选作元组标识的一个候选键程序主键 外键(foreign key)：如果关系模式R1中的某属性集不是R1的主键，而是另一个关系R2的主键则该属性集是关系模式R1的外键 实例讲解假设有如下两个表： ​ 学生（学号，姓名，性别，身份证号，教师编号） ​ 教师（教师编号，姓名，工资） 超键：由超键的定义可知，学生表中含有学号或者身份证号的任意组合都为此表的超键。如：（学号）、（学号，姓名）、（身份证号，性别）等。 候选键：候选键属于超键，它是最小的超键，就是说如果再去掉候选键中的任何一个属性它就不再是超键了。学生表中的候选键为：（学号）、（身份证号）。 主键：主键就是候选键里面的一个，是人为规定的，例如学生表中，我们通常会让“学号”做主键，教师表中让“教师编号”做主键。 外键：外键比较简单，学生表中的外键就是“教师编号”。外键主要是用来描述两个表的关系。 2. 数据库事务​ 数据库事务transanction正确执行的四个基本要素：ACID，原子性(Atomicity)、一致性(Correspondence)、隔离性(Isolation)、持久性(Durability)。 原子性：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样 一致性：在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。 隔离性：隔离状态执行事务，使它们好像是系统在给定时间内执行的唯一操作。如果有两个事务，运行在相同的时间内，执行 相同的功能，事务的隔离性将确保每一事务在系统中认为只有该事务在使用系统。这种属性有时称为串行化，为了防止事务操作间的混淆，必须串行化或序列化请 求，使得在同一时间仅有一个请求用于同一数据。 持久性：在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 3. 视图​ 视图是虚拟的表，与包含数据的表不一样，视图只包含使用时动态检索数据的查询；不包含任何列或数据。使用视图可以简化复杂的sql操作，隐藏具体的细节，保护数据；视图创建后，可以使用与表相同的方式利用它们。 ​ 视图不能被索引，也不能有关联的触发器或默认值，如果视图本身内有order by 则对视图再次order by将被覆盖 ​ 创建视图：create view XXX as XXXXXXXXXXXXXX; ​ 对于某些视图比如未使用联结子查询分组聚集函数Distinct Union等，是可以对其更新的，对视图的更新将对基表进行更新；但是视图主要用于简化检索，保护数据，并不用于更新，而且大部分视图都不可以更新。由一个基表定义的视图，只含有基表的主键或候补键，并且视图中没有用表达式或函数定义的属性，才允许更新。 4. drop,delete与truncate的区别drop直接删掉表；truncate删除表中数据，再插入时自增长id又从1开始 ；delete删除表中数据，可以加where字句。 DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。 一般而言，drop &gt; truncate &gt; delete 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。 truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。 delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。 truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老师想触发trigger,还是用delete。 Truncate table 表名 速度快,而且效率高,因为:truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。 5. 索引的工作原理及其种类​ 数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。​ 为表设置索引要付出代价的：一是增加了数据库的存储空间，二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动。 ​ 内容转B树tag 索引优点 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 建议创建索引列 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的；在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 不建议创建索引列 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 索引分类 ​ 唯一索引：唯一索引是不允许其中任何两行具有相同索引值的索引。 ​ 当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复键值的新数据。例如，如果在employee表中职员的姓(lname)上创建了唯一索引，则任何两个员工都不能同姓。 ​ 主键索引 ​ 数据库表经常有一列或列组合，其值唯一标识表中的每一行。该列称为表的主键。 在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 ​ 聚集索引 ​ 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 ​ 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 局部性原理与磁盘预读 ​ 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。​ 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 ​ 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B-/+Tree索引的性能 ​ 上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： ​ 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 ​ B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小。 ​ 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 ​ 综上所述，用B-Tree作为索引结构效率是非常高的。 不使用索引的情况 以%开头的LIKE查询不能够利用B-tree索引 数据类型出现隐式转换 explain select * from actor where last_name=1; explain select * from actor where last_name=’1; 复合索引的情况下，查询条件不满足索引最左的原则 Mysql估计使用索引比全表扫描慢 用or分割开的条件，or前条件有索引，or后的列没有索引 负向查询（not , not in, not like, &lt;&gt;, != ,!&gt;,!&lt; ） 不会使用索引 独立的列 索引 不能是表达式的一部分 select * from xxxx where id+1; 6. 连接种类外连接 左连接：left join 或 left outer join select * from table1 left join table2 on table1.id=table2.id 注释：包含table1的所有子句，根据指定条件返回table2相应的字段，不符合的以null显示 右连接：right join 或 right outer join select * from table1 right join table2 on table1.id=table2.id 注释：包含table2的所有子句，根据指定条件返回table1相应的字段，不符合的以null显示 完整外部联接:full join 或 full outer join select * from table1 full join table2 on table1.id=table2.id 注释：返回左右连接的和（见上左、右连接） 内连接：join 或 inner join select * from table1 join table2 on table1.id=table2.id 注释：只返回符合条件的table1和table2的列 等价： select a.,b. from table1 a,table2 b where a.id=b.id select * from table1 cross join table2 where table1.id=table2.id (注：cross join后加条件只能用where,不能用on) 交叉连接(完全) 概念：没有 WHERE 子句的交叉联接将产生联接所涉及的表的笛卡尔积。第一个表的行数乘以第二个表的行数等于笛卡尔积结果集的大小。（table1和table2交叉连接产生3*3=9条记录） 交叉连接：cross join (不带条件where…) sql语句 select * from table1 cross join table2 等价（与下列执行效果相同） select * from table1,table2 7. 数据库范式总结 1NF： 字段是最小的的单元不可再分 2NF：满足1NF,表中的字段必须完全依赖于全部主键而非部分主键 (一般我们都会做到) 3NF：满足2NF,非主键外的所有字段必须互不依赖 4NF：满足3NF,消除表中的多值依赖 第一范式（1NF） 第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。。 简而言之 第一范式就是无重复的列。 第二范式（2NF） 第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主关键字或主键、主码。 第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离 简而言之，第二范式就是非主属性非部分依赖于主关键字。 第三范式（3NF） 简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。例如，存在一个部 门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入工信息表中。 简而言之，第三范式就是属性不依赖于其它非主属性。 8. 数据库优化的思路SQL语句优化 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null。可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0 很多时候用 exists 代替 in 是一个好的选择 用Where子句替换HAVING 子句 因为HAVING 只会在检索出所有记录之后才对结果集进行过滤 索引优化 ​ 看上文索引 数据库结构优化 范式优化： 比如消除冗余（节省空间。。） 反范式优化：比如适当加冗余等（减少join） 拆分表： 垂直拆分和水平拆分 服务器硬件优化 ​ 这个么多花钱咯！ 9. MySql的复制原理以及流程基本原理流程，3个线程以及之间的关联； 主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中； 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进 自己的relay log中； 从：sql执行线程——执行relay log中的语句； 10. MySQL中myisam与innodb的区别，至少5点 InnoDB支持事物，而MyISAM不支持事物 InnoDB支持行级锁，而MyISAM支持表级锁 InnoDB支持MVCC, 而MyISAM不支持 多版本并发控制机制。乐观锁CAS操作 InnoDB支持外键，而MyISAM不支持 InnoDB不支持全文索引，而MyISAM支持。 11. innodb引擎的4大特性插入缓冲（insert buffer)、二次写(double write)、自适应哈希索引(ahi)、预读(read ahead) 12. 两者select count(*)哪个更快，为什么myisam更快，因为myisam内部维护了一个计数器，可以直接调取。 13. MySQL中varchar与char的区别以及varchar(50)中的50代表的涵义 varchar与char的区别 char是一种固定长度的类型，varchar则是一种可变长度的类型 varchar(50)中50的涵义 最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存，因为order by col采用fixed_length计算col长度(memory引擎也一样) int（20）中20的涵义 指显示字符的长度，20表示最大显示宽度为20，但仍占4字节存储，存储范围不变； 14. innodb的事务与日志的实现方式 有多少种日志 错误日志：记录出错信息，也记录一些警告信息或者正确的信息。 ​ 查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行 ​ 慢查询日志：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。 ​ 二进制日志：记录对数据库执行更改的所有操作。 ​ 中继日志: ​ 事务日志： 事物的4种隔离级别 读未提交(RU) ​ 读已提交(RC) ​ 可重复读(RR) ​ 串行 事务是如何通过日志来实现的 ​ 事务日志是通过redo和innodb的存储引擎日志缓冲（Innodb log buffer）来实现的，当开始一个事务的时候，会记录该事务的lsn(log sequence number)号; 当事务执行时会往InnoDB存储引擎的日志的日志缓存里面插入事务日志；当事务提交时，必须将存储引擎的日志缓冲写入磁盘（通过innodb_flush_log_at_trx_commit来控制），也就是写数据前，需要先写日志。这种方式称为“预写日志方式”。 15. MySQL binlog的几种日志录入格式以及区别 Statement：每一条会修改数据的sql都会记录在binlog中 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。(相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入 row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使 用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。) 缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执 行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以user-defined-functions(udf)会出现问题). 使用以下函数的语句也无法被复制：* LOAD_FILE()* UUID()* USER()* FOUND_ROWS()* SYSDATE() (除非启动时启用了–sysdate-is-now 选项) 同时在INSERT …SELECT 会产生比 RBR 更多的行级锁 Row:不记录sql语句上下文相关信息，仅保存哪条记录被修改 优点：binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节 而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题 缺点：所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改 都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。 Mixedlevel: 是以上两种level的混合使用 ​ 一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog,MySQL会根据执行的每一条具体的sql语句来区 分对待记录的日志形式，也就是在Statement和Row之间选择一种.新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变 更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。 16. MySQL数据库cpu飙升到500%的话他怎么处理 列出所有进程：show processlist 观察所有进程：多秒没有状态变化的(干掉) 查看超时日志或者错误日志 (做了几年开发，一般会是查询以及大批量的插入会导致cpu与i/o上涨 …. 当然不排除网络状态突然断了，导致一个请求服务器只接受到一半，比如where子句或分页子句没有发送，当然的一次被坑经历) 17. 一个6亿的表a，一个3亿的表b，通过外间tid关联，你如何最快的查询出满足条件的第50000到第50200中的这200条数据记录 如果A表TID是自增长,并且是连续的,B表的ID为索引 select * from a,b where a.tid = b.id and a.tid&gt;500000 limit 200; 如果A表的TID不是连续的,那么就需要使用覆盖索引.TID要么是主键,要么是辅助索引,B表ID也需要有索引 select * from b , (select tid from a limit 50000,200) a where b.id = a .tid; ### 18. MySQL中InnoDB引擎的行锁是通过加在什么上完成答：InnoDB是基于索引来完成行锁 例: select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定,并且 id 是有索引键的列, 如果 id 不是索引键那么InnoDB将完成表锁,,并发将无从谈起 19. xtrabackup实现原理​ 在InnoDB内部会维护一个redo日志文件，我们也可以叫做事务日志文件。事务日志会存储每一个InnoDB表数据的记录修改。当InnoDB启动时，InnoDB会检 查数据文件的事务日志，并执行两个步骤：它应用（前滚）已经提交的事务日志到数据文件，并将修改过但没有提交的数据进行回滚操作。 20. 存储过程与触发器的区别​ 触发器与存储过程非常相似，触发器也是SQL语句集，两者唯一的区别是触发器不能用EXECUTE语句调用，而是在用户执行Transact-SQL语句时自动触发（激活）执行。触发器是在一个修改了指定表中的数据时执行的存储过程。通常通过创建触发器来强制实现不同表中的逻辑相关数据的引用完整性和一致性。由于用户不能绕过触发器，所以可以用它来强制实施复杂的业务规则，以确保数据的完整性。触发器不同于存储过程，触发器主要是通过事件执行触发而被执行的， ​ 存储过程可以通过存储过程名称名字而直接调用。当对某一表进行诸如UPDATE、INSERT、DELETE这些操作SQLSERVER就会自动执行触发器所定义的SQL语句，从而确保对数据的处理必须符合这些SQL语句所定义的规则。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git分支管理]]></title>
    <url>%2F2018%2F03%2F16%2Fgit%2Fgit%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.引言git 和 svn 的一个显著区别就是提供更丰富的分支特性，我们今天就要说一下如何管理这些分支。关于 git 的分支管理，Vincent Driessen 有一篇文章说的非常好，地址在参考[1]。我这里主要就是参考他的文章。 2. 总览git 的分支整体预览图如下。 从上图可以看到主要包含下面几个分支： master: 主分支，主要用来版本发布。 develop：日常开发分支，该分支正常保存了开发的最新代码。 feature：具体的功能开发分支，只与 develop 分支交互。 release：release 分支可以认为是 master 分支的未测试版。比如说某一期的功能全部开发完成，那么就将 develop 分支合并到 release 分支，测试没有问题并且到了发布日期就合并到 master 分支，进行发布。 hotfix：线上 bug 修复分支。 除此之后还可以有 fast-track 等分支。 3. 主分支主分支包括 master 分支和 develop 分支。master 分支用来发布，HEAD 就是当前线上的运行代码。develop 分支就是我们的日常开发。使用这两个分支就具有了最简单的开发模式：develop 分支用来开发功能，开发完成并且测试没有问题则将 develop 分支的代码合并到 master 分支并发布。 这引入了几个问题： develop 分支只有发布完了才能进行下一个版本开发，开发会比较缓慢。 线上代码出现 bug 如何进行 bug 修复。 带着这两个问题往下看。 4. 辅助分支主要介绍的辅助分支如下： feature 分支 release 分支 hotfix 分支 通过这些分支，我们可以做到：团队成员之间并行开发，feature track 更加容易，开发和发布并行以及线上问题修复。 4.1 Feature 分支feature 分支用来开发具体的功能，一般 fork 自 develop 分支，最终可能会合并到 develop 分支。比如我们要在下一个版本增加功能1、功能2、功能3。那么我们就可以起三个feature 分支：feature1，feature2，feature3。（feature 分支命名最好能够自解释，这并不是一种好的命名。）随着我们开发，功能1和功能2都被完成了，而功能3因为某些原因完成不了，那么最终 feature1 和 feature2 分支将被合并到 develop 分支，而 feature3 分支将被干掉。 我们来看几个相关的命令。 从 develop 分支建一个 feature 分支，并切换到 feature 分支 12 $ git checkout -b myfeature developSwitched to a new branch “myfeature” 合并feature 分支到 develop 12345678 $ git checkout developSwitched to branch ‘develop’$ git merge –no-ff myfeatureUpdating ea1b82a..05e9557(Summary of changes)$ git branch -d myfeatureDeleted branch myfeature$ git push origin develop 上面我们 merge 分支的时候使用了参数 --no-ff，ff 是fast-forward 的意思，--no-ff就是禁用fast-forward。关于这两种模式的区别如下图。（可以使用 sourceTree 或者命令git log --graph查看。） 看了上面的图，那么使用非fast-forward模式来 merge 的好处就不言而喻了：我们知道哪些 commit 是某些 feature 相关的。虽然 git merge 的时候会自动判断是否使用fast-farward模式，但是有时候为了更明确，我们还是要加参数--no-ff或者--ff。 4.2 Release 分支release 分支在我看来是 pre-master。release 分支从 develop 分支 fork 出来，最终会合并到 develop 分支和 master 分支。合并到 master 分支上就是可以发布的代码了。有人可能会问那为什么合并回 develop 分支呢？很简单，有了 release 分支，那么相关的代码修复就只会在 release 分支上改动了，最后必然要合并到 develop 分支。下面细说。 我们最初所有的开发工作都在 develop 分支上，当我们这一期的功能开发完毕的时候，我们基于 develop 分支开一个新的 release 分支。这个时候我们就可以对 release 分支做统一的测试了，另外做一些发布准备工作：比如版本号之类的。 如果测试工作或者发布准备工作和具体的开发工作由不同人来做，比如国内的 RD 和 QA，这个 RD 就可以继续基于 develop 分支继续开发了。再或者说公司对于发布有严格的时间控制，开发工作提前并且完美的完成了，这个时候我们就可以在 develop 分支上继续我们下一期的开发了。同时如果测试有问题的话，我们将直接在 release 分支上修改，然后将修改合并到 develop 分支上。 待所有的测试和准备工作做完之后，我们就可以将 release 分支合并到 master 分支上，并进行发布了。 一些相关命令如下。 新建 release 分支 1234567 $ git checkout -b release-1.2 developSwitched to a new branch “release-1.2”$ ./bump-version.sh 1.2File modified successfully, version bumped to 1.2.$ git commit -a -m “Bumped version number to 1.2”[release-1.2 74d9424] Bumped version number to 1.21 files changed, 1 insertions(+), 1 deletions(-) release 分支合并到 master 分支 123456 $ git checkout masterSwitched to branch ‘master’$ git merge –no-ff release-1.2Merge made by recursive.(Summary of changes)$ git tag -a 1.2 release 分支合并到 develop 分支 12345 $ git checkout developSwitched to branch ‘develop’$ git merge –no-ff release-1.2Merge made by recursive.(Summary of changes) 最后，删除 release 分支 12 $ git branch -d release-1.2Deleted branch release-1.2 (was ff452fe). 4.3 Hotfix 分支顾名思义，hotfix 分支用来修复线上 bug。当线上代码出现 bug 时，我们基于 master 分支开一个 hotfix 分支，修复 bug 之后再将 hotfix 分支合并到 master 分支并进行发布，同时 develop 分支作为最新最全的代码分支，hotfix 分支也需要合并到 develop 分支上去。仔细想一想，其实 hotfix 分支和 release 分支功能类似。hotfix 的好处是不打断 develop 分支正常进行，同时对于现实代码的修复貌似也没有更好的方法了（总不能直接修改 master 代码吧:D）。 一些相关的命令。 新建 hotfix 分支 1234567 $ git checkout -b hotfix-1.2.1 masterSwitched to a new branch “hotfix-1.2.1”$ ./bump-version.sh 1.2.1Files modified successfully, version bumped to 1.2.1.$ git commit -a -m “Bumped version number to 1.2.1”[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.11 files changed, 1 insertions(+), 1 deletions(-) Fix bug 123 $ git commit -m “Fixed severe production problem”[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) buf fix 之后，hotfix 合并到 master 123456 $ git checkout masterSwitched to branch ‘master’$ git merge –no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes)$ git tag -a 1.2.1 hotfix 合并到 develop 分支 12345 $ git checkout developSwitched to branch ‘develop’$ git merge –no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes) 删除 hotfix 分支 12 $ git branch -d hotfix-1.2.1Deleted branch hotfix-1.2.1 (was abbe5d6). 参考 A successful Git branching model]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git介绍]]></title>
    <url>%2F2018%2F03%2F16%2Fgit%2Fgit%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[版本控制版本控制是一种记录一个或若干文件内容变化， 以便将来查阅特定版本修订情况的系统。 采用版本控制系统（VCS）可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态， 你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的 原因，又是谁在何时报告了某个功能缺陷等等。 本地版本控制系统 大多都是采用某种简 单的数据库来记录文件的历次更新差异。 集中化的版本控制系统 诸如 CVS、Subversion 以及Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修 订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 分布式版本控制系统 客户端并不只提取最新版本的文件快 照，而是把代码仓库完整地镜像下来。，任何一处协同工作用的服务器发生故障， 事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次 对代码仓库的完整备份。 Git 基础特点： 直接记录快照，而非差异比较 近乎所有操作都是本地执行 在 Git 中的绝大多数操作都只需要访问本地文件和资源，一般不需要来自网络上其它计算机的 信息。 Git 保证完整性 Git 中所有数据在存储前都计算校验和，然后以校验和来引用。 三种状态： 已提交（committed）数据已经安全的保存在本地数据库中。 已修改（modified）表示修改了文件，但 还没保存到数据库中。 已暂存 （staged） 对一个已修改文件的当前版本做了标记，使之包含在下次 提交的快照中。 三个工作区域的概念：Git 仓库、工作目录以及暂存区域 Git仓库目录是Git用来保存项目的元数据和对象数据库的地方。从其它计算机克隆仓库时，拷贝的就是这里的数据。 工作目录是对项目的某个版本独立提取出来的内容。放在磁盘上供你使用或修改。 暂存区域是一个文件，保存了下次将提交的文件列表信息 基本的 Git 工作流程如下： 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 获取Git仓库在现有目录中初始化仓库只需到此项目所在的目录，执行： 1$ git init 如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交： 123$ git add *.c$ git add README$ git commit -m &apos;initial project version&apos; 从现有仓库克隆1$ git clone git://github.com/schacon/grit.git 这会在当前目录下创建一个名为grit的目录，其中包含一个 .git 的目录，用于保存下载下来的所有版本记录，然后从中取出最新版本的文件拷贝。如果希望在克隆的时候，自己定义要新建的项目目录名称，可以在上面的命令末尾指定新的名字： 1$ git clone git://github.com/schacon/grit.git mygrit 记录每次更新到仓库工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪 在编辑过某些文件之后，Git 将这些文件标为已修改。我们逐步把这些修改过的文件放到暂存区域，直到最后一次性提交所有这些暂存起来的文件，如此重复。 检查当前文件状态1234567$ git status # On branch master # Untracked files: # (use "git add &lt;file&gt;..." to include in what will be committed) # # README nothing added to commit but untracked files present (use "git add" to track) 在状态报告中可以看到新建的README文件出现在“Untracked files”下面。未跟踪的文件意味着Git在之前的快照（提交）中没有这些文件；Git 不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”，因而不用担心把临时文件什么的也归入版本管理。不过现在的例子中，我们确实想要跟踪管理 README 这个文件。 跟踪新文件1$ git add README 暂存已修改文件1$ git add benchmarks.rb git add 命令（这是个多功能命令，根据目标文件的状态不同，此命令的效果也不同：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等） 忽略某些文件名为 .gitignore 的文件，列出要忽略的文件模式。 文件 .gitignore 的格式规范如下： 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 glob 模式是指 shell 所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 查看已暂存和未暂存的更新查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff 要看已经暂存起来的文件和上次提交时的快照之间的差异，可以用 git diff --cached 命令 请注意，单单 git diff 不过是显示还没有暂存起来的改动，而不是这次工作和上次提交之间的差异。所以有时候你一下子暂存了所有更新过的文件后，运行 git diff 后却什么也没有，就是这个原因。 提交更新现在的暂存区域已经准备妥当可以提交了。提交的时候不会记录这些还没暂存起来的变化 1$ git commit -m "Story 182: Fix benchmarks for speed" 记住，提交时记录的是放在暂存区域的快照，任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。Git 提供了一个跳过使用暂存区域的方式，只要在提交的时候，给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤： 1$ git commit -a -m &apos;added new benchmarks&apos; 移除文件要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件 1$ git rm grit.gemspec 最后提交的时候，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即 force 的首字母），以防误删除文件后丢失修改的内容。 把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。 1$ git rm --cached readme.txt 后面可以列出文件或者目录的名字，也可以使用 glob 模式。 移动文件Git 并不跟踪文件移动操作。如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。 1$ git mv file_from file_to 查看提交历史git log 会按提交时间列出所有的更新，最近的更新排在最上面。 -p 选项展开显示每次提交的内容差异，用 -2 则仅显示最近的两次更新 12345678910选项 说明 -p 按补丁格式显示每个更新之间的差异。 --stat 显示每次更新的文件修改统计信息。 --shortstat 只显示 --stat 中最后的行数修改添加移除统计。 --name-only 仅在提交信息后显示已修改的文件清单。 --name-status 显示新增、修改、删除的文件清单。 --abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 --relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 --graph 显示 ASCII 图形表示的分支合并历史。 --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 撤消操作修改最后一次提交使用 --amend 选项重新提交： 1$ git commit --amend 此命令将使用当前的暂存区域快照提交。如果刚才提交完没有作任何改动，直接运行此命令的话，相当于有机会重新编辑提交说明，但将要提交的文件快照和之前的一样。 取消已经暂存的文件1$ git reset HEAD benchmarks.rb 取消对文件的修改1$ git checkout -- benchmarks.rb 这条命令有些危险，所有对文件的修改都没有了，因为我们刚刚把之前版本的文件复制过来重写了此文件。所以在用这条命令前，请务必确定真的不再需要保留刚才的修改。 远程仓库的使用同他人协作开发某个项目时，需要管理这些远程仓库，以便推送或拉取数据，分享各自的工作进展。 查看当前的远程库 git remote 命令，它会列出每个远程库的简短名字。在克隆完某个项目后，至少可以看到一个名为 origin 的远程库，Git 默认使用这个名字来标识你所克隆的原始仓库： 123456$ git remote -v bakkdoor git://github.com/bakkdoor/grit.git cho45 git://github.com/cho45/grit.git defunkt git://github.com/defunkt/grit.git koke git://github.com/koke/grit.git origin git@github.com:mojombo/grit.git 添加远程仓库git remote add [shortname] [url] 从远程仓库抓取数据1$ git fetch [remote-name] 此命令会到远程仓库中拉取所有你本地仓库中还没有的数据。 fetch 命令只是将远端的数据拉到本地仓库，并不自动合并到当前工作分支，只有当你确实准备好了，才能手工合并。 推送数据到远程仓库1git push [remote-name] [branch-name] 只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。如果在你推数据前，已经有其他人推送了若干更新，那你的推送操作就会被驳回。你必须先把他们的更新抓取到本地，合并到自己的项目中，然后才可以再次推送。 查看远程仓库信息git remote show [remote-name] 远程仓库的删除和重命名1$ git remote rename pb paul 1$ git remote rm paul 打标签Git 也可以对某一时间点上的版本打上标签。 列显已有的标签列出现有标签的命令非常简单，直接运行 git tag 即可： 123$ git tag v0.1 v1.3 用特定的搜索模式列出符合条件的标签。 12345$ git tag -l &apos;v1.4.2.*&apos; v1.4.2.1 v1.4.2.2 v1.4.2.3 v1.4.2.4 新建标签Git 使用的标签有两种类型：轻量级的（lightweight）和含附注的（annotated）。轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明 含附注的标签创建一个含附注类型的标签非常简单，用 -a 12345$ git tag -a v1.4 -m &apos;my version 1.4&apos; $ git tag v0.1 v1.3 v1.4 轻量级标签轻量级标签实际上就是一个保存着对应提交对象的校验和信息的文件。 1234567$ git tag v1.4-lw $ git tag v0.1 v1.3 v1.4 v1.4-lw v1.5 分享标签默认情况下，git push 并不会把标签传送到远端服务器上，git push origin [tagname] 如果要一次推送所有本地新增的标签上去，可以使用 git push origin --tags git分支使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。 Git 保存的不是文件差异或者变化量，而只是一系列文件快照。在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次提交的作者等相关附属信息，包含零个或多个指向该提交对象的父对象指针：首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。 当使用 git commit 新建一个提交对象前，Git 会先计算每一个子目录（本例中就是项目根目录）的校验和，然后在 Git 仓库中将这些目录保存为树（tree）对象。之后 Git 创建的提交对象，除了包含相关提交信息以外，还包含着指向这个树对象（项目根目录）的指针，如此它就可以在将来需要的时候，重现此次快照的内容了。 作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针 Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针，Git 会使用 master 作为分支的默认名字。它在每次提交的时候都会自动向前移动。 创建一个新的分支指针。比如新建一个 testing 分支，可以使用 git branch 命令： 1$ git branch testing Git 是如何知道你当前在哪个分支上工作的呢？其实答案也很简单，它保存着一个名为 HEAD 的特别指针。在 Git 中，它是一个指向你正在工作中的本地分支的指针 切换到其他分支，==本质就是将HEAD指针指向不同的分支==，可以执行 git checkout 命令 1$ git checkout testing 每次提交后 HEAD 随着分支一起向前移动 由于 Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，所以创建和销毁一个分支就变得非常廉价。 分支的新建与合并分支的新建与切换 新建的分支取名为 iss53。要新建并切换到该分支，运行 git checkout 并加上 -b 参数： 1$ git checkout -b iss53 相当于执行下面这两条命令： 12$ git branch iss53$ git checkout iss53 在提交了若干次更新后，iss53 分支的指针也会随着向前推进 再创建一个hotfix 分支展开工作，切换分支时，Git 会把工作目录的内容恢复为检出某分支时它所指向的那个提交对象的快照。它会自动添加、删除和修改文件以确保目录的内容和你当时提交时完全一样。 合并master和hotfix 12$ git checkout master$ git merge hotfix 注意，合并时出现了==“Fast forward”==的提示。由于当前 master 分支所在的提交对象是要并入的 hotfix 分支的直接上游，Git 只需把 master 分支指针直接右移。 hotfix 已经完成了历史使命，可以删掉了。使用 git branch 的 -d 选项执行删除操作： 1$ git branch -d hotfix 分支的合并运行git merge 命令指定要合并进来的分支 1git merge iss53 注意，这次合并操作的底层实现，并不同于之前 hotfix 的并入方式。Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算。 Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。 遇到冲突时的分支合并如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（译注：逻辑上说，这种问题只能由人来裁决。） 任何包含未解决冲突的文件都会以未合并（unmerged）的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记，可以通过它们来手工定位并解决这些冲突。 1234567&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html &lt;div id="footer"&gt;contact : email.support@github.com&lt;/div&gt; ======= &lt;div id="footer"&gt; please contact us at support@github.com &lt;/div&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html 解决了所有文件里的所有冲突后，运行 git add 将把它们标记为已解决状态 分支的管理git branch 命令不仅仅能创建和删除分支，如果不加任何参数，它会给出当前所有分支的清单： 1234$ git branch iss53 * master ## 分支前的 * 字符：它表示当前所在的分支 testing 查看各个分支最后一个提交对象的信息，运行 git branch -v 筛选出你已经（或尚未）与当前分支合并的分支，可以用 --merge 和 --no-merged 选项比如用 git branch --merge 查看哪些分支已被并入当前分支（译注：也就是说哪些分支是当前分支的直接上游。）： 12$ git branch --merged$ git branch --no-merged 列表中没有 * 的分支通常都可以用 git branch -d 来删掉。但分支中还包含着尚未合并进来的工作成果会提示错误，因为那样做会丢失数据：可以用大写的删除选项 -D 强制执行 远程分支远程分支（remote branch）是对远程仓库中的分支的索引。它们是一些无法移动的本地分支；只有在 Git 进行网络交互时才会更新。 (远程仓库名)/(分支名) 这样的形式表示远程分支。比如我们想看看上次同 origin 仓库通讯时 master 分支的样子，就应该查看origin/master 分支。 克隆Git 会自动为你将此远程仓库命名为 origin，并下载其中所有的数据，建立一个指向它的 master 分支的指针，在本地命名为 origin/master，但你无法在本地更改其数据。接着，Git 建立一个属于你自己的本地 master 分支，始于 origin 上 master 分支相同的位置，你可以就此开始工作： 一次 Git 克隆会建立你自己的本地分支 master 和远程分支 origin/master，并且将它们都指向 origin 上的 master 分支（==注意：git的分布式特性，虽然实际上都指向本地克隆后的分支，但本地克隆和远端分支是一模一样的，所以即也等效于指向了远程分支 origin/master，但注意此时有两个分支指针，local和remote==）。 如果你在本地 master 分支做了些改动，与此同时，其他人向 git.ourcompany.com 推送了他们的更新，那么服务器上的 master 分支就会向前推进，而于此同时，你在本地的提交历史正朝向不同方向发展。不过只要你不和服务器通讯，你的 origin/master 指针仍然保持原位不会移动。 在本地工作的同时有人向远程仓库推送内容会让提交历史开始分流。 可以运行 git fetch origin 来同步远程服务器上的数据到本地。该命令首先找到 origin 是哪个服务器（本例为 git.ourcompany.com），从上面获取你尚未拥有的数据，更新你本地的数据库，然后把 origin/master 的指针移到它最新的位置上 推送本地分支git push (远程仓库名) (分支名) 若想把远程分支叫作 awesomebranch，可以用 git push origin serverfix:awesomebranch 跟踪远程分支从远程分支 checkout 出来的本地分支，称为 跟踪分支 (tracking branch)。跟踪分支是一种和某个远程分支有直接联系的本地分支。在跟踪分支里输入 git push，Git 会自行推断应该向哪个服务器的哪个分支推送数据。同样，在这些分支里运行 git pull 会获取所有远程索引，并把它们的数据都合并到本地分支中来。 git checkout -b [分支名][远程名]/[分支名] 删除远程分支git push [远程名] :[分支名] git push [远程名] [本地分支]:[远程分支] 语法，如果省略 [本地分支]，那就等于是在说“在这里提取空白然后把它变成[远程分支]” 分支的衍合把一个分支中的修改整合到另一个分支的办法有两种：merge 和 rebase merge它会把两个分支最新的快照（C3 和 C4）以及二者最新的共同祖先（C2）进行三方合并，合并的结果是产生一个新的提交对象（C5） 衍合（rebase）把在一个分支里提交的改变移到另一个分支里重放一遍。 12$ git checkout experiment$ git rebase master 它的原理是回到两个分支最近的共同祖先，根据当前分支（experiment）后续的历次提交对象（C3），生成一系列文件补丁，然后以基底分支（ master）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3’），从而改写 experiment 的提交历史，使它成为 master 分支的直接下游 回到 master 分支，进行一次快进合并,==注意这里的master没有指向C3‘== 注意，合并结果中最后一次提交所指向的快照，无论是通过衍合，还是三方合并，都会得到相同的快照内容，只不过提交历史不同罢了。衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。 一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁 ,衍合能产生一个更为整洁的提交历史。 衍合的风险一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行衍合操作。 在进行衍合的时候，实际上抛弃了一些现存的提交对象而创造了一些类似但不同的新的提交对象。如果你把原来分支中的提交对象发布出去，并且其他人更新下载后在其基础上开展工作，而稍后你又用 git rebase 抛弃这些提交对象，把新的重演后的提交对象发布出去的话，你的合作者就不得不重新合并他们的工作，这样当你再次从他们那里获取内容时，提交历史就会变得一团糟。 如图，C7先merge了C3和C6，后另一个人将C4按基C5 rebase，合并后master上指向C4’，然后下面的人需要mergeC4’ 和C7，此时C7已包含C4，C4‘内容包含了C4的改动，但此时衍合产生的提交对象 C4’ 的 SHA-1 校验值和之前 C4 完全不同，所以 Git 会把它们当作新的提交对象处理，把相同的内容又合并了一遍 服务器上的 Git远程仓库通常只是一个裸仓库（bare repository） — 即一个没有当前工作目录的仓库。因为该仓库只是一个合作媒介，所以不需要从硬盘上取出最新版本的快照；仓库里存放的仅仅是 Git 的数据。简单地说，裸仓库就是你工作目录中 .git 子目录内的内容。 协议Git 可以使用四种主要的协议来传输数据：本地传输，SSH 协议，Git 协议和 HTTP 协议 分布式 Git集中式工作流一个存放代码仓库的中心服务器，可以接受所有开发者提交的代码。 如果两个开发者从中心仓库克隆代码下来，同时作了一些修订，那么只有第一个开发者可以顺利地把数据推送到共享服务器。第二个开发者在提交他的修订之前，必须先下载合并服务器上的数据，解决冲突之后才能推送数据到共享服务器上。 集成管理员工作流由于 Git 允许使用多个远程仓库，开发者便可以建立自己的公共仓库，往里面写数据并共享给他人，而同时又可以从别人的仓库中提取他们的更新过来。这种情形通常都会有个代表着官方发布的项目仓库（blessed repository），开发者们由此仓库克隆出一个自己的公共仓库（developer public），然后将自己的提交推送上去，请求官方仓库的维护者拉取更新合并到主项目。维护者在自己的本地也有个克隆仓库（integration manager），他可以将你的公共仓库作为远程仓库添加进来，经过测试无误后合并到主干分支，然后再推送到官方仓库。 项目维护者可以推送数据到公共仓库 blessed repository。 贡献者克隆此仓库，修订或编写新代码。 贡献者推送数据到自己的公共仓库 developer public。 贡献者给维护者发送邮件，请求拉取自己的最新修订。 维护者在自己本地的 integration manger 仓库中，将贡献者的仓库加为远程仓库，合并更新并做测试。 维护者将合并后的更新推送到主仓库 blessed repository。 司令官与副官工作流负责集成项目中的特定部分，所以称为副官（lieutenant）。而所有这些集成管理员头上还有一位负责统筹的总集成管理员，称为司令官（dictator）。司令官维护的仓库用于提供所有协作者拉取最新集成的项目代码。 一般的开发者在自己的特性分支上工作，并不定期地根据主干分支（dictator 上的 master）衍合。 副官（lieutenant）将普通开发者的特性分支合并到自己的 master 分支中。 司令官（dictator）将所有副官的 master 分支并入自己的 master 分支。 司令官（dictator）将集成后的 master 分支推送到共享仓库 blessed repository 中，以便所有其他开发者以此为基础进行衍合。 这种工作流程并不常用，只有当项目极为庞杂，或者需要多级别管理时，才会体现出优势。（分而治之） git 工具储藏（Stashing）当你正在进行项目中某一部分的工作，里面的东西处于一个比较杂乱的状态，而你想转到其他分支上进行一些工作。问题是，你不想提交进行了一半的工作，否则以后你无法回到这个工作点。解决这个问题的办法就是git stash命令。 “‘储藏”“可以获取你工作目录的中间状态——也就是你修改过的被追踪的文件和暂存的变更——并将它保存到一个未完结变更的堆栈中，随时可以重新应用。 储藏你的工作为了往堆栈推送一个新的储藏，只要运行 git stash： 这时，你可以方便地切换到其他分支工作；你的变更都保存在栈上。要查看现有的储藏，你可以使用 git stash list 重写历史改变最近一次提交两件基本事情：改变提交说明，或者改变你刚刚通过增加，改变，删除而记录的快照。 1$ git commit --amend git commit --amend会获取你当前的暂存区并将它作为新提交对应的快照。 修正会改变提交的SHA-1值。这个很像是一次非常小的rebase——不要在你最近一次提交被推送后还去修正它。 Git挂钩当某些重要事件发生时，Git 以调用自定义脚本。有两组挂钩：客户端和服务器端。客户端挂钩用于客户端的操作，如提交和合并。服务器端挂钩用于 Git 服务器端的操作，如接收被推送的提交。 安装一个挂钩挂钩都被存储在 Git 目录下的hooks子目录中，即大部分项目中的.git/hooks。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 关键字]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%9F%BA%E7%A1%80%2F%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[staticstatic表示“全局”或者“静态”的意思，用来修饰成员变量和成员方法，也可以形成静态static代码块被static修饰的成员变量和成员方法独立于该类的任何对象,属于这个类。 静态变量和实例变量 对于静态变量在内存中只有一个拷贝（节省内存），JVM只为静态分配一次内存，在加载类的过程中完成静态变量的内存分配，可用类名直接访问（方便）。 对于实例变量，每创建一个实例，就会为实例变量分配一次内存，实例变量可以在内存中有多个拷贝，互不影响（灵活）。 所以一般在需要实现以下两个功能时使用静态变量：1.在对象之间共享值时 2.方便访问变量时 静态方法 静态方法可以直接通过类名调用，任何的实例也都可以调用，因此静态方法中不能用this和super关键字，不能直接访问所属类的实例变量和实例方法(就是不带static的成员变量和成员成员方法)，只能访问所属类的静态成员变量和成员方法。 static代码块 static代码块也叫静态代码块，是在类中独立于类成员的static语句块，可以有多个，位置可以随便放，它不在任何的方法体内，JVM加载类时会执行这些静态的代码块，如果static代码块有多个，JVM将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。 内部静态类参考内部类中的静态嵌套类 finalfinal数据:向编译器告知一块数据恒定不变,基本类型数值不变,引用类型引用不变 final参数:无法更改参数引用所指向的对象 final方法:锁定方法,继承类无法修改它的含义. private方法默认是final的 final类: 禁止继承该类 static final:全局常量 传值和传引用 传的是参数的一份拷贝,无论是简单数据类型还是引用对于参数传递，如果是简单数据类型，那么它传递的是值拷贝，对于类的实例它传递的是类的引用。需要注意的是，这条规则只适用于参数传递。 抽象类和接口抽象方法:只有声明没有方法体,抽象方法必须为public或者protected,默认是public abstract void f(); 抽象类:包含抽象方法的类叫做抽象类.抽象类不能用来创建对象 abstract class C{}; 接口:interface,一个完全抽象类,没有提供任何具体实现,只提供形式,接口方法是public的,实现它必须是public 接口冲突:编译报错继承可以扩展接口接口中可以含有变量和方法。但是要注意，接口中的变量会被隐式地指定为public static final变量（并且只能是public static final变量），而方法会被隐式地指定为public abstract方法且只能是public abstract方法,接口中的方法必须都是抽象方法 区别: 语法层面上的区别 1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法； 2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的； 3）接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法； 4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。 设计层面上的区别 1) 抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。 2)抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范。对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更；而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 因为继承多个有实现的类容易出现菱形继承的问题，即，两个父类继承自同一个基类，则子类中会包含两份祖父类的内容，不合并重复内容会引起一些歧义，而合并重复内容又会导致类成员的内存布局不能简单复制地从父类复制。 内部类成员内部类 成员内部类是最普通的内部类，它的定义为位于另一个类的内部 内部类拥有其外围类的所有元素的访问权. 在拥有外部类对象前是不可能创建内部类对象的 .this和.new 内部类对外部类对象的引用: 外部类名.this 外部类对象创建内部类对象: 外部类对象.new 外部类名.内部类名(); 编译器会默认为成员内部类添加了一个指向外部类对象的引用 内部类可以拥有private访问权限、protected访问权限、public访问权限及包访问权限。比如上面的例子，如果成员内部类Inner用private修饰，则只能在外部类的内部访问，如果用public修饰，则任何地方都能访问；如果用protected修饰，则只能在同一个包下或者继承外部类的情况下访问；如果是默认访问权限，则只能在同一个包下访问 局部内部类 局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内。不能有public、protected、private以及static修饰符的。 匿名内部类 创建一个继承自XXX类的匿名类的对象 匿名内部类引用外部变量是必须声明是final的 静态内部类如果不需要内部类对象与外部类对象有关系,可以将内部类声明为static,在静态嵌套类内部，不能访问外部类的非静态成员 区别: 静态内部类不能访问其外部类的非静态成员变量和方法 在非静态内部类中不可以声明静态成员 创建静态类内部对象时，不需要其外部类的对象 作用 java中的内部类和接口加在一起，可以实现多继承。() 可以使某些编码根简洁。(匿名内部类) 隐藏你不想让别人知道的操作。 在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问多继承即用多个内部类继承多个父类,外部类添加内部类引用,就可以实现外部类调用多个父类,也就是多继承 transient我们都知道一个对象只要实现了Serilizable接口，这个对象就可以被序列化，Java的这种序列化模式为开发者提供了很多便利，我们可以不必关系具体序列化的过程，只要这个类实现了Serilizable接口，这个的所有属性和方法都会自动序列化。 如果用transient声明一个实例变量，当对象存储时，它的值不需要维持。换句话来说就是，用transient关键字标记的成员变量不参与序列化过程.换句话说，这个字段的生命周期仅存于调用者的内存中。]]></content>
      <categories>
        <category>java基础</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CountDownLatch 和 CyclicBarrier 的区别]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[CountDownLatch 和 CyclicBarrier 的区别？ (1) CountDownLatch 的作用是允许 1 个线程等待其他线程执行完成之后， 它才执行；而 CyclicBarrier 则是允许 N 个线程相互等待到某个公共屏障点，然 后这一组线程再同时执行。 (2) CountDownLatch 的计数器的值无法被重置，这个初始值只能被设置一 次，是不能够重用的；CyclicBarrier 是可以重用的。 Semaphore 可以控制某个资源可被同时访问的个数，通过构造函数设定一定数量的许 可，通过 acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许 可。]]></content>
      <categories>
        <category>java基础</category>
        <category>java并发</category>
      </categories>
      <tags>
        <tag>java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发容器List/Set]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8set-list%2F</url>
    <content type="text"><![CDATA[对于List或者Set 而言，增、删操作其实都是针对整个容器，因此每次操作都不可避免的需要锁定整个容器空间，性能肯定会大打折扣。要实现一个线程安全的List/Set，只需要在修改操作的时候进行同步即可， 比如使用java.util.Collections.synchronizedList(List) 或者java.util.Collections.synchronizedSet(Set) 。当然也可以使用Lock 来实现线程安全的List/Set。 通常情况下我们的高并发都发生在“多读少写”的情况，因此如果能够实现一种更优秀的算法这对生产环境还是很有好处的。ReadWriteLock 当然是一种实现。CopyOnWriteArrayList/CopyOnWriteArraySet 确实另外一种思路。 CopyOnWriteArrayList/CopyOnWriteArraySet 的基本思想是一旦对容器有修改，那么就“复制”一份新的集合，在新的集合上修改，然后将新集合复制给旧的引用。当然了这部分少不了要加锁。显然对于CopyOnWriteArrayList/CopyOnWriteArraySet 来说最大的好处就是“读”操作不需要锁了。 读实现 List 仍然是基于数组的实现，因为只有数组是最快的。 为了保证无锁的读操作能够看到写操作的变化，因此数组array 是volatile 类型的。 get/indexOf/iterator 等操作都是无锁的，同时也可以看到所操作的都是某一时刻array的镜像（这得益于数组是不可变化的） add/set/remove/clear 等元素变化的都是需要加锁的，这里使用的是ReentrantLock。 /** The array, accessed only via getArray/setArray. */ private volatile transient Object[] array; public E get(int index) { return (E)(getArray()[index]); } private static int indexOf(Object o, Object[] elements, int index, int fence) { if (o == null) { for (int i = index; i &lt; fence; i++) if (elements[i] == null) return i; } else { for (int i = index; i &lt; fence; i++) if (o.equals(elements[i])) return i; } return -1; } public Iterator&lt;E&gt; iterator() { return new COWIterator&lt;E&gt;(getArray(), 0); } 写实现public void clear() { final ReentrantLock lock = this.lock; lock.lock(); try { setArray(new Object[0]); } finally { lock.unlock(); } } public E set(int index, E element) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); Object oldValue = elements[index]; if (oldValue != element) { int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len); newElements[index] = element; setArray(newElements); } else { // Not quite a no-op; ensures volatile write semantics setArray(elements); } return (E)oldValue; } finally { lock.unlock(); } } final void setArray(Object[] a) { array = a; } 对于set 操作，如果元素有变化，修改后setArray(newElements);将新数组赋值还好理解。那么如果一个元素没有变化，也就是上述代码的else 部分，为什么还需要进行一个无谓的setArray 操作？ 复制private final CopyOnWriteArrayList&lt;E&gt; al; /** * Creates an empty set. */ public CopyOnWriteArraySet() { al = new CopyOnWriteArrayList&lt;E&gt;(); } public boolean add(E e) { return al.addIfAbsent(e); } 对于CopyOnWriteArraySet ,只是持有一个CopyOnWriteArrayList，仅仅在add/addAll 的时候检测元素是否存在，如果存在就不加入集合中。]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发容器Map]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8Map%2F</url>
    <content type="text"><![CDATA[Map 可用的线程安全版本Map 实现是ConcurrentHashMap/ConcurrentSkipListMap/Hashtable/Properties 四个，但是Hashtable 是过时的类库，因此如果可以的应该尽可能的使用ConcurrentHashMap 和ConcurrentSkipListMap。 ConcurrentHashMap除了实现Map 接口里面对象的方法外，ConcurrentHashMap 还实现了ConcurrentMap 里面的四个方法。 API V putIfAbsent(K key,V value)如果不存在key对应的值,则将value以key加入Map,否则返回key对应的旧值 boolean remove(Object key,Object value)只有目前将键的条目映射到给定值时，才移除该键的条目 boolean replace(K key,V oldValue,V newValue) 只有目前将键的条目映射到给定值时，才替换该键的条目 V replace(K key,V value) 只有当前键存在的时候更新此键对于的值。 HashMap原理哈希算法:是将任意长度的二进制值映射为固定长度的较小二进制值 对象数组table size 描述的是Map 中元素的大小 threshold 描述的是达到指定元素个数后需要扩容 loadFactor 是扩容因子(loadFactor&gt;0)，也就是计算threshold 的。 table.length 数组的大小。 如果存取的元素大小达到了table.length*loadFactor 个，那么就需要扩充容量了。在HashMap 中每次扩容就是将扩大数组的一倍，使数组大小为原来的两倍。 key 是一个无尽的超大集合，而table 是一个较小的有限集合，那么一种方式就是将key 编码后的hashCode 值取模映射到table 上，但是由于与(&amp;)是比取模(%)更高效的操作，因此Java 中采用hash 值与数组大小-1后取与来确定数组索引的。 static int indexFor(int h, int length) { return h &amp; (length-1); } 解决碰撞冲突 同一个索引的数组元素组成一个链表，查找允许时循环链表找到需要的元素。 尽可能的将元素均匀的分布在数组上。 第一点table 中每一个元素是一个Map.Entry,这里链表上所有元素的hash 经过清单1 的indexFor 后将得到相同的数组索引；next 是指向下一个元素的索引，同一个链表上的元素就是通过next 串联起来的。第二点 static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 第三点构造数组时数组的长度是2的倍数,loadFactor 的默认值0.75和capacity 的默认值16是经过大量的统计分析得出的 get操作public V get(Object key) { if (key == null) return getForNullKey(); int hash = hash(key.hashCode()); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)];e != null;e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) //不同的hash 可能映射到同一个table[index]上，而相同的key 却同时映射到相同的hash 上 return e.value; } return null; } put 操作public V put(K key, V value) { if (key == null) return putForNullKey(value); int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); //查找index for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { //查找列表 Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); //添加新元素 return null; } void addEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e);//同时添加在首部,next指向e if (size++ &gt;= threshold) resize(2 * table.length); } 先查找,有则替换,无则添加在首部 HashMap 扩容void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); } //重新分布Map中的元素 void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } 扩充过程会导致元素数据的所有元素进行重新hash 计算，这个过程也叫rehash。显然这是一个非常耗时的过程，否则扩容都会导致所有元素重新计算hash。因此尽可能的选择合适的初始化大小是有效提高HashMap 效率的关键。太大了会导致过多的浪费空间， 太小了就可能会导致繁重的rehash 过程。在这个过程中loadFactor 也可以考虑。 …ConcurrentHashMap原理默认情况下ConcurrentHashMap 是用了16个类似HashMap 的结构，其中每一个HashMap 拥有一个独占锁。也就是说最终的效果就是通过某种Hash 算法，将任何一个元素均匀的映射到某个HashMap 的Map.Entry 上面，而对某个一个元素的操作就集中在其分布的HashMap上，与其它HashMap 无关。这样就支持最多16个并发的写操作。ConcurrentHashMap 将整个对象列表分为segmentMask+1个片段（Segment）。其中每一个片段是一个类似于HashMap 的结构，它有一个HashEntry 的数组，数组的每一项又是一个链表，通过HashEntry 的next 引用串联起来。 对于一次Map 的查找来说，首先就需要定位到Segment，然后从过Segment 定位到HashEntry 链表，最后才是通过遍历链表得到需要的元素。 要解决并发问题，加锁是必不可免的。Segment 除了有一个volatile 类型的元素大小count 外， Segment 还是集成自ReentrantLock 的。如果是读操作不加锁，写操作加锁，对于竞争资源来说就需要定义为volatile 类型的。所以volatile 能够近似保证正确性的情况下最大程度的降低加锁带来的影响，同时还与写操作的锁不产生冲突。 同时为了防止在遍历HashEntry 的时候被破坏，那么对于HashEntry 的数据结构来说，除了value 之外其他属性就应该是常量， 否则不可避免的会得到ConcurrentModificationException。这就是为什么HashEntry 数据结构中key,hash,next 是常量的原因(final 类型）。 定位Segmentprivate static int hash(int h) { h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16); } final Segment&lt;K,V&gt; segmentFor(int hash) { return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask]; } 显然在不能够对Segment 扩容的情况下， segments 的大小就应该是固定的。所以在ConcurrentHashMap 中segments/segmentMask/segmentShift 都是常量，一旦初始化后就不能被再次修改，其中segmentShift 是查找Segment 的一个常量偏移量。 有了Segment 以后再定位HashEntry 就和HashMap 中定位HashEntry 一样了，先将hash 值与Segment 中HashEntry 的大小减1进行与操作定位到HashEntry 链表，然后遍历链表就可以完成相应的操作了。 get操作V get(Object key, int hash) { if (count != 0) { // read-volatile HashEntry&lt;K,V&gt; e = getFirst(hash); while (e != null) { if (e.hash == hash &amp;&amp; key.equals(e.key)) { V v = e.value; if (v != null) return v; //如果为空还需要加锁再读取一次,尽管ConcurrentHashMap 不允许将value 为null 的值加入，但现在仍然能 //够读到一个为空的value 就意味着此值对当前线程还不可见（这是因为HashEntry 还没有完全构造完成就赋值导致的， return readValueUnderLock(e); // recheck } e = e.next; } } return null; } HashEntry&lt;K,V&gt; getFirst(int hash) { HashEntry&lt;K,V&gt;[] tab = table; return tab[hash &amp; (tab.length - 1)]; } V readValueUnderLock(HashEntry&lt;K,V&gt; e) { lock(); try { return e.value; } finally { unlock(); } } 描述在Segment中如何定位元素。首先判断Segment 的大小count&gt;0，Segment 的大小描述的是HashEntry 不为空(key 不为空)的个数。如果Segment 中存在元素那么就通过getFirst 定位到指定的HashEntry 链表的头节点上，然后遍历此节点，一旦找到key 对应的元素后就返回其对应的值。 put操作修改一个竞争资源肯定是要加锁的 V put(K key, int hash, V value, boolean onlyIfAbsent) { lock(); try { int c = count; if (c++ &gt; threshold) // ensure capacity rehash(); HashEntry&lt;K,V&gt;[] tab = table; int index = hash &amp; (tab.length - 1); HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) //找到e e = e.next; V oldValue; if (e != null) { oldValue = e.value; if (!onlyIfAbsent) e.value = value; } else { oldValue = null; ++modCount; tab[index] = new HashEntry&lt;K,V&gt;(key, hash, first, value); count = c; // write-volatile } return oldValue; } finally { unlock(); } } remove同put 一样，remove 也需要加锁，这是因为对table 可能会有变更。由于HashEntry 的next 节点是final 类型的，所以一旦删除链表中间一个元素，就需要将删除之前或者之后的元素重新加入新的链表。而Segment 采用的是将删除元素之前的元素一个个重新加入删除之后的元素之前（也就是链表头结点）来完成新链表的构造。 V remove(Object key, int hash, Object value) { lock(); try { int c = count - 1; HashEntry&lt;K,V&gt;[] tab = table; int index = hash &amp; (tab.length - 1); HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue = null; if (e != null) { V v = e.value; if (value == null || value.equals(v)) { oldValue = v; //将删除元素之前的元素一个个重新加入删除之后的元素之前（也就是链表头结点）来完成新链表的构造。 //新构建了列表,复杂,因为是final ++modCount; HashEntry&lt;K,V&gt; newFirst = e.next; for (HashEntry&lt;K,V&gt; p = first; p != e; p = p.next) newFirst = new HashEntry&lt;K,V&gt;(p.key, p.hash,newFirst, p.value); tab[index] = newFirst; count = c; // write-volatile } } return oldValue; } finally { unlock(); } } 例如原来列表是 1-&gt;2-&gt;3-&gt;4-&gt;5 删除3后为 2-&gt;1-&gt;4-&gt;5 size操作size 操作涉及到统计所有Segment 的大小，这样就会遍历所有的Segment，如果每次加锁就会导致整个Map 都被锁住了，任何需要锁的操作都将无法进行。这里用到了一个比较巧妙的方案解决此问题。 在Segment 中有一个变量modCount，用来记录Segment 结构变更的次数，结构变更包括增加元素和删除元素，每增加一个元素操作就+1，每进行一次删除操作+1，每进行一次清空操作(clear)就+1。也就是说每次涉及到元素个数变更的操作modCount 都会+1，而且一直是增大的，不会减小。遍历两次ConcurrentHashMap 中的segments，每次遍历是记录每一个Segment 的modCount，比较两次遍历的modCount 值的和是否相同，如果相同就返回在遍历过程中获取的Segment的count 的和，也就是所有元素的个数。如果不相同就重复再做一次。重复一次还不相同就将所有Segment 锁住，一个一个的获取其大小(count)，最后将这些count 加起来得到总的大小。当然了最后需要将锁一一释放 public int size() { final Segment&lt;K,V&gt;[] segments = this.segments; long sum = 0; long check = 0; int[] mc = new int[segments.length]; //用不加锁方法检测RETRIES_BEFORE_LOCK次 for (int k = 0; k &lt; RETRIES_BEFORE_LOCK; ++k) { check = 0; sum = 0; int mcsum = 0; for (int i = 0; i &lt; segments.length; ++i) { //顺序不可换。所以修改modCount 总是在修改count之前，也就是说如果读取到了一个count //的值，那么在count 变化之前的modCount 也就能够读取到 sum += segments[i].count; mcsum += mc[i] = segments[i].modCount; } if (mcsum != 0) { for (int i = 0; i &lt; segments.length; ++i) { check += segments[i].count; if (mc[i] != segments[i].modCount) { check = -1; // force retry break; } } } if (check == sum) break; } //加锁检查 if (check != sum) { // Resort to locking all segments sum = 0; for (int i = 0; i &lt; segments.length; ++i) segments[i].lock(); for (int i = 0; i &lt; segments.length; ++i) sum += segments[i].count; for (int i = 0; i &lt; segments.length; ++i) segments[i].unlock(); } //返回结果 if (sum &gt; Integer.MAX_VALUE) return Integer.MAX_VALUE; else return (int)sum; }]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java多线程条件变量]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[Condition 条件变量很大一个程度上是为了解决Object.wait/notify/notifyAll 难以使用的问题。 举例: Object.wait/notify/notifyAll表明只有持有锁的线程才可以调用 ,而synchronized表示加锁, 在生产者和消费者(只有一个产品),因为获取锁顺序不同, 会同时有多个生产者和消费者在等待,notify时,会通知生产者或消费者,实际只需要其中一个,不满足的会重新挂起,就会冗余 条件（也称为条件队列或条件变量）为线程提供了一个含义，以便在某个状态条件现在可能为true 的另一个线程通知它之前，一直挂起该线程（即让其“等待”）。因为访问此共享状态信息发生在不同的线程中，所以它必须受保护，因此要将某种形式的锁与该条件相关联。等待提供一个条件的主要属性是：以原子方式释放相关的锁，并挂起当前线程. ##API##获取： Lock.newCondition() void await() throws InterruptedException; void awaitUninterruptibly(); long awaitNanos(long nanosTimeout) throws InterruptedException; boolean await(long time, TimeUnit unit) throws InterruptedException; boolean awaitUntil(Date deadline) throws InterruptedException; void signal(); void signalAll(); 每一个 Lock 可以有任意数据的 Condition 对象，Condition 是与 Lock 绑定的，所以就有Lock的公平性特性：如果是公平锁，线程为按照 FIFO 的顺序从Condition.await 中释放，如果是非公平锁，那么后续的锁竞争就不保证FIFO 顺序了。 await* 操作await()操作实际上就是释放锁，然后挂起线程，一旦条件满足就被唤醒，再次获取锁！ public final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) { LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } 对于一个Condition.await()而言，如果释放了锁，要想再一次获取锁那么就需要进入队列，等待被通知获取锁。完整的await()操作是安装如下步骤进行的： 将当前线程加入Condition 锁队列。特别说明的是，这里不同于 AQS的队列，这里进入的是Condition 的FIFO 队列。 释放锁。 自旋(while)挂起，直到被唤醒或者超时或者CACELLED等 获取锁(acquireQueued)。并将自己从Condition 的FIFO 队列中释放。 一个Condition 可以在多个地方被await*()，那么就需要一个FIFO 的结构将这些Condition 串联起来，然后根据需要唤醒一个或者多个（通常是所有）。所以在Condition 内部就需要一个FIFO 的队列。 private transient Node firstWaiter; private transient Node lastWaiter; signal/signalAll 操作 按照signal/signalAll 的需求，就是要将Condition.await*()中 FIFO 队列中第一个 Node唤醒（或者全部 Node）唤醒。尽管所有 Node可能都被唤醒，但是要知道的是仍然只有一个线程能够拿到锁，其它没有拿到锁的线程仍然需要自旋等待 private void doSignal(Node first) { do { if ((firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; } while (!transferForSignal(first) &amp;&amp;(first = firstWaiter) != null); } private void doSignalAll(Node first) { lastWaiter = firstWaiter = null; do { Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; } while (first != null); } 上面的代码很容易看出来，signal 就是唤醒 Condition队列中的第一个非 CANCELLED 节点线程，而signalAll 就是唤醒所有非CANCELLED 节点线程。当然了遇到CANCELLED 线程就需要将其从FIFO 队列中剔除。]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发容器queue]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8Queue%2F</url>
    <content type="text"><![CDATA[Queue Queue并不是线程安全的，为了解决这个问题，引入了可阻塞的队列BlockingQueue。对于BlockingQueue 而言所有操作的是线程安全的，并且队列的操作可以被阻塞，直到满足某种条件。 ConcurrentLinkedQueue一个基于链接节点的无界线程安全队列。此队列按照FIFO（先进先出）原则对元素进行排序。 所有结构（head/tail/item/next）都是volatile 类型。 所有结构的操作都带有原子操作 由于队列中任何一个节点（Node）只有下一个节点的引用，所以这个队列是单向的 没有对队列长度进行计数，所以队列的长度是无限的 初始情况下队列头和队列尾都指向一个空节点，但是非null，这是为了方便操作，不需要每次去判断head/tail 是否为空。但是head 却不作为存取元素的节点，tail 在不等于head 情况下保存一个节点元素 入队public boolean offer(E e) { if (e == null) throw new NullPointerException(); Node&lt;E&gt; n = new Node&lt;E&gt;(e, null); for (;;) { Node&lt;E&gt; t = tail; Node&lt;E&gt; s = t.getNext(); if (t == tail) { if (s == null) { if (t.casNext(s, n)) { casTail(t, n); return true; } } else { casTail(t, s); } } } } CAS 操作来修改.包括几个if(); 出队public E poll() { for (;;) { Node&lt;E&gt; h = head; Node&lt;E&gt; t = tail; Node&lt;E&gt; first = h.getNext(); if (h == head) { if (h == t) { if (first == null) return null; else casTail(t, first); } else if (casHead(h, first)) { E item = first.getItem(); if (item != null) { first.setItem(null); return item; } // else skip over deleted item, continue loop, } } } } 头结点是为了标识队列起始，也为了减少空指针的比较，所以头结点总是一个item 为null的非null 节点。也就是说head!=null 并且head.item==null 总是成立。所以实际上获取的是head.next，一旦将头结点head 设置为head.next 成功就将新head 的item 设置为null。至于以前就的头结点h，h.item=null 并且h.next 为新的head，但是由于没有对h 的引用，所以最终会被GC回收。 遍历队列大小public int size() { int count = 0; for (Node&lt;E&gt; p = first(); p != null; p = p.getNext()) { if (p.getItem() != null) { // Collections.size() spec says to max out if (++count == Integer.MAX_VALUE) break; } } return count; } BlockingQueue 对于Queue 来说，BlockingQueue 是主要的线程安全版本。这是一个可阻塞的版本，也就是允许添加/删除元素被阻塞，直到成功为止。 BlockingQueue 相对于Queue 而言增加了两个操作：put/take LinkedBlockingQueue 原理 引入了两个锁，一个入队列锁，一个出队列锁。当然同时有一个队列不满的Condition和一个队列不空的Condition。一个锁意味着入队列和出队列同时只能有一个在进行，另一个必须等待其释放锁。而从ConcurrentLinkedQueue 的实现原理来看，事实上head 和last (ConcurrentLinkedQueue 中是tail)是分离的，互相独立的，这意味着入队列实际上是不会修改出队列的数据的，同时出队列也不会修改入队列，也就是说这两个操作是互不干扰的。 阻塞的入队列public void put(E e) throws InterruptedException { if (e == null) throw new NullPointerException(); int c = -1; final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try { try { while (count.get() == capacity) notFull.await(); } catch (InterruptedException ie) { notFull.signal(); // propagate to a non-interrupted thread throw ie; } insert(e); c = count.getAndIncrement(); if (c + 1 &lt; capacity) notFull.signal(); } finally { putLock.unlock(); } if (c == 0) signalNotEmpty(); } 如果在入队列的时候线程被中断，那么就需要发出一个notFull 的信号，表示下一个入队列的线程能够被唤醒（如果阻塞的话）。 入队列成功后如果队列不满需要补一个notFull 的信号。 入队列的过程允许被中断，所以总是抛出InterruptedException 异常。 如果队列不为空并且可能有一个元素的话就唤醒一个出队列线程。这么做说明之前队列一定为空，因为在加入队列之后队列最多只能为1，那么说明未加入之前是0，那么就可能有被阻塞的出队列线程，所以就唤醒一个出队列线程 notify丢失通知问题假设线程A 因为某种条件在条件队列中等待，同时线程B 因为另外一种条件在同一个条件队列中等待，也就是说线程A/B 都被同一个Conditon.await()挂起，但是等待的条件不同。现在假设线程B 的线程被满足，线程C 执行一个notify 操作，此时JVM 从Conditon.await()的多个线程（A/B）中随机挑选一个唤醒，不幸的是唤醒了A。此时A 的条件不满足，于是A 继续挂起。而此时B 仍然在傻傻的等待被唤醒的信号。也就是说本来给B 的通知却被一个无关的线程持有了，真正需要通知的线程B 却没有得到通知，而B 仍然在等待一个已经发生过的通知。 调用notifyall 会唤醒所有线程，然后这N 个线程竞争同一个锁，最多只有一个线程能够得到锁，于是其它线程又回到挂起状态。这意味每一次唤醒操作可能带来大量的上下文切换（如果N 比较大的话），同时有大量的竞争锁的请求。 出队列过程public E take() throws InterruptedException { E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try { try { while (count.get() == 0) notEmpty.await(); } catch (InterruptedException ie) { notEmpty.signal(); // propagate to a non-interrupted thread throw ie; } x = extract(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); } finally { takeLock.unlock(); } if (c == capacity) signalNotFull(); return x; } 获取出队列的锁takeLock，检测队列大小，如果队列为空，那么就挂起线程，等待队列不为空notEmpty 的唤醒。 将元素从头部移除，同时修改队列头部引用head。 队列大小减1。 释放锁takeLock。 唤醒notFull 线程（如果有挂起的入队列线程），告诉生产者，现在还有空闲的空间。为什么有异常在锁机制里面也是总遇到，这是因为，Java 里面没有一种直接的方法中断一个挂起的线程，所以通常情况下等于一个处于WAITING 状态的线程，允许设置一个中断位，一旦线程检测到这个中断位就会从WAITING 状态退出，以一个InterruptedException 的异常返回。所以只要是对一个线程挂起操作都会导致InterruptedException 的可能，比如Thread.sleep()、Thread.join()、Object.wait()。尽管LockSupport.park()不会抛出一个InterruptedException 异常，但是它会将当前线程的的interrupted 状态位置上，而对于Lock/Condition 而言，当捕捉到interrupted 状态后就认为线程应该终止任务，所以就抛出了一个InterruptedException 异常。 查询队列头元素public E peek() { if (count.get() == 0) return null; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try { Node&lt;E&gt; first = head.next; if (first == null) return null; else return first.item; } finally { takeLock.unlock(); } } 这里读取了Node的item 值，但是整个过程却是使用了takeLock 而非putLock。换句话说putLock 对Node.item的操作，peek()线程可能不可见！所以Node.item是volatile的 队列尾部加入元素private void insert(E x) { last = last.next = new Node&lt;E&gt;(x); } last=new Node(x)可能发生重排序 构建一个Node 对象n； 将Node 的n 赋给last 初始化n，设置item=x 在第二步peek 线程可能拿到了新的Node n，这时候它读取item，得到了一个null。显然这是不可靠的。所以对item 采用volatile. 出队了poll/take 和peek 都是使用的takeLock 锁，所以不会导致此问题。 删除操作和遍历操作由于同时获取了takeLock 和putLock，所以也不会导致此问题。 批量从队列中移除元素由于批量操作只需要一次获取锁，所以效率会比每次获取锁要高。但是需要说明的，需要同时获取takeLock/putLock 两把锁，因为当移除完所有元素后这会涉及到尾节点的修改（last 节点仍然指向一个已经移走的节点）。 public int drainTo(Collection&lt;? super E&gt; c, int maxElements) { if (c == null) throw new NullPointerException(); if (c == this) throw new IllegalArgumentException(); fullyLock(); try { int n = 0; Node&lt;E&gt; p = head.next; while (p != null &amp;&amp; n &lt; maxElements) { c.add(p.item); p.item = null; p = p.next; ++n; } if (n != 0) { head.next = p; assert head.item == null; if (p == null) last = head; if (count.getAndAdd(-n) == capacity) notFull.signalAll(); } return n; } finally { fullyUnlock(); } } ArrayBlockingQueue 原理 入队列就将尾索引往右移动一个，新元素加入尾索引的位置； 出队列就将头索引往尾索引方向移动一个，同时将旧头索引元素设为null，返回旧头索引的元素。 一旦数组已满，那么就不允许添加新元素（除非扩充容量） 如果尾索引移到了数组的最后（最大索引处），那么就从索引0开始，形成一个“闭合”的数组。 由于头索引和尾索引之间的元素都不能为空（因为为空不知道take 出来的元素为空还是队列为空），所以删除一个头索引和尾索引之间的元素的话，需要移动删除索引前面或者后面的所有元素，以便填充删除索引的位置。 由于是阻塞队列，那么显然需要一个锁，另外由于只是一份数据（一个数组），所以只能有一个锁，也就是同时只能有一个线程操作队列 首先有一个数组E[]，用来存储所有的元素。由于ArrayBlockingQueue 最终设置为一个不可扩展大小的Queue，所以这里items 就是初始化就固定大小的数组（final 类型）；另外有两个索引，头索引takeIndex，尾索引putIndex；一个队列的大小count；要支持阻塞就必须需要一个锁lock 和两个条件（非空、满），这三个元素都是不可变更类型的（final）。 添加元素先判断数量,再决定挂起 public void put(E e) throws InterruptedException { if (e == null) throw new NullPointerException(); final E[] items = this.items; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { try { while (count == items.length) notFull.await(); } catch (InterruptedException ie) { notFull.signal(); // propagate to non-interrupted thread throw ie; } insert(e); } finally { lock.unlock(); } } 移除元素先判断数量,再决定挂起 public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { try { while (count == 0) notEmpty.await(); } catch (InterruptedException ie) { notEmpty.signal(); // propagate to non-interrupted thread throw ie; } E x = extract(); return x; } finally { lock.unlock(); } } 数据结构 这个队列是循环的 删除任意一个元素public boolean remove(Object o) { if (o == null) return false; final E[] items = this.items; final ReentrantLock lock = this.lock; lock.lock(); try { int i = takeIndex; int k = 0; for (;;) { if (k++ &gt;= count) return false; if (o.equals(items[i])) { removeAt(i); return true; } i = inc(i); } } finally { lock.unlock(); } } void removeAt(int i) { final E[] items = this.items; // if removing front item, just advance if (i == takeIndex) { items[takeIndex] = null; takeIndex = inc(takeIndex); } else { // slide over all others up through putIndex. for (;;) { int nexti = inc(i); if (nexti != putIndex) { items[i] = items[nexti]; i = nexti; } else { items[i] = null; putIndex = i; break; } } } --count; notFull.signal(); } 对于其他的操作，由于都是带着Lock 的操作 PriorityBlockingQueuePriorityBlockingQueue 是无界的，因此就只有非空的信号，也就是说只有take()才能阻塞， put 是永远不会阻塞（ 除非达到Integer.MAX_VALUE 直到抛出一个OutOfMemoryError 异常）。 只有take()操作的时候才可能因为队列为空而挂起。同时其它需要操作队列变化和大小的只需要使用独占锁ReentrantLock 就可以了，非常方便。需要说明的是PriorityBlockingQueue采用了一个公平的锁 直接交换的 BlockingQueue—SynchronousQueue ##每个插入操作必须等待另一个线程的移除操作，同样任何一个移除操作都等待另一个线程的插入操作。因此此队列内部其实没有任何一个元素，或者说容量是0，严格说并不是一种容器。由于队列没有容量，因此不能调用peek 操作，因为只有移除元素时才有元素。 SynchronousQueue 内部没有容量，但是由于一个插入操作总是对应一个移除操作，反过来同样需要满足。那么一个元素就不会再SynchronousQueue 里面长时间停留，一旦有了插入线程和移除线程，元素很快就从插入线程移交给移除线程。也就是说这更像是一种信道（管道），资源从一个方向快速传递到另一方向。 尽管元素在SynchronousQueue 内部不会“停留”，但是并不意味之SynchronousQueue 内部没有队列。实际上SynchronousQueue 维护者线程队列，也就是插入线程或者移除线程在不同时存在的时候就会有线程队列。既然有队列，同样就有公平性和非公平性特性，公平性保证正在等待的插入线程或者移除线程以FIFO 的顺序传递资源。 显然这是一种快速传递元素的方式，也就是说在这种情况下元素总是以最快的方式从插入着（生产者）传递给移除着（消费者），这在多任务队列中是最快处理任务的方式。 DelayQueue它描述的是一种延时队列。这个队列的特性是，队列中的元素都要延迟时间（超时时间），只有一个元素达到了延时时间才能出队列，也就是说每次从队列中获取的元素总是最先到达延时的元素。这种队列的场景就是计划任务。比如以前要完成计划任务，很有可能是使用Timer/TimerTask，这是一种循环检测的方式，也就是在循环里面遍历所有元素总是检测元素是否满足条件，一旦满足条件就执行相关任务。显然这中方式浪费了很多的检测工作，因为大多数时间总是在进行无谓的检测。而DelayQueue 却能避免这种无谓的检测 单向队列总结 如果不需要阻塞队列，优先选择ConcurrentLinkedQueue；如果需要阻塞队列，队列大小固定优先选择ArrayBlockingQueue，队列大小不固定优先选择LinkedBlockingQueue；如果需要对队列进行排序，选择PriorityBlockingQueue；如果需要一个快速交换的队列，选择SynchronousQueue；如果需要对队列中的元素进行延时操作，则选择DelayQueue。 Deque ArrayDeque ArrayDeque 并不是一个固定大小的队列，每次队列满了以后就将队列容量扩大一倍（doubleCapacity()），因此加入一个元素总是能成功，而且也不会抛出一个异常。 LinkList 在示意图中，LinkedList 总是有一个“傀儡”节点，用来描述队列“头部”，但是并不表示头部元素，它是一个执行null 的空节点。 双向链表的数据结构比较简单，操作起来也比较容易，从事从“傀儡”节点开始，“傀儡”节点的下一个元素就是队列的头部，前一个元素是队列的尾部，换句话说，“傀儡”节点在头部和尾部之间建立了一个通道，是整个队列形成一个循环，这样就可以从任意一个节点的任意一个方向能遍历完整的队列。 LinkedBlockDeque 要想支持阻塞功能，队列的容量一定是固定的，否则无法在入队的时候挂起线程。也就是capacity 是final 类型的。 既然是双向链表，每一个结点就需要前后两个引用，这样才能将所有元素串联起来，支持双向遍历。也即需要prev/next 两个引用。 双向链表需要头尾同时操作，所以需要first/last 两个节点，当然可以参考LinkedList那样采用一个节点的双向来完成，那样实现起来就稍微麻烦点。 既然要支持阻塞功能，就需要锁和条件变量来挂起线程。这里使用一个锁两个条件变量来完成此功能。 实现略]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java线程池]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[线程池API 大多数并发应用程序是围绕执行任务（Task）进行管理的。把一个应用程序的工作（work）分离到任务中，可以简化程序的管理；这种分离还在不同事物间划分了自然的分界线，可以方便程序在出现错误时进行恢复；同时这种分离还可以为并行工作提供一个自然的结构，有利于提高程序的并发性。 所谓的任务拆分就是确定每一个执行任务（工作单元）的边界。理想情况下独立的工作单元有最大的吞吐量，这些工作单元不依赖于其它工作单元的状态、结果或者其他资源等。因此将任务尽可能的拆分成一个个独立的工作单元有利于提高程序的并发性。 任务的执行策略包括4W3H 部分： 任务在什么（What）线程中执行 任务以什么（What）顺序执行（FIFO/LIFO/优先级等） 同时有多少个（How Many）任务并发执行 允许有多少个（How Many）个任务进入执行队列 系统过载时选择放弃哪一个（Which）任务，如何（How）通知应用程序这个动作 任务执行的开始、结束应该做什么（What）处理 如何满足上面的条件 首先明确一定是在Java 里面可以供使用者调用的启动线程类是Thread。因此Runnable 或者Timer/TimerTask 等都是要依赖Thread 来启动的，因此在ThreadPool 里面同样也是靠Thread 来启动多线程的。 默认情况下Runnable 接口执行完毕后是不能拿到执行结果的，因此在ThreadPool里就定义了一个Callable 接口来处理执行结果。 为了异步阻塞的获取结果，Future 可以帮助调用线程获取执行结果。 Executor 解决了向线程池提交任务的入口问题，同时ScheduledExecutorService 解决了如何进行重复调用任务的问题。 CompletionService 解决了如何按照执行完毕的顺序获取结果的问题，这在某些情况下可以提高任务执行的并发，调用线程不必在长时间任务上等待过多时间。 显然线程的数量是有限的，而且也不宜过多，因此合适的任务队列是必不可少的，BlockingQueue 的容量正好可以解决此问题。 固定任务容量就意味着在容量满了以后需要一定的策略来处理过多的任务（新任务），RejectedExecutionHandler 正好解决此问题。 一定时间内阻塞就意味着有超时，因此TimeoutException 就是为了描述这种现象。TimeUnit 是为了描述超时时间方便的一个时间单元枚举类。 有上述问题就意味了配置一个合适的线程池是很复杂的，因此Executors 默认的一些线程池配置可以减少这个操作。 线程池的类体系结构 Executor 的execute 方法只是执行一个Runnable 的任务 ExecutorService 在Executor 的基础上增加了一些方法，其中有两个核心的方法： Future&lt;?&gt; submit(Runnable task) Future submit(Callable task) 这两个方法都是向线程池中提交任务，它们的区别在于Runnable 在执行完毕后没有结果，Callable 执行完毕后有一个结果。这在多个线程中传递状态和结果是非常有用的。另外他们的相同点在于都返回一个Future 对象。Future 对象可以阻塞线程直到运行完毕（获取结果，如果有的话），也可以取消任务执行，当然也能够检测任务是否被取消或者是否执行完毕。 ScheduledExecutorService 描述的功能和Timer/TimerTask 类似，解决那些需要任务重复执行的问题。这包括延迟时间一次性执行、延迟时间周期性执行以及固定延迟时间周期性执行等。当然了继承ExecutorService 的ScheduledExecutorService 拥有ExecutorService 的全部特性。 ScheduledThreadPoolExecutor 是继承ThreadPoolExecutor 的ScheduledExecutorService 接口实现，周期性任务调度的类实现 CompletionService 接口，它是用于描述顺序获取执行结果的一个线程池包装器 Executors 类里面提供了一些静态工厂，生成一些常用的线程池 newSingleThreadExecutonewSingleThreadExecutor：创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newFixedThreadPool：创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newCachedThreadPool：创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 newScheduledThreadPool：创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 newSingleThreadScheduledExecutor：创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。 退出线程是有多种执行状态的，同样管理线程的线程池也有多种状态。JVM 会在所有线程（非后台daemon 线程）全部终止后才退出，为了节省资源和有效释放资源关闭一个线程池就显得很重要。有时候无法正确的关闭线程池，将会阻止JVM 的结束。 线程池Executor 是异步的执行任务，因此任何时刻不能够直接获取提交的任务的状态。这些任务有可能已经完成，也有可能正在执行或者还在排队等待执行。因此关闭线程池可能出现一下几种情况： 平缓关闭：已经启动的任务全部执行完毕，同时不再接受新的任务 立即关闭：取消所有正在执行和未执行的任务 线程池的四种状态: 线程池在构造前（new 操作）是初始状态，一旦构造完成线程池就进入了执行状态RUNNING 线程池运行中可以通过shutdown()和shutdownNow()来改变运行状态 一旦shutdown()或者shutdownNow()执行完毕，线程池就进入TERMINATED 状态，此时线程池就结束了。 isTerminating()描述的是SHUTDOWN 和STOP 两种状态。 isShutdown()描述的是非RUNNING 状态，也就是SHUTDOWN/STOP/TERMINATED三种状态。 线程池API shutdownNow()会返回那些已经进入了队列但是还没有执行的任务列表awaitTermination 描述的是等待线程池关闭的时间，如果等待时间线程池还没有关闭将会抛出一个超时异常。 平缓关闭线程池使用shutdown() 立即关闭线程池使用shutdownNow()，同时得到未执行的任务列表 检测线程池是否正处于关闭中，使用isShutdown() 检测线程池是否已经关闭使用isTerminated() 定时或者永久等待线程池关闭结束使用awaitTermination()操作 线程池数据结构与线程构造方法 线程池需要支持多个线程并发执行，因此有一个线程集合Collection来执行线程任务； 涉及任务的异步执行，因此需要有一个集合来缓存任务队列Collection； 很显然在多个线程之间协调多个任务，那么就需要一个线程安全的任务集合，同时还需要支持阻塞、超时操作，那么BlockingQueue 是必不可少的； 既然是线程池，出发点就是提高系统性能同时降低资源消耗，那么线程池的大小就有限制，因此需要有一个核心线程池大小（线程个数）和一个最大线程池大小（线程个数），有一个计数用来描述当前线程池大小； 如果是有限的线程池大小，那么长时间不使用的线程资源就应该销毁掉，这样就需要一个线程空闲时间的计数来描述线程何时被销毁； 前面描述过线程池也是有生命周期的，因此需要有一个状态来描述线程池当前的运行状态； 线程池的任务队列如果有边界，那么就需要有一个任务拒绝策略来处理过多的任务，同时在线程池的销毁阶段也需要有一个任务拒绝策略来处理新加入的任务； 上面种的线程池大小、线程空闲实际那、线程池运行状态等等状态改变都不是线程安全的，因此需要有一个全局的锁（mainLock）来协调这些竞争资源； 除了以上数据结构以外，ThreadPoolExecutor 还有一些状态用来描述线程池的运行计数，例如线程池运行的任务数、曾经达到的最大线程数创建线程 public interface ThreadFactory {Thread newThread(Runnable r); } static class DefaultThreadFactory implements ThreadFactory {static final AtomicInteger poolNumber = new AtomicInteger(1); final ThreadGroup group; final AtomicInteger threadNumber = new AtomicInteger(1); final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null)? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = &quot;pool-&quot; + poolNumber.getAndIncrement() + &quot;-thread-&quot;; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; } }同一个线程池的所有线程属于同一个线程组，也就是创建线程池的那个线程组 另外对于线程池中的所有线程默认都转换为非后台线程，这样主线程退出时不会直接退出JVM，而是等待线程池结束。还有一点就是默认将线程池中的所有线程都调为同一个级别，这样在操作系统角度来看所有系统都是公平的，不会导致竞争堆积。 线程池中线程生命周期一个线程Worker 被构造出来以后就开始处于运行状态。 private final class Worker implements Runnable { private final ReentrantLock runLock = new ReentrantLock(); private Runnable firstTask; Thread thread; Worker(Runnable firstTask) { this.firstTask = firstTask; } private void runTask(Runnable task) { final ReentrantLock runLock = this.runLock; runLock.lock(); try { task.run(); } finally { runLock.unlock(); } } public void run() { try { Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) { runTask(task); task = null; } } finally { workerDone(this); } } } 当提交一个任务时，如果需要创建一个线程（何时需要在下一节中探讨）时，就调用线程工厂创建一个线程，同时将线程绑定到Worker 工作队列中。需要说明的是，Worker 队列构造的时候带着一个任务Runnable 一旦线程池启动线程后（调用线程run()）方法，那么线程工作队列Worker 就从第1个任务开始执行（这时候发现构造Worker 时传递一个任务的好处了），一旦第1个任务执行完毕，就从线程池的任务队列中取出下一个任务进行执行。循环如此，直到线程池被关闭或者任务抛出了一个RuntimeException。 线程池任务执行流程 任务可能在将来某个时刻被执行，有可能不是立即执行。 任务可能在一个新的线程中执行或者线程池中存在的一个线程中执行。 任务无法被提交执行有以下两个原因：线程池已经关闭或者线程池已经达到了容量限制。 所有失败的任务都将被“当前”的任务拒绝策略RejectedExecutionHandler 处理。 public void execute(Runnable command) { //1 if (command == null) throw new NullPointerException(); //2,3 if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) { //4 if (runState == RUNNING &amp;&amp; workQueue.offer(command)) { //5 if (runState != RUNNING || poolSize == 0) //6 ensureQueuedTaskHandled(command); } //7 else if (!addIfUnderMaximumPoolSize(command)) reject(command); // is shutdown or saturated } } 上述过程: 如果任务command 为空，则抛出空指针异常，返回。否则进行2。 如果当前线程池大小大于或等于核心线程池大小，进行4。否则进行3。 创建一个新工作队列（线程，参考上一节），成功直接返回，失败进行4。 如果线程池正在运行并且任务加入线程池队列成功，进行5，否则进行7。 如果线程池已经关闭或者线程池大小为0，进行6，否则直接返回。 如果线程池已经关闭则执行拒绝策略返回，否则启动一个新线程来进行执行任务，返回。 如果线程池大小不大于最大线程池数量，则启动新线程来进行执行，否则进行拒绝策略，结束。 何时任务立即执行 runState == RUNNING &amp;&amp; ( poolSize &lt; corePoolSize || poolSize &lt; maxnumPoolSize &amp;&amp; workQueue.isFull()) submitpublic Future&lt;?&gt; submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;Object&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); execute(ftask); return ftask; } public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask; } Future 在Future 接口中提供了5个方法。 V get() throws InterruptedException, ExecutionException： 等待计算完成，然后获取其结果。 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException,TimeoutException。最多等待为使计算完成所给定的时间之后，获取其结果（如果结果可用）。 boolean cancel(boolean mayInterruptIfRunning)：试图取消对此任务的执行。 boolean isCancelled()：如果在任务正常完成前将其取消，则返回true。 boolean isDone()：如果任务已完成，则返回true。可能由于正常终止、异常或取消而完成，在所有这些情况中，此方法都将返回true。 执行初始情况下任务状态state=0，任务执行(innerRun)后状态变为运行状态RUNNING(state=1)，执行完毕后变成运行结束状态RAN(state=2)。任务在初始状态或者执行状态被取消后就变为状态CANCELLED(state=4) void innerRun() { if (!compareAndSetState(0, RUNNING)) return; try { runner = Thread.currentThread(); if (getState() == RUNNING) // recheck after setting thread innerSet(callable.call()); else releaseShared(0); // cancel } catch (Throwable ex) { innerSetException(ex); } } 执行一个任务有四步：设置运行状态、设置当前线程（AQS 需要）、执行任务(Runnable#run或者Callable#call）、设置执行结果。这里也可以看到，一个任务只能执行一次，因为执行完毕后它的状态不在为初始值0，要么为CANCELLED，要么为RAN。 取消任务boolean innerCancel(boolean mayInterruptIfRunning) { for (;;) { int s = getState(); if (ranOrCancelled(s)) return false; if (compareAndSetState(s, CANCELLED)) break; } if (mayInterruptIfRunning) { Thread r = runner; if (r != null) r.interrupt(); } releaseShared(0); done(); return true; } 获取结果V innerGet() throws InterruptedException, ExecutionException { acquireSharedInterruptibly(0); if (getState() == CANCELLED) throw new CancellationException(); if (exception != null) throw new ExecutionException(exception); return result; } //AQS#acquireSharedInterruptibly public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); //park current Thread for result } protected int tryAcquireShared(int ignore) { return innerIsDone()? 1 : -1; } boolean innerIsDone() { return ranOrCancelled(getState()) &amp;&amp; runner == null; } 当调用Future#get()的时候尝试去获取一个共享变量。这就涉及到AQS 的使用方式了。这里获取一个共享变量的状态是任务是否结束(innerIsDone())，也就是任务是否执行完毕或者被取消。如果不满足条件，那么在AQS 中就会doAcquireSharedInterruptibly(arg)挂起当前线程，直到满足条件 延迟、周期性任务调度的实现ScheduledThreadPoolExecutor 和ThreadPoolExecutor 的唯一区别在于任务是有序（按照执行时间顺序）的，并且需要到达时间点（临界点）才能执行，并不是任务队列中有任务就需要执行的。也就是说唯一不同的就是任务队列BlockingQueueworkQueue 不一样。ScheduledThreadPoolExecutor 的任务队列是java.util.concurrent.ScheduledThreadPoolExecutor.DelayedWorkQueue ， 它是基于java.util.concurrent.DelayQueue队列的实现。 由于DelayQueue 在获取元素时需要检测元素是否“可用”，也就是任务是否达到“临界点”（指定时间点），因此加入元素和移除元素会有一些额外的操作。 移除元素（出队列）的过程是这样的： 总是检测队列的头元素（顺序最小元素，也是最先达到临界点的元素） 检测头元素与当前时间的差，如果大于0，表示还未到底临界点，因此等待响应时间（使用条件变量available) 如果小于或者等于0，说明已经到底临界点或者已经过了临界点，那么就移除头元素，并且唤醒其它等待任务队列的线程。 同样加入元素也会有相应的条件变量操作。当前仅当队列为空或者要加入的元素比队列中的头元素还小的时候才需要唤醒“等待线程”去检测元素。因为头元素都没有唤醒那么比头元素更延迟的元素就更加不会唤醒。]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClassLoader]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjvm%2FClassLoader%2F</url>
    <content type="text"><![CDATA[ClassLoaderhttp://blog.csdn.net/briblue/article/details/54973413 JAVA类加载流程Java语言系统自带有三个类加载器: Bootstrap ClassLoader 最顶层的加载类，主要加载核心类库，%JRE_HOME%\lib下的rt.jar、resources.jar、charsets.jar和class等。另外需要注意的是可以通过启动jvm时指定-Xbootclasspath和路径来改变Bootstrap ClassLoader的加载目录。比如java -Xbootclasspath/a:path被指定的文件追加到默认的bootstrap路径中。我们可以打开我的电脑，在上面的目录下查看，看看这些jar包是不是存在于这个目录。 Extention ClassLoader 扩展的类加载器，加载目录%JRE_HOME%\lib\ext目录下的jar包和class文件。还可以加载-D java.ext.dirs选项指定的目录。 Appclass Loader也称为SystemAppClass 加载当前应用的classpath的所有类。 我们上面简单介绍了3个ClassLoader。说明了它们加载的路径。并且还提到了-Xbootclasspath和-D java.ext.dirs这两个虚拟机参数选项。 加载顺序 Bootstrap CLassloder Extention ClassLoader AppClassLoader BootstrapClassLoader、ExtClassLoader、AppClassLoader实际是查阅相应的环境属性sun.boot.class.path、java.ext.dirs和java.class.path来加载资源文件的。 ClassLoader cl = Test.class.getClassLoader(); //ClassLoader is:sun.misc.Launcher$AppClassLoader@73d16e93 int.class.getClassLoader();//int.class是由Bootstrap ClassLoader加载的 每个类加载器都有一个父加载器比如加载Test.class是由AppClassLoader完成，那么AppClassLoader也有一个父加载器，通过getParent方法 ClassLoader is:sun.misc.Launcher$AppClassLoader@73d16e93 ClassLoader&apos;s parent is:sun.misc.Launcher$ExtClassLoader@15db9742 AppClassLoader的父加载器是ExtClassLoader,但通过getParent方法获取ExtClassLoader的父加载器空指针异常,注意父加载器不是父类,先看加载器类图 URLClassLoader的源码中并没有找到getParent()方法。这个方法在ClassLoader.java中。 public abstract class ClassLoader { // The parent class loader for delegation // Note: VM hardcoded the offset of this field, thus all new fields // must be added *after* it. private final ClassLoader parent; // The class loader for the system // @GuardedBy(&quot;ClassLoader.class&quot;) private static ClassLoader scl; private ClassLoader(Void unused, ClassLoader parent) { this.parent = parent; ... } protected ClassLoader(ClassLoader parent) { this(checkCreateClassLoader(), parent); } protected ClassLoader() { this(checkCreateClassLoader(), getSystemClassLoader()); } public final ClassLoader getParent() { if (parent == null) return null; return parent; } public static ClassLoader getSystemClassLoader() { initSystemClassLoader(); if (scl == null) { return null; } return scl; } private static synchronized void initSystemClassLoader() { if (!sclSet) { if (scl != null) throw new IllegalStateException(&quot;recursive invocation&quot;); sun.misc.Launcher l = sun.misc.Launcher.getLauncher(); if (l != null) { Throwable oops = null; //通过Launcher获取ClassLoader scl = l.getClassLoader(); try { scl = AccessController.doPrivileged( new SystemClassLoaderAction(scl)); } catch (PrivilegedActionException pae) { oops = pae.getCause(); if (oops instanceof InvocationTargetException) { oops = oops.getCause(); } } if (oops != null) { if (oops instanceof Error) { throw (Error) oops; } else { // wrap the exception throw new Error(oops); } } } sclSet = true; } } } getParent()实际上返回的就是一个ClassLoader对象parent，parent的赋值是在ClassLoader对象的构造方法中，它有两个情况： 由外部类创建ClassLoader时直接指定一个ClassLoader为parent。 由getSystemClassLoader()方法生成，也就是在sun.misc.Laucher通过getClassLoader()获取，也就是AppClassLoader。直白的说，一个ClassLoader创建时如果没有指定parent，那么它的parent默认就是AppClassLoader。 private ClassLoader loader;public Launcher() { // Create the extension class loader ClassLoader extcl; try { extcl = ExtClassLoader.getExtClassLoader(); } catch (IOException e) { } try { //将ExtClassLoader对象实例传递进去 loader = AppClassLoader.getAppClassLoader(extcl); } catch (IOException e) {} }public ClassLoader getClassLoader() { return loader; } 代码已经说明了问题AppClassLoader的parent是一个ExtClassLoader实例。ExtClassLoader的parent为null。 那么BootstrapClassLoader呢？ Bootstrap ClassLoader是由C++编写的。Bootstrap ClassLoader是由C/C++编写的，它本身是虚拟机的一部分，所以它并不是一个JAVA类，也就是无法在java代码中获取它的引用，JVM启动时通过Bootstrap类加载器加载rt.jar等核心jar包中的class文件，之前的int.class,String.class都是由它加载。JVM初始化sun.misc.Launcher并创建Extension ClassLoader和AppClassLoader实例。并将ExtClassLoader设置为AppClassLoader的父加载器。Bootstrap没有父加载器，但是它却可以作用一个ClassLoader的父加载器。比如ExtClassLoader。 双亲委托一个类加载器查找class和resource时，是通过“委托模式”进行的，它首先判断这个class是不是已经加载成功，如果没有的话它并不是自己进行查找，而是先通过父加载器，然后递归下去，直到Bootstrap ClassLoader，如果Bootstrap classloader找到了，直接返回，如果没有找到，则一级一级返回，最后到达自身去查找这些对象。这种机制就叫做双亲委托。 重要方法loadClass()JDK文档中是这样写的，通过指定的全限定类名加载class，它通过同名的loadClass(String,boolean)方法。 protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException 一般实现这个方法的步骤是 执行findLoadedClass(String)去检测这个class是不是已经加载过了。 执行父加载器的loadClass方法。如果父加载器为null，则jvm内置的加载器去替代，也就是Bootstrap ClassLoader。这也解释了ExtClassLoader的parent为null,但仍然说Bootstrap ClassLoader是它的父加载器。 如果向上委托父加载器没有加载成功，则通过findClass(String)查找 如果class在上面的步骤中找到了，参数resolve又是true的话，那么loadClass()又会调用resolveClass(Class)这个方法来生成最终的Class对象。 protected Class&lt;?&gt; loadClass(String name, boolean resolve)throws ClassNotFoundException{ synchronized (getClassLoadingLock(name)) { // 首先，检测是否已经加载 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { //父加载器不为空则调用父加载器的loadClass c = parent.loadClass(name, false); } else { //父加载器为空则调用Bootstrap Classloader c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //父加载器没有找到，则调用findclass c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { //调用resolveClass() resolveClass(c); } return c; } } 自定义ClassLoader自定义步骤 编写一个类继承自ClassLoader抽象类。 复写它的findClass()方法。 在findClass()方法中调用defineClass()。 defineClass() 这个方法在编写自定义classloader的时候非常重要，它能将class二进制内容转换成Class对象，如果不符合要求的会抛出各种异常。 一个ClassLoader创建时如果没有指定parent，那么它的parent默认就是AppClassLoader。 public class DiskClassLoader extends ClassLoader { private String mLibPath; public DiskClassLoader(String path) { // TODO Auto-generated constructor stub mLibPath = path; } @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { // TODO Auto-generated method stub String fileName = getFileName(name); File file = new File(mLibPath,fileName); try { FileInputStream is = new FileInputStream(file); ByteArrayOutputStream bos = new ByteArrayOutputStream(); int len = 0; try { while ((len = is.read()) != -1) { bos.write(len); } } catch (IOException e) { e.printStackTrace(); } byte[] data = bos.toByteArray(); is.close(); bos.close(); return defineClass(name,data,0,data.length); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } return super.findClass(name); } //获取要加载 的class文件名 private String getFileName(String name) { // TODO Auto-generated method stub int index = name.lastIndexOf(&apos;.&apos;); if(index == -1){ return name+&quot;.class&quot;; }else{ return name.substring(index)+&quot;.class&quot;; } } } 调用 //创建自定义classloader对象。 DiskClassLoader diskLoader = new DiskClassLoader(&quot;D:\\lib&quot;); //加载class文件 Class c = diskLoader.loadClass(&quot;com.frank.test.Test&quot;); 关键字 路径BootStrap ClassLoader、ExtClassLoader、AppClassLoader都是加载指定路径下的jar包。如果我们要突破这种限制，实现自己某些特殊的需求，我们就得自定义ClassLoader，自已指定加载的路径，可以是磁盘、内存、网络或者其它。 常见的用法是将Class文件按照某种加密手段进行加密，然后按照规则编写自定义的ClassLoader进行解密，这样我们就可以在程序中加载特定了类，并且这个类只能被我们自定义的加载器进行加载，提高了程序的安全性。 Context ClassLoader 线程上下文类加载器public class Thread implements Runnable { /* The context ClassLoader for this thread */ private ClassLoader contextClassLoader; public void setContextClassLoader(ClassLoader cl) { SecurityManager sm = System.getSecurityManager(); if (sm != null) { sm.checkPermission(new RuntimePermission(&quot;setContextClassLoader&quot;)); } contextClassLoader = cl; } public ClassLoader getContextClassLoader() { if (contextClassLoader == null) return null; SecurityManager sm = System.getSecurityManager(); if (sm != null) { ClassLoader.checkClassLoaderPermission(contextClassLoader, Reflection.getCallerClass()); } return contextClassLoader; } } 每个Thread都有一个相关联的ClassLoader，默认是AppClassLoader。并且子线程默认使用父线程的ClassLoader除非子线程特别设置。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>ClassLoader</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java多线程锁机制]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[java.util.concurrent.locks.LockAPIvoid lock();//获取锁。如果锁不可用，出于线程调度目的，将禁用当前线程，并且在获得锁之前，该线程将一直处于休眠状态。 void lockInterruptibly() throws InterruptedException;//如果当前线程未被中断，则获取锁。 Condition newCondition();//返回绑定到此Lock 实例的新Condition 实例。 void unlock();//释放锁 ...... AQSAbstractQueuedSynchronizer，简称AQS，是J.U.C 最复杂的一个类。AbstractQueuedSynchronizer 是CountDownLatch/FutureTask/ReentrantLock/RenntrantReadWriteLock/Semaphore 的基础，因此AbstractQueuedSynchronizer是Lock/Executor 实现的前提。 两个操作获取锁：首先判断当前状态是否允许获取锁，如果是就获取锁，否则就阻塞操作或者获取失败，也就是说如果是独占锁就可能阻塞，如果是共享锁就可能失败。另外如果是阻塞线程，那么线程就需要进入阻塞队列。当状态位允许获取锁时就修改状态，并且如果进了队列就从队列中移除。 释放锁:这个过程就是修改状态位，如果有线程因为状态位阻塞的话就唤醒队列中的一个或者更多线程。 条件： 原子性操作同步器的状态位：int_32表示，CAS操作 阻塞和唤醒线程：LockSupport.park(),LockSupport.unpark() …. 有序队列:采用CHL 列表 CHL模型采用下面的算法完成FIFO 的入队列和出队列过程。 对于入队列(enqueue)：采用CAS 操作，每次比较尾结点是否一致，然后插入的到尾结点中。 对于出队列(dequeue):由于每一个节点也缓存了一个状态，决定是否出队列，因此当不满足条件时就需要自旋等待，一旦满足条件就将头结点设置为下一个节点。 核心字段 private volatile int state; private transient volatile Node head; private transient volatile Node tail; 其中state 描述的有多少个线程取得了锁，对于互斥锁来说state&lt;=1。head/tail 加上CAS 操作就构成了一个CHL 的FIFO 队列。 Node 结构体 volatile int waitStatus; //节点的等待状态，一个节点可能位于以下几种状态： • CANCELLED = 1： 节点操作因为超时或者对应的线程被interrupt。节点不应该留 在此状态，一旦达到此状态将从CHL 队列中踢出。 • SIGNAL = -1： 节点的继任节点是（或者将要成为）BLOCKED 状态（例如通过 LockSupport.park()操作），因此一个节点一旦被释放（解锁）或者取消就需要唤醒 （LockSupport.unpack()）它的继任节点。 • CONDITION = -2：表明节点对应的线程因为不满足一个条件（Condition）而被阻塞。 • 0： 正常状态，新生的非CONDITION 节点都是此状态。 • 非负值标识节点不需要被通知（唤醒）。 volatile Node prev;//此节点的前一个节点。节点的 waitStatus 依赖于前一个节点的状态。 volatile Node next;//此节点的后一个节点。后一个节点是否被唤醒（uppark()）依赖于当前节点是否被释放。 volatile //Thread thread;节点绑定的线程。 Node nextWaiter;下一个等待条件（Condition）的节点 ##public void …ReentrantLock.lock() ##ReentrantLock是可重入锁。 如果该锁没有被另一个线程保持，则获取该锁并立即返回，将锁的保持计数设置为1。 如果当前线程已经保持该锁，则将保持计数加1，并且该方法立即返回。 如果该锁被另一个线程保持，则出于线程调度的目的，禁用当前线程，并且在获得锁之前，该线程将一直处于休眠状态，此时锁保持计数被设置为1。 公平锁和非公平锁 如果获取一个锁是按照请求的顺序得到的，那么就是公平锁，否则就是非公平锁。公平锁保证一个阻塞的线程最终能够获得锁，因为是有序的，所以总是可以按照请求的顺序获得锁。不&gt;公平锁意味着后请求锁的线程可能在其前面排列的休眠线程恢复前拿到锁，这样就有可能提高并发的性能。因为通常情况下挂起的线程重新开始与它真正开始运行，二者之间会产生严重的延时。因此非公平锁就可以利用这段时间完成操作。这是非公平锁在某些时候比公平锁性能要好的原因之一。 非公平锁 final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } 公平锁 final void lock() { acquire(1); } 非公平锁在第一次获取锁，或者其它线程释放锁后（可能等待），优先采用compareAndSetState(0,1)然后设置AQS 独占线程而持有锁，这样有时候比acquire(1)顺序检查锁持有而要高效。即 公平锁实现（AQS)是 Lock 的基础，对于一个FairSync 而言，lock()就直接调用AQS 的acquire(int arg); public final void acquire(int arg) 以独占模式获取对象，忽略中断。通过至少调用一次tryAcquire(int) 来实现此方法，并在成功时返回。否则在成功之前，一直调用tryAcquire(int)将线程加入队列，线程可能重复被阻塞或不被阻塞。 在AQS 中acquire 的实现如下： public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp;acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 分为四步： 如果tryAcquire(arg)成功，那就没有问题，已经拿到锁，整个lock()过程就结束了。如果失败进行操作2。 创建一个独占节点（Node）并且此节点加入CHL 队列末尾。进行操作3。 自旋尝试获取锁，失败根据前一个节点来决定是否挂起（park()），直到成功获取到锁。进行操作4。 如果当前线程已经中断过，那么就中断当前线程（清除中断位）。 tryAcquire(arg) protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState();//AQS中的state if (c == 0) { if (isFirst(current) &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; }state 来描述当前有多少线程持有锁。isFirst(current)是一个很复杂的逻辑，大体上的意思是说判断AQS 是否为空或者当前线程是否在队列头（为了区分公平与非公平锁）。判断如果是其他线程持有则执行2，如果是当前线程持有则state+acuires addWaiter(mode)tryAcquire 失败就意味着入队列了。 static final Node EXCLUSIVE = null; //独占节点模式 static final Node SHARED = new Node(); //共享节点模式 addWaiter(mode)中的mode 就是节点模式，也就是共享锁还是独占锁模式 private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node; } private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize Node h = new Node(); // Dummy header h.next = node; node.prev = h; if (compareAndSetHead(h)) { tail = node; return h; } } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } acquireQueued(node,arg)自旋请求锁，如果可能的话挂起线程，直到得到锁，返回当前线程是否中断过（如果park()过并且中断过的话有一个interrupted 中断位）。 final boolean acquireQueued(final Node node, int arg) { try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } catch (RuntimeException ex) { cancelAcquire(node); throw ex; } } private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int s = pred.waitStatus; if (s &lt; 0) return true; if (s &gt; 0) { do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else compareAndSetWaitStatus(pred, 0, Node.SIGNAL); return false; } 如果前一个节点的等待状态waitStatus&lt;0，也就是前面的节点还没有获得到锁，那么返回true，表示当前节点（线程）就应该park()了 selfInterrupt()private static void selfInterrupt() { Thread.currentThread().interrupt(); } 如果线程曾经中断过（或者阻塞过，那么就再中断一次 回顾在AQS 中acquire 的实现如下： public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp;acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } public void …ReentrantLock.lock()unlock操作实际上就调用了 AQS的 release 操作，释放持有的锁。 Releasepublic final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 此操作里面总是尝试去释放锁，如果成功，那么就看AQS队列中的头结点是否为空并且能否被唤醒，如果可以的话就唤醒继任节点 tryReleaseprotected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } 判断持有锁的线程是否是当前线程，如果不是就抛出异常；将AQS 状态位减少要释放的次数，若为0，清空AQS 持有锁的独占线程；将剩余的状态位写回AQS unparkSuccessor(Node node)当tryRelease 操作成功后（也就是完全释放了锁），release 操作才能检查是否需要唤醒下一个继任节点。 private void unparkSuccessor(Node node) { //此时node 是需要是需要释放锁的头结点 //清空头结点的waitStatus，也就是不再需要锁了 compareAndSetWaitStatus(node, Node.SIGNAL, 0); //从头结点的下一个节点开始寻找继任节点，当且仅当继任节点的waitStatus&lt;=0才是有效继任节点，否则将这些waitStatus&gt;0（也就是CANCELLED 的节点）从AQS 队列中 剔除 Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } //如果找到一个有效的继任节点，就唤醒此节点线程 if (s != null) LockSupport.unpark(s.thread); } acquireQueued 对比unparkSuccessor，一旦头节点的继任节点被唤醒，那么继任节点就会尝试去获取锁（在acquireQueued 中 node 就是有效的继任节点，p 就是唤醒它的头结点），如果成功就会将头结点设置为自身，并且将头结点的前任节点清空，这样前任节点（已经过时了）就可以被GC 释放了。]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm虚拟机内存原理]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjvm%2FJVM_%E5%86%85%E5%AD%98%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Jvm运行时，内存划分如图所示 程序计数器Jvm将这个计数看作当前线程执行某条字节码的行数，会根据计数器的值来选取需要执行的操作语句。这个属于线程私有，不可共享，如果共享会导致计数混乱，无法准确的执行当前线程需要执行的语句 虚拟机栈经常说到的栈内存就是指虚拟机栈。Java中每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈虚拟机栈用来执行java方法，而本地方法栈用来执行Native方法。抛出异常的情况和虚拟机栈一样。 堆是jvm中内存最大、线程共享的一块区域。唯一的目的是存储对象实例。这里也是垃圾收集器主要收集的区域。由于现代垃圾收集器都采用的是分代收集算法，所以java堆也分为新生代和老年代。 可以通过参数-Xmx(jvm最大可用内存)和-Xms(jvm初始内存)来调整堆内存，如果扩大至无法继续扩展时，会出现OutOfMemoryError的错误。 方法区Jvm中内存共享的一片区域，用来存储类信息、常量、静态变量、class文件。垃圾收集器也会对这部分区域进行回收，比如常量池的清理和类型的卸载，但是效果不理想。方法区内存不够用的时候，也会抛出OutOfMemoryError错误。 对象创建虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程，加载检查之后会在堆中划分出一定的内存。 在完成new指令之后，紧接着会调用方法将对象初始化，这时一个完整的对象才算创建了出来。 对象的访问 异常OutOfMemoryError错误Java堆内存的OutOfMemoryError异常是实际应用中常见的内存溢出异常情况。当出现Java堆内存溢出时，异常堆栈信息“java.lang.OutOfMemoryError”会跟着进一步提示“Java heap space”。 当产生的对象过多时，会出现这个错误信息。解决办法：调整虚拟机堆参数(-Xmx和-Xms)。 Java方法区会存储类信息、常量、静态变量等。如果产生了大量类，比如某个ssh项目因为加载了框架和大量jar包，这样class文件都会载入内存的方法区，这样如果出现内存无法继续扩展的情况，也会出现java.lang.OutOfMemoryError，然后紧跟着PermGen space信息。通过-XX:PermSize和-XX:MaxPermSize可以限制方法区大小。 StackOverflow错误Java中栈内存溢出，通常是由于栈深度超过限制深度，导致出现该问题。很常见的情况是，使用递归的时候，不小心忘了指定递归结束的时刻，导致递归深度超过限制深度，出现栈内存溢出。 内存回收机制Java内存运行时区域的各个部分，其中程序计数器、VM栈、本地方法栈三个区域随线程而生，随线程而灭；栈中的帧随着方法进入、退出而有条不紊的进行着出栈入栈操作。而Java堆和方法区（包括运行时常量池）则不一样，我们必须等到程序实际运行期间才能知道会创建哪些对象，这部分内存的分配和回收都是动态的。 判断对象已死1)引用计数算法 引用计数算法无法解决对象循环引用的问题。 根搜索算法（通过一系列的称为“GCRoots”的点作为起始进行向下搜索，当一个对象到GCRoots没有任何引用链（ReferenceChain）相连，则证明此对象是不可用的）GC Roots包括： 在VM栈（帧中的本地变量）中的引用。 方法区中的静态引用和常量引用的对象。 JNI（即一般说的Native方法）中的引用。 2)生存还是死亡？ 判定一个对象死亡，至少经历两次标记过程：如果对象在进行根搜索后，发现没有与GC Roots相连接的引用链，那它将会被第一次标记，并在稍后执行他的finalize()方法（如果它有的话）。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束。 finalize()方法是对象最后一次逃脱死亡命运的机会，稍后GC将进行第二次规模稍小的标记，如果在finalize()中对象成功拯救自己（只要重新建立到GC Roots的连接即可，譬如把自己赋值到某个引用上），那在第二次标记时它将被移除出“即将回收”的集合，如果对象这时候还没有逃脱，那基本上它就真的离死不远了。需要关闭外部资源之类的事情，基本上它能做的使用try-finally可以做的更好。 3)回收方法区 方法区即后文提到的永久代，这区GC的“性价比”一般比较低：在堆中，尤其是在新生代，进行一次GC可以一般可以回收70%~95%的空间，而永久代的GC效率远小于此。目前方法区主要回收两部分内容：废弃常量与无用类。需要满足下面3个条件： 该类所有的实例都已经被GC，也就是JVM中不存在该Class的任何实例 加载该类的ClassLoader已经被GC。 该类对应的java.lang.Class对象没有在任何地方被引用，如不能在任何地方通过反射访问该类的方法。 垃圾收集算法标记－清除算法（Mark-Sweep）首先标记出所有需要回收的对象，然后回收所有需要回收的对象。主要缺点有两个，一是效率问题，标记和清理两个过程效率都不高，二是空间问题，标记清理之后会产生大量不连续的内存碎片，空间碎片太多可能会导致后续使用中无法找到足够的连续内存而提前触发另一次的垃圾搜集动作。 复制算法（Copying）(新生代)将内存分为一块较大的eden空间和2块较少的survivor空间，每次使用eden和其中一块survivor，当回收时将eden和 survivor还存活的对象一次过拷贝到另外一块survivor空间上，然后清理掉eden和用过的survivor。复制收集算法在对象存活率高的时候，效率有所下降。 HotSpot默认eden:survivor是8:1,当survivor空间不够时,会分配老生代中 标记－整理（Mark-Compact）(老生代)算法标记过程仍然一样，但后续步骤不是进行直接清理，而是令所有存活的对象一端移动，然后直接清理掉这端边界以外的内存。分代收集(Generational Collection)算法 分代收集(Generational Collection)算法此算法只是根据对象不同的存活周期将内存划分为几块。一般是把Java堆分作新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。]]></content>
      <categories>
        <category>java基础</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm虚拟机垃圾回收]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjvm%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[范围：要回收哪些区域在JVM五种内存模型中，有三个是不需要进行垃圾回收的：程序计数器、JVM栈、本地方法栈。因为它们的生命周期是和线程同步的，随着线程的销毁，它们占用的内存会自动释放，所以只有方法区和堆需要进行GC。 前提：如何判断对象已死所有的垃圾收集算法都面临同一个问题，那就是找出应用程序不可到达的内存块，将其释放，这里面讲的不可达主要是指应用程序已经没有内存块的引用了， 在Java中，某个对象对应用程序是可到达的是指：这个对象被根（根主要是指类的静态变量，或者活跃在所有线程栈的对象的引用）引用或者对象被另一个可到达的对象引用。 引用计数算法引用计数是最简单直接的一种方式，这种方式在每一个对象中增加一个引用的计数，这个计数代表当前程序有多少个引用引用了此对象，如果此对象的引用计数变为0，那么此对象就可以作为垃圾收集器的目标对象来收集。优点：简单，直接，不需要暂停整个应用缺点： 需要编译器的配合，编译器要生成特殊的指令来进行引用计数的操作； 不能处理循环引用的问题 因此这种方法是垃圾收集的早期策略，现在很少使用。Sun的JVM并没有采用引用计数算法来进行垃圾回收，而是基于根搜索算法的。 可达性分析算法（根搜索算法）通过一系列的名为“GC Root”的对象作为起点，从这些节点向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Root没有任何引用链相连时，则该对象不可达，该对象是不可使用的，垃圾收集器将回收其所占的内存。 在java语言中，可作为GCRoot的对象包括以下几种： java虚拟机栈(栈帧中的本地变量表)中的引用的对象。 方法区中的类静态属性引用的对象。 方法区中的常量引用的对象。 本地方法栈中JNI本地方法的引用对象。 四种引用GC在收集一个对象的时候会判断是否有引用指向对象，在JAVA中的引用主要有四种： 强引用（Strong Reference）强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。 软引用（Soft Reference）如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。下面举个例子，假如有一个应用需要读取大量的本地图片，如果每次读取图片都从硬盘读取，则会严重影响性能，但是如果全部加载到内存当中，又有可能造成内存溢出，此时使用软引用可以解决这个问题。设计思路是：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了内存溢出的问题。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 弱引用（Weak Reference）弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。生命周期在下一次垃圾回收之前 虚引用（Phantom Reference）“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用主要用于检测对象是否已经从内存中删除，跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。虚引用的唯一目的是当对象被回收时收到一个系统通知。 finalize() 方法通过可达性分析，那些不可达的对象并不是立即被销毁，他们还有被拯救的机会。如果要回收一个不可达的对象，要经历两次标记过程。首先是第一次标记，并判断对象是否覆写了 finalize 方法，如果没有覆写，则直接进行第二次标记并被回收。如果对象有覆写finalize 方法，则会将改对象加入一个叫“F-Queue”的队列中，虚拟机会建立一个低优先级的 Finalizer 线程去执行它，这里说的“执行”是指该线程会去触发 finalize 方法，但是并不会等待 finalize 方法执行完成。主要是因为 finalize 方法的不确定性，它可能要花很长时间才能执行完成，甚至死循环，永远不结束，这将导致整个 GC 工作的异常，甚至崩溃。关于拯救，可以在 finalize 方法中将自己（this关键字）赋值给类变量或其他对象的成员变量，则第二次标记时它将被移出回收的集合，如果对象并未被拯救，则最终被回收。finalize 方法只会被调用一次，如果一个在 finalize 被拯救的对象再次需要回收，则它的 finalize 将不会再被触发了。不建议使用finalize 方法，它的运行代价高，不确定性大，GC 也不会等待它执行完成，它的功能完全可以被 try-finally 代替。 方法区的回收方法区也会被回收，其被回收的内存有：废弃常量、无用的类。在 HotSpot 虚拟机规范里，将永久带作为方法区的实现。废弃常量：没有被引用的常量，如 String。判断无用的类：(1).该类的所有实例都已经被回收，即java堆中不存在该类的实例对象。(2).加载该类的类加载器已经被回收。(3).该类所对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射机制访问该类的方法。 各种垃圾收集算法标记-清除算法步骤：1、标记：从根集合开始扫描，标记存活对象；2、清除：再次扫描真个内存空间，回收未被标记的对象。此算法一般没有虚拟机采用优点1：解决了循环引用的问题优点2：与复制算法相比，不需要对象移动，效率较高，而且还不需要额外的空间不足1：每个活跃的对象都要进行扫描，而且要扫描两次，效率较低，收集暂停的时间比较长。不足2：产生不连续的内存碎片 标记-整理（压缩）算法对标记-清除算法的改进标记过程与标记-清除算法一样，但是标记完成后，存活对象向一端移动，然后清理边界的内存步骤：1、标记：从根集合开始扫描，标记存活对象；2、整理：再次扫描真个内存空间，并往内存一段移动存活对象，再清理掉边界的对象。不会产生内存碎片，但是依旧移动对象的成本。适合老年代还有一种算法是标记-清除-整理（压缩），是在多次标记清除后，再进行一次整理，这样就减少了移动对象的成本。 复制算法将内存分成两块容量大小相等的区域，每次只使用其中一块，当这一块内存用完了，就将所有存活对象复制到另一块内存空间，然后清除前一块内存空间。此种方法实现简单、效率较高，优点：1、不会产生内存碎；2、没有了先标记再删除的步骤，而是通过Tracing从 From内存中找到存活对象，复制到另一块To内存区域，From只要移动堆顶指针便可再次使用。缺点：1、复制的代价较高，所有适合新生代，因为新生代的对象存活率较低，需要复制的对象较少；2、需要双倍的内存空间，而且总是有一块内存空闲，浪费空间。 分代收集算法所有商业虚拟机都采用这种方式，将堆分成新生代和老年代，新生代使用复制算法，老年代使用标记-整理算法 GC 类型1.Minor GC 针对新生代的 GC2.Major GC 针对老年代的 GC3.Full GC 针对新生代、老年代、永久带的 GC 为什么要分不同的 GC 类型，主要是1、对象有不同的生命周期，经研究，98%的对象都是临时对象；2、根据各代的特点应用不同的 GC 算法，提高 GC 效率。 各种垃圾收集器串行收集器（Serial Collector）单线程，会发生停顿适用场景：1.单 CPU、新生代小、对停顿时间要求不高的应用2.client 模式下或32位 Windows 上的默认收集器新生代均采用复制算法，老年代用标记-整理算法（Serial Old Collector）在单核 CPU 上面的运行效果较好，甚至可能超过并行垃圾收集器，因为并行垃圾收集器有线程的切换消耗。当 Eden 空间分配不足时触发原理：1.拷贝 Eden 和 From 空间的存活对象到 To 空间2.部分对象可能晋升到老年代（大对象、达到年龄的对象、To 空间不足时）3.清空 Eden、From 空间，From 与 To 空间交换角色 ParNew（Serial 收集器的多线程版本）新生代收集器，是 Serial 的多线程版，是 Server 模式下的虚拟机中首选的新生代收集器，不是默认收集器。除了 Serial 外，是唯一能与 CMS 收集器配合工作的收集器。多线程下，性能较好，单线程下，并不会比 Serial 好。 并行收集器（Parallel Scavenge）特性：1.并行、停顿2.并行线程数：CPU &lt;= 8 := 8,CPU &gt; 8 := (3+ cpu * 5) / 8,也可强制指定 GC 线程数3.自适应调节策略，如果把该策略打开，则虚拟机会自动调整新生代的大小比例和晋升老年代的对象大小、年龄等细节参数4.吞吐量优先收集器，即可用设置一个 GC 时间，收集器将尽可能的在该时间内完成 GC 吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间），即吞吐量越高，则垃圾收集时间就要求越短 用户可以设置最大垃圾收集停顿时间或者吞吐量但并不是把最大垃圾收集停顿时间设置得越短越好，因为它是以牺牲吞吐量和新生代空间的代价来换取的，比如收集300M 空间总会比收集500M 空间更快，再如收集频率加高，本来10秒收集一次，每次停顿100毫秒，但是现在改成了5秒收集一次，每次停顿70毫秒，停顿时间是小了，但是吞吐量确也降下来了。 适用场景：1.多 CPU、对停顿时间要求高的应用2.是 Server 端的默认新生代收集器 Serial Old是 Serial 收集器的老年代版本，依旧是单线程收集器，采用标记-整理算法， Parallel Old略 CMS（并发-标记-清除）CMS 是一种以获取最短回收停顿时间为目标的收集器。 步骤： 初始标记 此阶段仅仅是标记一下 GC Roots 能直接关联到的对象，速度很快，但是会停顿 注意：这里不是 GC Roots Tracing 的过程 并发标记 GC Roots Tracing 的过程，这个阶段可以与用户线程一起工作，不会造成停顿,从而导致整个停顿时间大大降低 重新标记 是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录 在Concurrent Marking阶段，记下所有改动的reference，在Remarking阶段再trace一遍从变动过的reference开始的局部object graph。 并发清除 优点：停顿时间短，但是总的 GC 时间长缺点： 并发程序都是 CPU 敏感的，并发标记和并发清除可能会抢占应用 CPU 总的 GC 时间长 无法处理浮动垃圾 浮动垃圾：在并发清除过程中，程序还在运行，可能产生新的垃圾，但是本次 GC 确不可能清除掉这些新产生的垃圾了，所以这些新产生垃圾就叫浮动垃圾，也就是说在一次 CMS 的 GC 后，用户获取不到一个完全干净的内存空间，还是或多或少存在浮动垃圾的。 由于在并发标记和并发清除阶段，用户程序依旧在运行，所以也就需要为用户程序的运行预留一定空间，而不能想其他收集器一样会暂停用户程序的运行。在此期间，就可能发生预留空间不足，导致程序异常的情况。 是基于标记-清除的收集器，所以会产生内存碎片 G1 G1算法将堆划分为若干个区域（Region），它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。 在G1中，还有一种特殊的区域，叫Humongous区域。 如果一个对象占用的空间超过了分区容量50%以上，G1收集器就认为这是一个巨型对象。默认直接会被分配在年老代，但是如果它是一个短期存在的巨型对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了能找到连续的H区，有时候不得不启动Full GC。 G1提供了两种GC模式，Young GC和Mixed GC，两种都是Stop The World(STW)的 G1 Young GCYoung GC主要是对Eden区进行GC，它在Eden空间耗尽时会被触发。在这种情况下，Eden空间的数据移动到Survivor空间中，如果Survivor空间不够，Eden空间的部分数据会直接晋升到年老代空间。Survivor区的数据移动到新的Survivor区中，也有部分数据晋升到老年代空间中。最终Eden空间的数据为空，GC停止工作，应用线程继续执行。 G1 Mix GC独立管理整个 java heap 空间，而不需要其他收集器的配合。步骤： 初始标记与CMS 一样，只是标记一下 GC Roots 能直接关联到的对象，速度很快，但是需要停顿 并发标记GC Roots Tracing 过程，并发执行 最终标记并行执行，需要停顿 筛选回收并行执行，需要停顿 优点： 存在并发与并行操作，最大化利用硬件资源，提升收集效率 分代收集，虽然 G1可以独立管理整个 Heap，但是它还是保留了分代的概念，实际上,在分区时，这些区域(regions)被映射为逻辑上的 Eden, Survivor, 和 old generation(老年代)空间，使其有目的的收集特定区域的内存。 空间整合，G1回收内存时，是将某个或多个区域的存活对象拷贝至其他空区域，同时释放被拷贝的内存区域，这种方式在整体上看是标记-整理，在局部看（两个 Region 之间）是复制算法，所以不会产生内存碎片 可预测的停顿时间 G1相对于CMS的区别 G1在压缩空间方面有优势 G1通过将内存空间分成区域（Region）的方式避免内存碎片问题 Eden, Survivor, Old区不再固定、在内存使用效率上来说更灵活 G1可以通过设置预期停顿时间（Pause Time）来控制垃圾收集时间避免应用雪崩现象 G1在回收内存后会马上同时做合并空闲内存的工作、而CMS默认是在STW（stop the world）的时候做 G1会在Young GC中使用、而CMS只能在O区使用 CMS还是默认首选的GC策略、可能在以下场景下G1更适合： 服务端多核CPU、JVM内存占用较大的应用（至少大于4G） 应用在运行过程中会产生大量内存碎片、需要经常压缩空间 想要更可控、可预期的GC停顿周期；防止高并发下应用雪崩现象 内存分配策略 对象优先在 Eden 区分配 大对象直接进入老年代 长期存活的对象将进入老年代 动态对象年龄判断。并不是新生代对象的年龄一定要达到某个值，才会进入老年代。Survivor空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半，那么年龄等于或大于该年龄的对象就直接进入老年代，无须等待设置的年龄 空间分配担保 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。]]></content>
      <categories>
        <category>java基础</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm虚拟机内存模型]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjvm%2F%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[java内存多任务处理器是因为计算机的运算速度与他的存储和通信子系统速度的差距太大。 衡量一个一个服务性能好坏，可以用tps（每秒事务数），tps与并发能力有密切的关系，线程并发协调有序，效率会高，如果线程频繁阻塞或死锁，讲大大降低并发能力 硬件的效率与一致性引入缓存来作为内存和处理器之间的缓冲，但引入了一个问题：缓存一致性 缓存一致性在多处理器中，每个处理器都有自己的高速缓存，而他们又共享一块主存，当多处理器运算涉及同一块主存时，就会出现数据不一致。为了解决这些问题，各个处理器访问缓存时都遵循一些协议：MSI，MOSI等等。 java内存模型（JMM）java内存模型主要是定义程序中各个变量的访问规则。此处的变量与Java编程时所说的变量不一样，指包括了实例字段、静态字段和构成数组对象的元素，但是不包括局部变量与方法参数，后者是线程私有的，不会被共享。 Java内存模型中规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存（可以与前面将的处理器的高速缓存类比），线程的工作内存中保存了该线程使用到的变量到主内存副本拷贝，线程对变量的所有操作（读取、赋值）都必须在工作内存中进行，而不能直接读写主内存中的变量。不同线程之间无法直接访问对方工作内存中的变量，线程间变量值的传递均需要在主内存来完成 内存间交互操作关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存- 同步到主内存之间的实现细节，Java内存模型定义了以下八种操作来完成： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行read和load操作，如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。Java内存模型还规定了在执行上述八种基本操作时，必须满足如下规则： 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，lock和unlock必须成对出现 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。 Java与线程线程是比进程更轻量级的调度单位，可以把一个进程的资源分配和执行调度分开 线程安全当多个对象访问一个对象，]]></content>
      <categories>
        <category>java基础</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shiro配置-编码-加密]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fshiro%2FINI%E9%85%8D%E7%BD%AE-%E7%BC%96%E7%A0%81%E5%8A%A0%E5%AF%86-Realm%2F</url>
    <content type="text"><![CDATA[INI配置Shiro是从根对象SecurityManager进行身份验证和授权的；也就是所有操作都是自它开始的，这个对象是线程安全且真个应用只需要一个即可 java代码初始化12345678910111213141516171819202122232425DefaultSecurityManager securityManager = new DefaultSecurityManager(); //设置 authenticator ModularRealmAuthenticator authenticator = new ModularRealmAuthenticator(); authenticator.setAuthenticationStrategy(new AtLeastOneSuccessfulStrategy()); securityManager.setAuthenticator(authenticator); //设置 authorizer ModularRealmAuthorizer authorizer = new ModularRealmAuthorizer(); authorizer.setPermissionResolver(new WildcardPermissionResolver()); securityManager.setAuthorizer(authorizer); //设置 Realm DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/shiro&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;&quot;); JdbcRealm jdbcRealm = new JdbcRealm(); jdbcRealm.setDataSource(ds); jdbcRealm.setPermissionsLookupEnabled(true); securityManager.setRealms(Arrays.asList((Realm) jdbcRealm)); //将 SecurityManager 设置到 SecurityUtils 方便全局使用 SecurityUtils.setSecurityManager(securityManager); Subject subject = SecurityUtils.getSubject(); 等价INI配置文件（shiro-config.ini） ,类似ioc容器1234567891011121314151617181920212223[main]#authenticatorauthenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticatorauthenticationStrategy=org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategyauthenticator.authenticationStrategy=$authenticationStrategysecurityManager.authenticator=$authenticator#authorizerauthorizer=org.apache.shiro.authz.ModularRealmAuthorizerpermissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolverauthorizer.permissionResolver=$permissionResolversecurityManager.authorizer=$authorizer#realmdataSource=com.alibaba.druid.pool.DruidDataSourcedataSource.driverClassName=com.mysql.jdbc.DriverdataSource.url=jdbc:mysql://localhost:3306/shirodataSource.username=rootdataSource.password=rootjdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealmjdbcRealm.dataSource=$dataSourcejdbcRealm.permissionsLookupEnabled=truesecurityManager.realms=$jdbcRealm java初始化123Factory&lt;org.apache.shiro.mgt.SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro-config.ini&quot;); // ini 配置文件路径，其支持“classpath:”（类路径）、“file:”（文件系统）、“url:”（网络）三种路径格式，org.apache.shiro.mgt.SecurityManager securityManager = factory.getInstance(); //将 SecurityManager设置到SecurityUtils方便全局使用 SecurityUtils.setSecurityManager(securityManager); Subject subject = SecurityUtils.getSubject(); INI 配置分类123456789101112131415161718[main] #提供了对根对象securityManager及其依赖的配置securityManager=org.apache.shiro.mgt.DefaultSecurityManager ………… securityManager.realms=$jdbcRealm [users] #提供了对用户/密码及其角色的配置，用户名=密码，角色1，角色 2 username=password,role1,role2 [roles] #提供了角色及权限之间关系的配置，角色=权限 1，权限 2 role1=permission1,permission2 [urls] #用于web，提供了对web url拦截相关的配置，url=拦截器[参数]，拦截器/index.html = anon /admin/** = authc, roles[admin], perms[&quot;permission1&quot;] 编码/加密在涉及到密码存储问题上，应该加密/生成密码摘要存储，而不是存储明文密码 编码/解码Shiro提供了base64和16进制字符串编码/解码的API支持，方便一些编码解码操作。Shiro内部的一些数据的存储/表示都使用了base64和16进制字符串。12345678910//Base64String str = &quot;hello&quot;; String base64Encoded = Base64.encodeToString(str.getBytes()); String str2 = Base64.decodeToString(base64Encoded); Assert.assertEquals(str, str2); //16进制 String str = &quot;hello&quot;; String hexEncoded = Hex.encodeToString(str.getBytes()); String str2 = new String(Hex.decode(hexEncoded.getBytes())); Assert.assertEquals(str, str2); 还有一个可能经常用到的类CodecSupport，提供了toBytes(str,”utf-8”)/toString(bytes,”utf-8”)用于在byte数组/String 之间转换。 散列算法用于生成数据的摘要信息，是一种不可逆的算法,如MD5,SHA等。一般进行散列时最好提供一个salt（盐），这样生成的散列值相对来说更难破解 1234567String str = &quot;hello&quot;;String salt = &quot;123&quot;;String md5 = new Md5Hash(str, salt).toString();// 还可以转换为toBase64()/toHex(),new Md5Hash(str, salt, 2).toString()String sha1 = new Sha256Hash(str,salt).toString();//另外还有如 SHA1、SHA512 算法//Shiro还提供了通用的散列支持：内部使用MessageDigest String simpleHash = new SimpleHash(&quot;SHA-1&quot;, str, salt).toString(); 为了方便使用，Shiro 提供了 HashService，默认提供了 DefaultHashService 实现1234567891011DefaultHashService hashService = new DefaultHashService(); // 默认算法 SHA-512hashService.setHashAlgorithmName(&quot;SHA-512&quot;);hashService.setPrivateSalt(new SimpleByteSource(&quot;123&quot;)); // 私盐,默认无,其在散列时自动与用户传入的公盐混合产生一个新盐； String hashService.setGeneratePublicSalt(true);// 是否生成公盐，默认 falsehashService.setRandomNumberGenerator(new SecureRandomNumberGenerator());//用于生成公盐。默认就这个hashService.setHashIterations(1);//生成Hash值的迭代次数HashRequest request = new HashRequest.Builder().setAlgorithmName(&quot;MD5&quot;) .setSource(ByteSource.Util.bytes(&quot;hello&quot;)).setSalt(ByteSource.Util.bytes(&quot;123&quot;)).setIterations(2) .build();String hex = hashService.computeHash(request).toHex(); 加密/解密Shiro还提供对称式加密/解密算法的支持，如AES、Blowfish 等； AES例子12345678AesCipherService aesCipherService = new AesCipherService(); aesCipherService.setKeySize(128); //设置 key 长度 //生成 key Key key = aesCipherService.generateNewKey(); String text = &quot;hello&quot;; //加密 String encrptText = aesCipherService.encrypt(text.getBytes(), key.getEncoded()).toHex(); //解密 String text2 = new String(aesCipherService.decrypt(Hex.decode(encrptText), key.getEncoded()).getBytes()); Assert.assertEquals(text, text2); PasswordService/CredentialsMatcherShiro 提供了 PasswordService 及 CredentialsMatcher 用于提供加密密码及验证密码服务。 Shiro 默认提供了 PasswordService 实现 DefaultPasswordService；CredentialsMatcher 实现 PasswordMatcher 及 HashedCredentialsMatcher（更强大）。 DefaultPasswordService 配合 PasswordMatcher 实现简单的密码加密与验证服务 定义Realm1234567891011public class MyRealm extends AuthorizingRealm &#123; private PasswordService passwordService; public void setPasswordService(PasswordService passwordService) &#123; this.passwordService = passwordService; &#125; //省略 doGetAuthorizationInfo，具体看代码 @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; return new SimpleAuthenticationInfo(&quot;wu&quot;,passwordService.encryptPassword(&quot;123&quot;),getName()); &#125;&#125; 为了方便，直接注入一个 passwordService 来加密密码，实际使用时需要在 Service 层使用 passwordService 加密密码并存到数据库。 配置ini12345678910passwordService=org.apache.shiro.authc.credential.DefaultPasswordService hashService=org.apache.shiro.crypto.hash.DefaultHashService passwordService.hashService=$hashServicepasswordMatcher=org.apache.shiro.authc.credential.PasswordMatcher passwordMatcher.passwordService=$passwordServicemyRealm=com.github.zhangkaitao.shiro.chapter5.hash.realm.MyRealmmyRealm.passwordService=$passwordServicemyRealm.credentialsMatcher=$passwordMatcher 为什么要设置credentialsMatcher? myRealm间接继承了AuthenticatingRealm，将credentialsMatcher赋值给myRealm，其在调用getAuthenticationInfo方法获取到AuthenticationInfo信息后，会使用credentialsMatcher来验证凭据是否匹配，如果不匹配将抛出IncorrectCredentialsException异常。 HashedCredentialsMatcher 实现密码验证服务Shiro提供了CredentialsMatcher的散列实现HashedCredentialsMatcher，和之前的PasswordMatcher不同的是，它只用于密码验证，且可以提供自己的盐，而不是随机生成盐1SimpleAuthenticationInfo ai = new SimpleAuthenticationInfo(username, password, getName()); ai.setCredentialsSalt(ByteSource.Util.bytes(username+salt2)); //盐是用户名+随机数 通过SimpleAuthenticationInfo的credentialsSalt设置盐，HashedCredentialsMatcher 会自动识别这个盐。 密码重试次数限制123456789101112131415161718public boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info) &#123; String username = (String) token.getPrincipal(); // retry count + 1 Element element = passwordRetryCache.get(username); if (element == null) &#123; element = new Element(username, new AtomicInteger(0)); passwordRetryCache.put(element); &#125; AtomicInteger retryCount = (AtomicInteger) element.getObjectValue(); if (retryCount.incrementAndGet() &gt; 5) &#123; // if retry count &gt; 5 throw throw new ExcessiveAttemptsException(); &#125; boolean matches = super.doCredentialsMatch(token, info); if (matches) &#123; // clear retry count passwordRetryCache.remove(username); &#125; return matches;&#125; Realm定义实体及关系 即用户-角色之间是多对多关系，角色-权限之间是多对多关系；且用户和权限之间通过角 色建立关系 1234567891011121314151617181920212223242526272829public class UserRealm extends AuthorizingRealm &#123; private UserService userService = new UserServiceImpl(); protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; String username = (String) principals.getPrimaryPrincipal(); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); authorizationInfo.setRoles(userService.findRoles(username)); authorizationInfo.setStringPermissions(userService.findPermissions(username)); return authorizationInfo; &#125; protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; String username = (String) token.getPrincipal(); User user = userService.findByUsername(username); if (user == null) &#123; throw new UnknownAccountException();//没找到帐号 &#125; if (Boolean.TRUE.equals(user.getLocked())) &#123; throw new LockedAccountException(); //帐号锁定 &#125; //交给 AuthenticatingRealm 使用 CredentialsMatcher 进行密码匹配，如果觉得人家 的不好可以在此判断或自定义实现 SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(user.getUsername(), //用户名 user.getPassword(), //密码 ByteSource.Util.bytes(user.getCredentialsSalt()),//salt=username+salt getName() //realm name ); return authenticationInfo; &#125;&#125; UserRealm父类AuthorizingRealm将获取Subject相关信息分成两步：获取身份验证信息（doGetAuthenticationInfo）及授权信息（doGetAuthorizationInfo）； doGetAuthenticationInfo 获取身份验证相关信息:首先根据传入的用户名获取 User 信 息；然后如果 user 为空，那么抛出没找到帐号异常 UnknownAccountException；如果 user 找到但锁定了抛出锁定异常 LockedAccountException；最后生成 AuthenticationInfo 信息， 交给间接父类 AuthenticatingRealm 使用 CredentialsMatcher 进行判断密码是否匹配，如果不 匹配将抛出密码错误异常 IncorrectCredentialsException；另外如果密码重试此处太多将抛出 超出重试次数异常 ExcessiveAttemptsException；在组装 SimpleAuthenticationInfo 信息时， 需要传入：身份信息（ 用户名）、凭据（ 密 文 密码）、盐（username+salt），CredentialsMatcher 使用盐加密传入的明文密码和此处的密文密码进行匹配。 - doGetAuthorizationInfo 获取授权信息：PrincipalCollection 是一个身份集合，因为我们 现在就一个 Realm，所以直接调用 getPrimaryPrincipal 得到之前传入的用户名即可；然后根 据用户名调用 UserService 接口获取角色及权限信息。]]></content>
      <categories>
        <category>java框架</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAuth]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fshiro%2FOAuth%2F</url>
    <content type="text"><![CDATA[OAuth2 集成目前很多开放平台如新浪微博开放平台都在使用提供开放 API 接口供开发者使用，随之带 来了第三方应用要到开放平台进行授权的问题，OAuth 就是干这个的，OAuth2 是 OAuth 协议的下一个版本，相比 OAuth1，OAuth2 整个授权流程更简单安全了，但不兼容 OAuth1， 具 体 可以到 OAuth2 官 网 http://oauth.net/2/ 查看，OAuth2 协 议 规 范 可以参考 http://tools.ietf.org/html/rfc6749。目前有好多参考实现供选择，可以到其官网查看下载。 OAuth 角色 资源拥有者（resource owner）：能授权访问受保护资源的一个实体，可以是一个人，那我 们称之为最终用户；如新浪微博用户 zhangsan； 资源服务器（resource server）：存储受保护资源，客户端通过 access token 请求资源，资源服务器响应受保护资源给客户端；存储着用户 zhangsan 的微博等信息。 授权服务器（authorization server）：成功验证资源拥有者并获取授权之后，授权服务器 颁发授权令牌（Access Token）给客户端。 客户端（client）：如新浪微博客户端weico、微格等第三方应用，也可以是它自己的官方应用；其本身不存储资源，而是资源拥有者授权通过后，使用它的授权（授权令牌）访问受保护资源，然后客户端把相应的数据展示出来/提交到服务器。“客户端”术语不代表任 何特定实现（如应用运行在一台服务器、桌面、手机或其他设备）。 客户端从资源拥有者那请求授权。授权请求可以直接发给资源拥有者，或间接的通过授权服务器这种中介，后者更可取。 客户端收到一个授权许可，代表资源服务器提供的授权。 客户端使用它自己的私有证书及授权许可到授权服务器验证。 如果验证成功，则下发一个访问令牌。 客户端使用访问令牌向资源服务器请求受保护资源。 资源服务器会验证访问令牌的有效性，如果成功则下发受保护资源 授权码类型的开放授权 Client初始化协议的执行流程。首先通过HTTP 302来重定向RO用户代理到AS。Client在redirect_uri中应包含如下参数：client_id, scope (描述被访问的资源), redirect_uri (即Client的URI), state (用于抵制CSRF攻击). 此外，请求中还可以包含access_type和approval_prompt参数。当approval_prompt=force时，AS将提供交互页面，要求RO必须显式地批准（或拒绝）Client的此次请求。如果没有approval_prompt参数，则默认为RO批准此次请求。当access_type=offline时，AS将在颁发access_token时，同时还会颁发一个refresh_token。因为access_token的有效期较短（如3600秒），为了优化协议执行流程，offline方式将允许Client直接持refresh_token来换取一个新的access_token。 AS认证RO身份，并提供页面供RO决定是否批准或拒绝Client的此次请求（当approval_prompt=force时）。 若请求被批准，AS使用步骤(1)中Client提供的redirect_uri重定向RO用户代理到Client。redirect_uri须包含authorization_code，以及步骤1中Client提供的state。若请求被拒绝，AS将通过redirect_uri返回相应的错误信息。 Client拿authorization_code去访问AS以交换所需的access_token。Client请求信息中应包含用于认证Client身份所需的认证数据，以及上一步请求authorization_code时所用的redirect_uri。 AS在收到authorization_code时需要验证Client的身份，并验证收到的redirect_uri与第3步请求authorization_code时所使用的redirect_uri相匹配。如果验证通过，AS将返回access_token，以及refresh_token（若access_type=offline）。 服务器端POM 依赖此处我们使用 apache oltu oauth2 服务端实现，需要引入 authzserver（授权服务器依赖）和 resourceserver（资源服务器依赖） &lt;dependency&gt; &lt;groupId&gt;org.apache.oltu.oauth2&lt;/groupId&gt; &lt;artifactId&gt;org.apache.oltu.oauth2.authzserver&lt;/artifactId&gt; &lt;version&gt;0.31&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.oltu.oauth2&lt;/groupId&gt; &lt;artifactId&gt;org.apache.oltu.oauth2.resourceserver&lt;/artifactId&gt; &lt;version&gt;0.31&lt;/version&gt; &lt;/dependency&gt; 实体类表和SqlDAOServicepublic interface UserService { public User createUser(User user);// 创建用户 public User updateUser(User user);// 更新用户 public void deleteUser(Long userId);// 删除用户 public void changePassword(Long userId, String newPassword); //修改密码 User findOne(Long userId);// 根据id查找用户 List&lt;User&gt; findAll();// 得到所有用户 public User findByUsername(String username);// 根据用户名查找用户 } public interface ClientService { public Client createClient(Client client);// 创建客户端 public Client updateClient(Client client);// 更新客户端 public void deleteClient(Long clientId);// 删除客户端 Client findOne(Long clientId);// 根据id查找客户端 List&lt;Client&gt; findAll();// 查找所有 Client findByClientId(String clientId);// 根据客户端id查找客户端 Client findByClientSecret(String clientSecret);//根据客户端安全KEY查找客户端 } 此处通过OAuthService实现进行auth code和access token的维护。 public interface OAuthService { public void addAuthCode(String authCode, String username);// 添加 auth code public void addAccessToken(String accessToken, String username); // 添加 access token boolean checkAuthCode(String authCode); // 验证auth code是否有效 boolean checkAccessToken(String accessToken); // 验证access token是否有效 String getUsernameByAuthCode(String authCode);// 根据auth code获取用户名 String getUsernameByAccessToken(String accessToken);// 根据access token获取用户名 long getExpireIn();//auth code / access token 过期时间 public boolean checkClientId(String clientId);// 检查客户端id是否存在 public boolean checkClientSecret(String clientSecret);// 坚持客户端安全KEY是否存在 } 控制器AuthorizeController@Controller public class AuthorizeController { @Autowired private OAuthService oAuthService; @Autowired private ClientService clientService; @RequestMapping(&quot;/authorize&quot;) public Object authorize(Model model, HttpServletRequest request) throws URISyntaxException, OAuthSystemException { try { //构建OAuth 授权请求 OAuthAuthzRequest oauthRequest = new OAuthAuthzRequest(request); //检查传入的客户端id是否正确 if (!oAuthService.checkClientId(oauthRequest.getClientId())) { OAuthResponse response = OAuthASResponse .errorResponse(HttpServletResponse.SC_BAD_REQUEST) .setError(OAuthError.TokenResponse.INVALID_CLIENT) .setErrorDescription(Constants.INVALID_CLIENT_DESCRIPTION) .buildJSONMessage(); return new ResponseEntity( response.getBody(), HttpStatus.valueOf(response.getResponseStatus())); } Subject subject = SecurityUtils.getSubject(); //如果用户没有登录，跳转到登陆页面 if(!subject.isAuthenticated()) { if(!login(subject, request)) {//登录失败时跳转到登陆页面 model.addAttribute(&quot;client&quot;, clientService.findByClientId(oauthRequest.getClientId())); return &quot;oauth2login&quot;; } } String username = (String)subject.getPrincipal(); //生成授权码 String authorizationCode = null; //responseType目前仅支持CODE，另外还有TOKEN String responseType = oauthRequest.getParam(OAuth.OAUTH_RESPONSE_TYPE); if (responseType.equals(ResponseType.CODE.toString())) { OAuthIssuerImpl oauthIssuerImpl = new OAuthIssuerImpl(new MD5Generator()); authorizationCode = oauthIssuerImpl.authorizationCode(); oAuthService.addAuthCode(authorizationCode, username); } //进行OAuth响应构建 OAuthASResponse.OAuthAuthorizationResponseBuilder builder = OAuthASResponse.authorizationResponse(request, HttpServletResponse.SC_FOUND); //设置授权码 builder.setCode(authorizationCode); //得到到客户端重定向地址 String redirectURI = oauthRequest.getParam(OAuth.OAUTH_REDIRECT_URI); //构建响应 final OAuthResponse response = builder.location(redirectURI).buildQueryMessage(); //根据OAuthResponse返回ResponseEntity响应 HttpHeaders headers = new HttpHeaders(); headers.setLocation(new URI(response.getLocationUri())); return new ResponseEntity(headers, HttpStatus.valueOf(response.getResponseStatus())); } catch (OAuthProblemException e) { //出错处理 String redirectUri = e.getRedirectUri(); if (OAuthUtils.isEmpty(redirectUri)) { //告诉客户端没有传入redirectUri直接报错 return new ResponseEntity( &quot;OAuth callback url needs to be provided by client!!!&quot;, HttpStatus.NOT_FOUND); } //返回错误消息（如?error=） final OAuthResponse response = OAuthASResponse.errorResponse(HttpServletResponse.SC_FOUND) .error(e).location(redirectUri).buildQueryMessage(); HttpHeaders headers = new HttpHeaders(); headers.setLocation(new URI(response.getLocationUri())); return new ResponseEntity(headers, HttpStatus.valueOf(response.getResponseStatus())); } } private boolean login(Subject subject, HttpServletRequest request) { if(&quot;get&quot;.equalsIgnoreCase(request.getMethod())) { return false; } String username = request.getParameter(&quot;username&quot;); String password = request.getParameter(&quot;password&quot;); if(StringUtils.isEmpty(username) || StringUtils.isEmpty(password)) { return false; } UsernamePasswordToken token = new UsernamePasswordToken(username, password); try { subject.login(token); return true; } catch (Exception e) { request.setAttribute(&quot;error&quot;, &quot;登录失败:&quot; + e.getClass().getName()); return false; } } }; 首先通过如http://localhost:8080/chapter17-server/authorize?client_id=c1ebe466-1cdc-4bd3-ab69-77c3561b9dee&amp;response_type=code&amp;redirect_uri=http://localhost:9080/chapter17-client/oauth2-login访问授权页面； 该控制器首先检查clientId是否正确；如果错误将返回相应的错误信息； 然后判断用户是否登录了，如果没有登录首先到登录页面登录； 登录成功后生成相应的auth code即授权码，然后重定向到客户端地址，如http://localhost:9080/chapter17-client/oauth2-login?code=52b1832f5dff68122f4f00ae995da0ed；在重定向到的地址中会带上code参数（授权码），接着客户端可以根据授权码去换取access token。 AccessTokenController@RestController public class AccessTokenController { @Autowired private OAuthService oAuthService; @Autowired private UserService userService; @RequestMapping(&quot;/accessToken&quot;) public HttpEntity token(HttpServletRequest request) throws URISyntaxException, OAuthSystemException { try { //构建OAuth请求 OAuthTokenRequest oauthRequest = new OAuthTokenRequest(request); //检查提交的客户端id是否正确 if (!oAuthService.checkClientId(oauthRequest.getClientId())) { OAuthResponse response = OAuthASResponse .errorResponse(HttpServletResponse.SC_BAD_REQUEST) .setError(OAuthError.TokenResponse.INVALID_CLIENT) .setErrorDescription(Constants.INVALID_CLIENT_DESCRIPTION) .buildJSONMessage(); return new ResponseEntity( response.getBody(), HttpStatus.valueOf(response.getResponseStatus())); } // 检查客户端安全KEY是否正确 if (!oAuthService.checkClientSecret(oauthRequest.getClientSecret())) { OAuthResponse response = OAuthASResponse .errorResponse(HttpServletResponse.SC_UNAUTHORIZED) .setError(OAuthError.TokenResponse.UNAUTHORIZED_CLIENT) .setErrorDescription(Constants.INVALID_CLIENT_DESCRIPTION) .buildJSONMessage(); return new ResponseEntity( response.getBody(), HttpStatus.valueOf(response.getResponseStatus())); } String authCode = oauthRequest.getParam(OAuth.OAUTH_CODE); // 检查验证类型，此处只检查AUTHORIZATION_CODE类型，其他的还有PASSWORD或REFRESH_TOKEN if (oauthRequest.getParam(OAuth.OAUTH_GRANT_TYPE).equals( GrantType.AUTHORIZATION_CODE.toString())) { if (!oAuthService.checkAuthCode(authCode)) { OAuthResponse response = OAuthASResponse .errorResponse(HttpServletResponse.SC_BAD_REQUEST) .setError(OAuthError.TokenResponse.INVALID_GRANT) .setErrorDescription(&quot;错误的授权码&quot;) .buildJSONMessage(); return new ResponseEntity( response.getBody(), HttpStatus.valueOf(response.getResponseStatus())); } } //生成Access Token OAuthIssuer oauthIssuerImpl = new OAuthIssuerImpl(new MD5Generator()); final String accessToken = oauthIssuerImpl.accessToken(); oAuthService.addAccessToken(accessToken, oAuthService.getUsernameByAuthCode(authCode)); //生成OAuth响应 OAuthResponse response = OAuthASResponse .tokenResponse(HttpServletResponse.SC_OK) .setAccessToken(accessToken) .setExpiresIn(String.valueOf(oAuthService.getExpireIn())) .buildJSONMessage(); //根据OAuthResponse生成ResponseEntity return new ResponseEntity( response.getBody(), HttpStatus.valueOf(response.getResponseStatus())); } catch (OAuthProblemException e) { //构建错误响应 OAuthResponse res = OAuthASResponse .errorResponse(HttpServletResponse.SC_BAD_REQUEST).error(e) .buildJSONMessage(); return new ResponseEntity(res.getBody(), HttpStatus.valueOf(res.getResponseStatus())); } } } 首先通过如http://localhost:8080/chapter17-server/accessToken，POST提交如下数据：client_id= c1ebe466-1cdc-4bd3-ab69-77c3561b9dee&amp; client_secret= d8346ea2-6017-43ed-ad68-19c0f971738b&amp;grant_type=authorization_code&amp;code=828beda907066d058584f37bcfd597b6&amp;redirect_uri=http://localhost:9080/chapter17-client/oauth2-login访问； 该控制器会验证client_id、client_secret、auth code的正确性，如果错误会返回相应的错误； 如果验证通过会生成并返回相应的访问令牌access token。 资源控制器@RestController public class UserInfoController { @Autowired private OAuthService oAuthService; @RequestMapping(&quot;/userInfo&quot;) public HttpEntity userInfo(HttpServletRequest request) throws OAuthSystemException { try { //构建OAuth资源请求 OAuthAccessResourceRequest oauthRequest = new OAuthAccessResourceRequest(request, ParameterStyle.QUERY); //获取Access Token String accessToken = oauthRequest.getAccessToken(); //验证Access Token if (!oAuthService.checkAccessToken(accessToken)) { // 如果不存在/过期了，返回未验证错误，需重新验证 OAuthResponse oauthResponse = OAuthRSResponse .errorResponse(HttpServletResponse.SC_UNAUTHORIZED) .setRealm(Constants.RESOURCE_SERVER_NAME) .setError(OAuthError.ResourceResponse.INVALID_TOKEN) .buildHeaderMessage(); HttpHeaders headers = new HttpHeaders(); headers.add(OAuth.HeaderType.WWW_AUTHENTICATE, oauthResponse.getHeader(OAuth.HeaderType.WWW_AUTHENTICATE)); return new ResponseEntity(headers, HttpStatus.UNAUTHORIZED); } //返回用户名 String username = oAuthService.getUsernameByAccessToken(accessToken); return new ResponseEntity(username, HttpStatus.OK); } catch (OAuthProblemException e) { //检查是否设置了错误码 String errorCode = e.getError(); if (OAuthUtils.isEmpty(errorCode)) { OAuthResponse oauthResponse = OAuthRSResponse .errorResponse(HttpServletResponse.SC_UNAUTHORIZED) .setRealm(Constants.RESOURCE_SERVER_NAME) .buildHeaderMessage(); HttpHeaders headers = new HttpHeaders(); headers.add(OAuth.HeaderType.WWW_AUTHENTICATE, oauthResponse.getHeader(OAuth.HeaderType.WWW_AUTHENTICATE)); return new ResponseEntity(headers, HttpStatus.UNAUTHORIZED); } OAuthResponse oauthResponse = OAuthRSResponse .errorResponse(HttpServletResponse.SC_UNAUTHORIZED) .setRealm(Constants.RESOURCE_SERVER_NAME) .setError(e.getError()) .setErrorDescription(e.getDescription()) .setErrorUri(e.getUri()) .buildHeaderMessage(); HttpHeaders headers = new HttpHeaders(); headers.add(OAuth.HeaderType.WWW_AUTHENTICATE, 、 oauthResponse.getHeader(OAuth.HeaderType.WWW_AUTHENTICATE)); return new ResponseEntity(HttpStatus.BAD_REQUEST); } } }; 首先通过如http://localhost:8080/chapter17-server/userInfo? access_token=828beda907066d058584f37bcfd597b6进行访问； 该控制器会验证access token的有效性；如果无效了将返回相应的错误，客户端再重新进行授权； 如果有效，则返回当前登录用户的用户名。 Spring配置文件只列举spring-config-shiro.xml中的shiroFilter的filterChainDefinitions属性： &lt;property name=&quot;filterChainDefinitions&quot;&gt; &lt;value&gt; / = anon /login = authc /logout = logout /authorize=anon /accessToken=anon /userInfo=anon /** = user &lt;/value&gt; &lt;/property&gt; 对于oauth2的几个地址/authorize、/accessToken、/userInfo都是匿名可访问的 服务器维护客户端管理就是进行客户端的注册，如新浪微博的第三方应用就需要到新浪微博开发平台进行注册；用户管理就是进行如新浪微博用户的管理。 对于授权服务和资源服务的实现可以参考新浪微博开发平台的实现：http://open.weibo.com/wiki/授权机制说明http://open.weibo.com/wiki/微博API 客户端客户端流程：如果需要登录首先跳到oauth2服务端进行登录授权，成功后服务端返回auth code，然后客户端使用auth code去服务器端换取access token，最好根据access token获取用户信息进行客户端的登录绑定。这个可以参照如很多网站的新浪微博登录功能，或其他的第三方帐号登录功能。 POM依賴 org.apache.oltu.oauth2 org.apache.oltu.oauth2.client 0.31 OAuth2Token类似于UsernamePasswordToken和CasToken；用于存储oauth2服务端返回的auth code。 public class OAuth2Token implements AuthenticationToken { private String authCode; private String principal; public OAuth2Token(String authCode) { this.authCode = authCode; } //省略getter/setter } OAuth2AuthenticationFilter用于oauth2客户端的身份验证控制；如果当前用户还没有身份验证，首先会判断url中是否有code（服务端返回的auth code），如果没有则重定向到服务端进行登录并授权，然后返回auth code；接着OAuth2AuthenticationFilter会用auth code创建OAuth2Token，然后提交给Subject.login进行登录；接着OAuth2Realm会根据OAuth2Token进行相应的登录逻辑。 public class OAuth2AuthenticationFilter extends AuthenticatingFilter { //oauth2 authc code参数名 private String authcCodeParam = &quot;code&quot;; //客户端id private String clientId; //服务器端登录成功/失败后重定向到的客户端地址 private String redirectUrl; //oauth2服务器响应类型 private String responseType = &quot;code&quot;; private String failureUrl; //省略setter protected AuthenticationToken createToken(ServletRequest request, ServletResponse response) throws Exception { HttpServletRequest httpRequest = (HttpServletRequest) request; String code = httpRequest.getParameter(authcCodeParam); return new OAuth2Token(code); } protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) { return false; } protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception { String error = request.getParameter(&quot;error&quot;); String errorDescription = request.getParameter(&quot;error_description&quot;); if(!StringUtils.isEmpty(error)) {//如果服务端返回了错误 WebUtils.issueRedirect(request, response, failureUrl + &quot;?error=&quot; + error + &quot;error_description=&quot; + errorDescription); return false; } Subject subject = getSubject(request, response); if(!subject.isAuthenticated()) { if(StringUtils.isEmpty(request.getParameter(authcCodeParam))) { //如果用户没有身份验证，且没有auth code，则重定向到服务端授权 saveRequestAndRedirectToLogin(request, response); return false; } } //执行父类里的登录逻辑，调用Subject.login登录 return executeLogin(request, response); } //登录成功后的回调方法 重定向到成功页面 protected boolean onLoginSuccess(AuthenticationToken token, Subject subject, ServletRequest request, ServletResponse response) throws Exception { issueSuccessRedirect(request, response); return false; } //登录失败后的回调 protected boolean onLoginFailure(AuthenticationToken token, AuthenticationException ae, ServletRequest request, ServletResponse response) { Subject subject = getSubject(request, response); if (subject.isAuthenticated() || subject.isRemembered()) { try { //如果身份验证成功了 则也重定向到成功页面 issueSuccessRedirect(request, response); } catch (Exception e) { e.printStackTrace(); } } else { try { //登录失败时重定向到失败页面 WebUtils.issueRedirect(request, response, failureUrl); } catch (IOException e) { e.printStackTrace(); } } return false; } } 该拦截器的作用： 首先判断有没有服务端返回的error参数，如果有则直接重定向到失败页面； 接着如果用户还没有身份验证，判断是否有auth code参数（即是不是服务端授权之后返回的），如果没有则重定向到服务端进行授权； 否则调用executeLogin进行登录，通过auth code创建OAuth2Token提交给Subject进行登录； 登录成功将回调onLoginSuccess方法重定向到成功页面； 登录失败则回调onLoginFailure重定向到失败页面。 OAuth2Realmpublic class OAuth2Realm extends AuthorizingRealm { private String clientId; private String clientSecret; private String accessTokenUrl; private String userInfoUrl; private String redirectUrl; //省略setter public boolean supports(AuthenticationToken token) { return token instanceof OAuth2Token; //表示此Realm只支持OAuth2Token类型 } protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); return authorizationInfo; } protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { OAuth2Token oAuth2Token = (OAuth2Token) token; String code = oAuth2Token.getAuthCode(); //获取 auth code String username = extractUsername(code); // 提取用户名 SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(username, code, getName()); return authenticationInfo; } private String extractUsername(String code) { try { OAuthClient oAuthClient = new OAuthClient(new URLConnectionClient()); OAuthClientRequest accessTokenRequest = OAuthClientRequest .tokenLocation(accessTokenUrl) .setGrantType(GrantType.AUTHORIZATION_CODE) .setClientId(clientId).setClientSecret(clientSecret) .setCode(code).setRedirectURI(redirectUrl) .buildQueryMessage(); //获取access token OAuthAccessTokenResponse oAuthResponse = oAuthClient.accessToken(accessTokenRequest, OAuth.HttpMethod.POST); String accessToken = oAuthResponse.getAccessToken(); Long expiresIn = oAuthResponse.getExpiresIn(); //获取user info OAuthClientRequest userInfoRequest = new OAuthBearerClientRequest(userInfoUrl) .setAccessToken(accessToken).buildQueryMessage(); OAuthResourceResponse resourceResponse = oAuthClient.resource( userInfoRequest, OAuth.HttpMethod.GET, OAuthResourceResponse.class); String username = resourceResponse.getBody(); return username; } catch (Exception e) { throw new OAuth2AuthenticationException(e); } } } 此Realm首先只支持OAuth2Token类型的Token；然后通过传入的auth code去换取access token；再根据access token去获取用户信息（用户名），然后根据此信息创建AuthenticationInfo；如果需要AuthorizationInfo信息，可以根据此处获取的用户名再根据自己的业务规则去获取。 Spring shiro配置（spring-config-shiro.xml）&lt;bean id=&quot;oAuth2Realm&quot; class=&quot;com.github.zhangkaitao.shiro.chapter18.oauth2.OAuth2Realm&quot;&gt; &lt;property name=&quot;cachingEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;authenticationCachingEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;authenticationCacheName&quot; value=&quot;authenticationCache&quot;/&gt; &lt;property name=&quot;authorizationCachingEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;authorizationCacheName&quot; value=&quot;authorizationCache&quot;/&gt; &lt;property name=&quot;clientId&quot; value=&quot;c1ebe466-1cdc-4bd3-ab69-77c3561b9dee&quot;/&gt; &lt;property name=&quot;clientSecret&quot; value=&quot;d8346ea2-6017-43ed-ad68-19c0f971738b&quot;/&gt; &lt;property name=&quot;accessTokenUrl&quot; value=&quot;http://localhost:8080/chapter17-server/accessToken&quot;/&gt; &lt;property name=&quot;userInfoUrl&quot; value=&quot;http://localhost:8080/chapter17-server/userInfo&quot;/&gt; &lt;property name=&quot;redirectUrl&quot; value=&quot;http://localhost:9080/chapter17-client/oauth2-login&quot;/&gt; &lt;/bean&gt;; 此OAuth2Realm需要配置在服务端申请的clientId和clientSecret；及用于根据auth code换取access token的accessTokenUrl地址；及用于根据access token换取用户信息（受保护资源）的userInfoUrl地址。]]></content>
      <categories>
        <category>java框架</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>shiro</tag>
        <tag>OAuth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shiro RemableMe-单点登录]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fshiro%2FRemableMe-%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95-%2F</url>
    <content type="text"><![CDATA[RememberMeShiro 提供了记住我（RememberMe）的功能，比如访问如淘宝等一些网站时，关闭了浏览 器下次再打开时还是能记住你是谁，下次访问时无需再登录即可访问，基本流程如下 RememberMe 配置spring-shiro-web.xml 配置：&lt;!-- 会话 Cookie 模板 --&gt; &lt;bean id=&quot;sessionIdCookie&quot; class=&quot;org.apache.shiro.web.servlet.SimpleCookie&quot;&gt; &lt;constructor-arg value=&quot;sid&quot;/&gt; &lt;property name=&quot;httpOnly&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;maxAge&quot; value=&quot;-1&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;rememberMeCookie&quot; class=&quot;org.apache.shiro.web.servlet.SimpleCookie&quot;&gt; &lt;constructor-arg value=&quot;rememberMe&quot;/&gt; &lt;property name=&quot;httpOnly&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;maxAge&quot; value=&quot;2592000&quot;/&gt;&lt;!-- 30 天 --&gt; &lt;/bean&gt; sessionIdCookie：maxAge=-1 表示浏览器关闭时失效此 Cookie； rememberMeCookie：即记住我的 Cookie，保存时长 30 天； rememberMe 管理器，cipherKey 是加密 rememberMe Cookie 的密钥；默认 AES 算法 &lt;!-- 安全管理器 --&gt; &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; …… &lt;property name=&quot;rememberMeManager&quot; ref=&quot;rememberMeManager&quot;/&gt; &lt;/bean&gt; 另外对于过滤器，一般这样使用： 访问一般网页，如个人在主页之类的，我们使用 user 拦截器即可，user 拦截器只要用户登 录(isRemembered()==true or isAuthenticated()==true)过即可访问成功； 访问特殊网页，如我的订单，提交订单页面，我们使用 authc 拦截器即可，authc 拦截器会 判断用户是否是通过 Subject.login（isAuthenticated()==true）登录的，如果是才放行，否则会跳转到登录页面叫你重新登录 因此 RememberMe 使用过程中，需要配合相应的拦截器来实现相应的功能，用错了拦截器 可能就不能满足你的需求了。 SSL对于 SSL 的支持，Shiro 只是判断当前 url 是否需要 SSL 登录，如果需要自动重定向到 https 进行访问。 …略 单点登录Shiro 1.2 开始提供了 Jasig CAS 单点登录的支持，单点登录主要用于多系统集成，即在多个 系统中，用户只需要到一个中央服务器登录一次即可访问这些系统中的任何一个，无须多次登录。 大体流程如下： 访问客户端需要登录的页面 http://localhost:9080/ client/，此时会跳到单点登录服务器 https://localhost:8443/ server/login?service=https://localhost:9443/ client/cas； 如果此时单点登录服务器也没有登录的话，会显示登录表单页面，输入用户名/密码进 行登录； 登录成功后服务器端会回调客户端传入的地址： https://localhost:9443/client/cas?ticket=ST-1-eh2cIo92F9syvoMs5DOg-cas01.example.org，且带 着一个 ticket； 客户端会把 ticket 提交给服务器来验证 ticket 是否有效；如果有效服务器端将返回用户 身份； 客户端可以再根据这个用户身份获取如当前系统用户/角色/权限信息。 并发登录人数控制如果同时 有多人登录：要么不让后者登录；要么踢出前者登录（强制退出）。比如 spring security 就 直接提供了相应的功能；Shiro 的话没有提供默认实现，不过可以很容易的在 Shiro 中加入 这个功能。 &lt;bean id=&quot;kickoutSessionControlFilter&quot; class=&quot;com.github.zhangkaitao.shiro.chapter18.web.shiro.filter.KickoutSessionControlFilter&quot;&gt; &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot;/&gt; &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot;/&gt; &lt;property name=&quot;kickoutAfter&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;maxSession&quot; value=&quot;2&quot;/&gt; &lt;property name=&quot;kickoutUrl&quot; value=&quot;/login?kickout=1&quot;/&gt; &lt;/bean&gt; cacheManager：使用cacheManager获取相应的cache来缓存用户登录的会话；用于保存用户—会话之间的关系的； sessionManager：用于根据会话ID，获取会话进行踢出操作的； kickoutAfter：是否踢出后来登录的，默认是false；即后者登录的用户踢出前者登录的用户； maxSession：同一个用户最大的会话数，默认1；比如2的意思是同一个用户允许最多同时两个人登录； kickoutUrl：被踢出后重定向到的地址； KickoutSessionControlFilter核心代码：public class KickoutSessionControlFilter extends AccessControlFilter{ private String kickoutUrl; //踢出后到的地址 private boolean kickoutAfter; //踢出之前登录的/之后登录的用户 默认踢出之前登录的用户 private int maxSession; //同一个帐号最大会话数 默认1 private SessionManager sessionManager; private Cache&lt;String, Deque&lt;Serializable&gt;&gt; cache; public void setKickoutUrl(String kickoutUrl) { this.kickoutUrl = kickoutUrl; } public void setKickoutAfter(boolean kickoutAfter) { this.kickoutAfter = kickoutAfter; } public void setMaxSession(int maxSession) { this.maxSession = maxSession; } public void setSessionManager(SessionManager sessionManager) { this.sessionManager = sessionManager; } public void setCacheManager(CacheManager cacheManager) { this.cache = cacheManager.getCache(&quot;shiro-activeSessionCache&quot;); } /** * 是否允许访问，返回true表示允许 */ @Override protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception { return false; } /** * 表示访问拒绝时是否自己处理，如果返回true表示自己不处理且继续拦截器链执行，返回false表示自己已经处理了（比如重定向到另一个页面）。 */ @Override protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception { Subject subject = getSubject(request, response); if(!subject.isAuthenticated() &amp;&amp; !subject.isRemembered()) { //如果没有登录，直接进行之后的流程 return true; } Session session = subject.getSession(); String username = (String) subject.getPrincipal(); Serializable sessionId = session.getId(); // 初始化用户的队列放到缓存里 Deque&lt;Serializable&gt; deque = cache.get(username); if(deque == null) { deque = new LinkedList&lt;Serializable&gt;(); cache.put(username, deque); } //如果队列里没有此sessionId，且用户没有被踢出；放入队列 if(!deque.contains(sessionId) &amp;&amp; session.getAttribute(&quot;kickout&quot;) == null) { deque.push(sessionId); } //如果队列里的sessionId数超出最大会话数，开始踢人 while(deque.size() &gt; maxSession) { Serializable kickoutSessionId = null; if(kickoutAfter) { //如果踢出后者 kickoutSessionId=deque.getFirst(); kickoutSessionId = deque.removeFirst(); } else { //否则踢出前者 kickoutSessionId = deque.removeLast(); } try { Session kickoutSession = sessionManager.getSession(new DefaultSessionKey(kickoutSessionId)); if(kickoutSession != null) { //设置会话的kickout属性表示踢出了 kickoutSession.setAttribute(&quot;kickout&quot;, true); } } catch (Exception e) {//ignore exception e.printStackTrace(); } } //如果被踢出了，直接退出，重定向到踢出后的地址 if (session.getAttribute(&quot;kickout&quot;) != null) { //会话被踢出了 try { subject.logout(); } catch (Exception e) { } WebUtils.issueRedirect(request, response, kickoutUrl); return false; } return true; } }]]></content>
      <categories>
        <category>java框架</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shiro简介]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fshiro%2F%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[简介整体架构 Authentication：身份认证/登录 Authorization：授权， Session Manager：会话管理 Cryptography：加密，保护数据的安全性 WebSupport：Web支持 Caching：缓存 Concurrency：并发 Testing：测试 RunAs：允许一个用户假装为另一个用户的身份进行访问； RememberMe：记住我 认证授权核心 -Subject：主体-SecurityManager：安全管理器-Realm：域 Shiro架构 Authenticator：认证器 Authrizer：授权器 SessionDAO：用于会话的CRUD CacheManager：缓存控制器，来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能 身份验证 principals：身份 credentials：证明/凭证 授权 RBAC:基于资源的访问控制(Resource-Based Access Control)]]></content>
      <categories>
        <category>java框架</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shiro会话管理-缓存-spring集成]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fshiro%2F%E4%BC%9A%E8%AF%9D%E7%AE%A1%E7%90%86-%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86-Spring%20%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[会话管理显然会话管理原理很简单,通过session管理会话,根据token获取对应会话即可,那token怎么传呢,在tomcat中,session我们通过cookie设置会话id,那么shiro中没有cookie,他是如何做的呢 Shiro提供了完整的企业级会话管理功能，不依赖于底层容器（如web容器tomcat），不管JavaSE还是JavaEE环境都可以使用，提供了会话管理、会话事件监听、会话存储/持久化、容器无关的集群、失效/过期支持、对Web的透明支持、SSO单点登录的支持等特性。即直接使用Shiro的会话管理可以直接替换如Web容器的会话管理。 会话所谓会话，即用户访问应用时保持的连接关系，在多次交互中应用能够识别出当前访问的 用户是谁，且可以在多次交互中保存一些数据。 Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); Session接口 public interface Session { Serializable getId(); Date getStartTimestamp(); Date getLastAccessTime(); long getTimeout() throws InvalidSessionException; void setTimeout(long maxIdleTimeInMillis) throws InvalidSessionException; String getHost(); void touch() throws InvalidSessionException; //修改最后一次访问时间 void stop() throws InvalidSessionException; Collection&lt;Object&gt; getAttributeKeys() throws InvalidSessionException; Object getAttribute(Object key) throws InvalidSessionException; void setAttribute(Object key, Object value) throws InvalidSessionException; Object removeAttribute(Object key) throws InvalidSessionException; } 会话管理器会话管理器管理着应用中所有 Subject 的会话的创建、维护、删除、失效、验证等工作。是 Shiro 的核心组件，顶层组件 SecurityManager 直接继承了 SessionManager，且提供了 SessionsSecurityManager 实 现 直 接把会话管理委托给相应的 SessionManager ， DefaultSecurityManager 及 DefaultWebSecurityManager 默认 SecurityManager 都继承了 SessionsSecurityManager。 SecurityManager 提供了如下接口： Session start(SessionContext context); //启动会话 Session getSession(SessionKey key) throws SessionException; //根据会话 Key 获取会话 另外用于 Web 环境的 WebSessionManager 又提供了如下接口： boolean isServletContainerSessions();//是否使用 Servlet 容器的会话 Shiro 还提供了 ValidatingSessionManager 用于验资并过期会话： void validateSessions();//验证所有会话是否过期 会话监听器会话监听器用于监听会话创建、过期及停止事件 public interface SessionListener { void onStart(Session session); void onStop(Session session); void onExpiration(Session session); } 会话存储/持久化Shiro 提供 SessionDAO 用于会话的 CRUD，即 DAO（Data Access Object）模式实现 public interface SessionDAO { Serializable create(Session session); Session readSession(Serializable sessionId) throws UnknownSessionException; void update(Session session) throws UnknownSessionException; void delete(Session session); Collection&lt;Session&gt; getActiveSessions(); } AbstractSessionDAO提供了SessionDAO的基础实现，如生成会话ID等；CachingSessionDAO 提供了对开发者透明的会话缓存的功能，只需要设置相应的 CacheManager 即可； MemorySessionDAO 直接在内存中进行会话维护；而 EnterpriseCacheSessionDAO 提供了缓 存功能的会话维护，默认情况下使用 MapCache 实现，内部使用 ConcurrentHashMap 保存 缓存的会话。 Shiro 提供了使用 Ehcache 进行会话存储，Ehcache 可以配合 TerraCotta 实现容器无关的分布式集群。 sessionDAO=org.apache.shiro.session.mgt.eis.EnterpriseCacheSessionDAO sessionDAO.activeSessionsCacheName=shiro-activeSessionCache sessionManager.sessionDAO=$sessionDAO cacheManager = org.apache.shiro.cache.ehcache.EhCacheManager cacheManager.cacheManagerConfigFile=classpath:ehcache.xml securityManager.cacheManager = $cacheManager 配置 ehcache.xml &lt;cache name=&quot;shiro-activeSessionCache&quot; maxEntriesLocalHeap=&quot;10000&quot; overflowToDisk=&quot;false&quot; eternal=&quot;false&quot; diskPersistent=&quot;false&quot; timeToLiveSeconds=&quot;0&quot; timeToIdleSeconds=&quot;0&quot; statistics=&quot;true&quot; /&gt; 另外可以通过如下 ini 配置设置会话 ID 生成器： sessionIdGenerator=org.apache.shiro.session.mgt.eis.JavaUuidSessionIdGenerator sessionDAO.sessionIdGenerator=$sessionIdGenerator 如果自定义实现 SessionDAO，继承 CachingSessionDAO 即可： public class MySessionDAO extends CachingSessionDAO { private JdbcTemplate jdbcTemplate = JdbcTemplateUtils.jdbcTemplate(); protected Serializable doCreate(Session session) { Serializable sessionId = generateSessionId(session); assignSessionId(session, sessionId); String sql = &quot;insert into sessions(id, session) values(?,?)&quot;; jdbcTemplate.update(sql, sessionId, SerializableUtils.serialize(session)); return session.getId(); } protected void doUpdate(Session session) { if(session instanceof ValidatingSession &amp;&amp; !((ValidatingSession)session).isValid()) { return; //如果会话过期/停止 没必要再更新了 } String sql = &quot;update sessions set session=? where id=?&quot;; jdbcTemplate.update(sql, SerializableUtils.serialize(session), session.getId()); } protected void doDelete(Session session) { String sql = &quot;delete from sessions where id=?&quot;; jdbcTemplate.update(sql, session.getId()); } protected Session doReadSession(Serializable sessionId) { String sql = &quot;select session from sessions where id=?&quot;; List&lt;String&gt; sessionStrList = jdbcTemplate.queryForList(sql, String.class, sessionId); if(sessionStrList.size() == 0) return null; return SerializableUtils.unSerialize(sessionStrList.get(0)); } } Shiro 提供了会话验证调度器，用于定期的验证会话是否已过期，如果过期将停止会话；出 于性能考虑，一般情况下都是获取会话时来验证会话是否过期并停止会话的；但是如在 web 环境中，如果用户不主动退出是不知道会话是否过期的，因此需要定期的检测会话是否过 期，Shiro 提供了会话验证调度器 SessionValidationScheduler 来做这件事情。 sessionValidationScheduler=org.apache.shiro.session.mgt.ExecutorServiceSessionValidationScheduler sessionValidationScheduler.interval = 3600000 sessionValidationScheduler.sessionManager=$sessionManager sessionManager.globalSessionTimeout=1800000 sessionManager.sessionValidationSchedulerEnabled=true sessionManager.sessionValidationScheduler=$sessionValidationScheduler Shiro 也提供了使用 Quartz 会话验证调度器 sessionValidationScheduler=org.apache.shiro.session.mgt.quartz.QuartzSessionValidationScheduler sessionFactorysessionFactory 是创建会话的工厂，根据相应的 Subject 上下文信息来创建会话；默认提供了 SimpleSessionFactory 用来创建 SimpleSession 会话。 可以自定义session 缓存管理Shiro 提供了类似于 Spring 的 Cache 抽象，即 Shiro 本身不实现 Cache，但是对 Cache 进行 了又抽象，方便更换不同的底层 Cache 实现。对于 Cache 的一些概念可以参考《Spring Cache 抽象详解》：http://jinnianshilongnian.iteye.com/blog/2001040。 Shiro 提供的 Cache 接口： public interface Cache&lt;K, V&gt; { //根据 Key 获取缓存中的值 public V get(K key) throws CacheException; //往缓存中放入 key-value，返回缓存中之前的值 public V put(K key, V value) throws CacheException; //移除缓存中 key 对应的值，返回该值 public V remove(K key) throws CacheException; //清空整个缓存 public void clear() throws CacheException; //返回缓存大小 public int size(); //获取缓存中所有的 key public Set&lt;K&gt; keys(); //获取缓存中所有的 value public Collection&lt;V&gt; values(); } Shiro 提供的 CacheManager 接口： public interface CacheManager { //根据缓存名字获取一个 Cache public &lt;K, V&gt; Cache&lt;K, V&gt; getCache(String name) throws CacheException; } Shiro 还提供了 CacheManagerAware 用于注入 CacheManager： public interface CacheManagerAware { //注入 CacheManager void setCacheManager(CacheManager cacheManager); } Shiro 内部相应的组件（DefaultSecurityManager）会自动检测相应的对象（如 Realm）是否 实现了 CacheManagerAware 并自动注入相应的 CacheManager。 Realm 缓存Shiro 提供了 CachingRealm，其实现了 CacheManagerAware 接口，提供了缓存的一些基础 实现；另外 AuthenticatingRealm 及 AuthorizingRealm 分别提供了对 AuthenticationInfo 和 AuthorizationInfo 信息的缓存。 但缓存也可以使用spring cache或者自己通过AOP实现的cache Session缓存如 securityManager 实现了 SessionsSecurityManager，其会自动判断 SessionManager 是否实 现了 CacheManagerAware 接口，如果实现了会把 CacheManager 设置给它。然后 sessionManager 会判断相应的 sessionDAO（如继承自 CachingSessionDAO）是否实现了 CacheManagerAware，如果实现了会把 CacheManager 设置给它 spring集成Shiro 的组件都是 JavaBean/POJO 式的组件，所以非常容易使用 Spring 进行组件管理 &lt;!-- 缓存管理器 使用 Ehcache 实现 --&gt; &lt;bean id=&quot;cacheManager&quot; class=&quot;org.apache.shiro.cache.ehcache.EhCacheManager&quot;&gt; &lt;property name=&quot;cacheManagerConfigFile&quot; value=&quot;classpath:ehcache.xml&quot;/&gt; &lt;/bean&gt; &lt;!-- 凭证匹配器 --&gt; &lt;bean id=&quot;credentialsMatcher&quot; class=&quot; com.github.zhangkaitao.shiro.chapter12.credentials.RetryLimitHashedCredentialsMatcher&quot;&gt; &lt;constructor-arg ref=&quot;cacheManager&quot;/&gt; &lt;property name=&quot;hashAlgorithmName&quot; value=&quot;md5&quot;/&gt; &lt;property name=&quot;hashIterations&quot; value=&quot;2&quot;/&gt; &lt;property name=&quot;storedCredentialsHexEncoded&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt; &lt;!-- Realm 实现 --&gt; &lt;bean id=&quot;userRealm&quot; class=&quot;com.github.zhangkaitao.shiro.chapter12.realm.UserRealm&quot;&gt; &lt;property name=&quot;userService&quot; ref=&quot;userService&quot;/&gt; &lt;property name=&quot;credentialsMatcher&quot; ref=&quot;credentialsMatcher&quot;/&gt; &lt;property name=&quot;cachingEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;authenticationCachingEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;authenticationCacheName&quot; value=&quot;authenticationCache&quot;/&gt; &lt;property name=&quot;authorizationCachingEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;authorizationCacheName&quot; value=&quot;authorizationCache&quot;/&gt; &lt;/bean&gt; &lt;!-- 会话 ID 生成器 --&gt; &lt;bean id=&quot;sessionIdGenerator&quot; class=&quot;org.apache.shiro.session.mgt.eis.JavaUuidSessionIdGenerator&quot;/&gt; &lt;!-- 会话 DAO --&gt; &lt;bean id=&quot;sessionDAO&quot; class=&quot;org.apache.shiro.session.mgt.eis.EnterpriseCacheSessionDAO&quot;&gt; &lt;property name=&quot;activeSessionsCacheName&quot; value=&quot;shiro-activeSessionCache&quot;/&gt; &lt;property name=&quot;sessionIdGenerator&quot; ref=&quot;sessionIdGenerator&quot;/&gt; &lt;/bean&gt; &lt;!-- 会话验证调度器 --&gt; &lt;bean id=&quot;sessionValidationScheduler&quot; class=&quot;org.apache.shiro.session.mgt.quartz.QuartzSessionValidationScheduler&quot;&gt; &lt;property name=&quot;sessionValidationInterval&quot; value=&quot;1800000&quot;/&gt; &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot;/&gt; &lt;/bean&gt; &lt;!-- 会话管理器 --&gt; &lt;bean id=&quot;sessionManager&quot; class=&quot;org.apache.shiro.session.mgt.DefaultSessionManager&quot;&gt; &lt;property name=&quot;globalSessionTimeout&quot; value=&quot;1800000&quot;/&gt; &lt;property name=&quot;deleteInvalidSessions&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;sessionValidationSchedulerEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;sessionValidationScheduler&quot; ref=&quot;sessionValidationScheduler&quot;/&gt; &lt;property name=&quot;sessionDAO&quot; ref=&quot;sessionDAO&quot;/&gt; &lt;/bean&gt; &lt;!-- 安全管理器 --&gt; &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.mgt.DefaultSecurityManager&quot;&gt; &lt;property name=&quot;realms&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;userRealm&quot;/&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot;/&gt; &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot;/&gt; &lt;/bean&gt; &lt;!-- 相当于调用 SecurityUtils.setSecurityManager(securityManager) --&gt; &lt;bean class=&quot;org.springframework.beans.factory.config.MethodInvokingFactoryBean&quot;&gt; &lt;property name=&quot;staticMethod&quot; value=&quot;org.apache.shiro.SecurityUtils.setSecurityManager&quot;/&gt; &lt;property name=&quot;arguments&quot; ref=&quot;securityManager&quot;/&gt; &lt;/bean&gt; &lt;!-- Shiro 生命周期处理器--&gt; &lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt; Web集成 &lt;!-- 会话 Cookie 模板 --&gt; &lt;bean id=&quot;sessionIdCookie&quot; class=&quot;org.apache.shiro.web.servlet.SimpleCookie&quot;&gt; &lt;constructor-arg value=&quot;sid&quot;/&gt; &lt;property name=&quot;httpOnly&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;maxAge&quot; value=&quot;180000&quot;/&gt; &lt;/bean&gt; &lt;!-- 会话管理器 --&gt; &lt;bean id=&quot;sessionManager&quot; class=&quot;org.apache.shiro.web.session.mgt.DefaultWebSessionManager&quot;&gt; &lt;property name=&quot;globalSessionTimeout&quot; value=&quot;1800000&quot;/&gt; &lt;property name=&quot;deleteInvalidSessions&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;sessionValidationSchedulerEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;sessionValidationScheduler&quot; ref=&quot;sessionValidationScheduler&quot;/&gt; &lt;property name=&quot;sessionDAO&quot; ref=&quot;sessionDAO&quot;/&gt; &lt;property name=&quot;sessionIdCookieEnabled&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;sessionIdCookie&quot; ref=&quot;sessionIdCookie&quot;/&gt; &lt;/bean&gt; &lt;!-- 安全管理器 --&gt; &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; &lt;property name=&quot;realm&quot; ref=&quot;userRealm&quot;/&gt; &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot;/&gt; &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot;/&gt; &lt;/bean&gt; sessionIdCookie 是用于生产 Session ID Cookie 的模板； 会话管理器使用用于 web 环境的 DefaultWebSessionManager； 安全管理器使用用于 web 环境的 DefaultWebSecurityManager。 身份验证过滤器 &lt;!-- Shiro 的 Web 过滤器 --&gt; &lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;property name=&quot;loginUrl&quot; value=&quot;/login.jsp&quot;/&gt; &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/unauthorized.jsp&quot;/&gt; &lt;property name=&quot;filterChainDefinitions&quot;&gt; &lt;value&gt; /index.jsp = anon /unauthorized.jsp = anon /login.jsp = authc /logout = logout /** = user &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; shiroFilter：此处使用 ShiroFilterFactoryBean 来创建 ShiroFilter 过滤器；filterChainDefinitions 用于声明 url 和 filter 的关系 接着需要在 web.xml 中进行如下配置： &lt;filter&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; DelegatingFilterProxy 会自动到 Spring 容器中查找名字为 shiroFilter 的 bean 并把 filter 请求 交给它处理。 Shiro 权限注解Shiro 提供了相应的注解用于权限控制，如果使用这些注解就需要使用 AOP 的功能来进行 判断，如 Spring AOP；Shiro 提供了 Spring AOP 集成用于权限注解的解析和验证。 在 spring-mvc.xml 配置文件添加 Shiro Spring AOP 权限注解的支持 &lt;aop:config proxy-target-class=&quot;true&quot;&gt;&lt;/aop:config&gt; &lt;bean class=&quot; org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;/bean&gt; 如上配置用于开启 Shiro Spring AOP 权限注解的支持；&lt;aop:config proxy-target-class=”true”&gt; 接着就可以在相应的控制器（AnnotationController）中使用如下方式进行注解： @RequiresRoles(&quot;admin&quot;) @RequestMapping(&quot;/hello2&quot;) public String hello2() { return &quot;success&quot;; } 当验证失败，其会抛出UnauthorizedException异常，此时可以使用Spring的ExceptionHandler （DefaultExceptionHandler）来进行拦截处理： 权限注解 表示当前 Subject 已经通过 @RequiresAuthentication 表示当前 Subject 已经身份验证或者通过记住我登录的 @RequiresUser 表示当前 Subject 没有身份验证或通过记住我登录过，即是游客身份。@RequiresGuest 表示当前 Subject 需要角色 admin 和 user。 @RequiresRoles(value={“admin”, “user”}, logical= Logical.AND) 表示当前 Subject 需要权限 user:a 或 user:b。 @RequiresPermissions (value={“user:a”, “user:b”}, logical= Logical.OR)]]></content>
      <categories>
        <category>java框架</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring AOP]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fspring%2Fspring%20aop%2F</url>
    <content type="text"><![CDATA[概念 切面（Aspect）：一个关注点的模块化。在Spring AOP中，切面可以使用基于模式）或者基于Aspect注解方式来实现。通俗点说就是我们加入的切面类（比如log类 连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是表示一个方法的执行。 通知（Advice）：在切面的某个特定的连接点上执行的动作。其中包括了“around”、“before”和“after”等不同类型的通知 切入点（Pointcut）：匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 引入（Introduction）：用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个bean实现IsModified接口，以便简化缓存机制。 目标对象（Target Object）： 被一个或者多个切面所通知的对象。也被称做被通知（advised）对象。 AOP代理（AOP Proxy）：AOP框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）：把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 通知类型： 前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）。 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知。 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 环绕通知（Around Advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 基于注解的实现spring.xml 12&lt;aop:aspectj-autoproxy/&gt; &lt;context:component-scan base-package="com.xxx.webmvc.templete.aop" /&gt; 例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Component // 加入到IoC容器@Aspect // 指定当前类为切面类public class ExampleAop &#123; // 指定切入点表达式，拦截那些方法，即为那些类生成代理对象 // @Pointcut("execution(* com.bie.aop.UserDao.save(..))") ..代表所有参数 // @Pointcut("execution(* com.bie.aop.UserDao.*())") 指定所有的方法 // @Pointcut("execution(* com.bie.aop.UserDao.save())") 指定save方法 @Pointcut("execution(* com.xxx.webmvc.templete.service.UserService.add(..))") public void pointCut() &#123; &#125; @Before(value="pointCut() &amp;&amp; args(user)") public void begin(User user) &#123; System.out.println("before "+user); &#125; @After(value="pointCut()") public void close(JoinPoint point) &#123; point.getArgs(); System.out.println("after"); &#125; @AfterThrowing(value="pointCut()", throwing = "t") public void AfterThrowing(Throwable t) &#123; System.out.println("method AfterThrowing "+ t); &#125; @AfterReturning(value="pointCut()", returning = "obj") public void AfterReturning(Object obj) &#123; System.out.println("method AfterReturning "+obj); &#125; @Around(value = "execution(* com.xxx.webmvc.templete.service.UserService.query(..)) &amp;&amp; args(user)") public Object Around(ProceedingJoinPoint pjp, User user) &#123; System.out.println("method Around start"); Object object = null; try &#123; object = pjp.proceed(); &#125; catch (Throwable e) &#123; System.out.println("method exception"); &#125; System.out.println("method Around end and return "+object); return object; &#125; &#125; 错误： error at ::0 formal unbound in pointcut 原因：参数绑定错误，原&quot;execution(* com.xxx.webmvc.templete.service.UserService.query(..))&quot;改为&quot;execution(* com.xxx.webmvc.templete.service.UserService.query(..)) &amp;&amp; args(user)&quot;即可 不生效 注意&lt;aop:aspectj-autoproxy/&gt;]]></content>
      <categories>
        <category>java框架</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cache]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fspring%2Fspring%20cache%2F</url>
    <content type="text"><![CDATA[spring cache的使用缓存某些方法的执行结果设置好缓存配置之后我们就可以使用 @Cacheable 注解来缓存方法执行的结果了spring cache的使用是非常简单的,只需要在方法上标注 @Cacheable 注解就可以 123456789@Cacheable(&quot;getName&quot;) public String getName(String id) &#123; return xxx; &#125; @Cacheable(&quot;getUser&quot;) public User searchCity(String userId)&#123; return this.query(userId); &#125; 整个@Cacheable的流程是:先从缓存中读取，如果没有再调用方法获取数据，然后把数据添加到缓存中. 缓存数据一致性保证在我们配置了查询用户名的方法缓存之后,假如这时候我们改名了,我想让我自己查询的时候能看到我自己的改变。这时候我们该怎么做呢？ @CacheEvict移除缓存我们这时候可以在修改方法上加上@CacheEvict注解,来更新查询方法的结果缓存 12345@CacheEvict(value = &quot;getName&quot;, key = &quot;#user.id&quot;) //移除指定key的数据 public User updateUser(User user) &#123; users.update(user); return user; &#125; 这时候再调用新的查询接口,我们就不会走缓存里的方法,会重新去数据库中查询.因为调用修改用户的方法的时候, @CacheEvict会根据相应的key和value作为条件从缓存中移除相应的数据. @CachePut 既要保证方法被调用，又希望结果被缓存据前面的例子，我们知道，如果使用了 @Cacheable 注释，则当重复使用相同参数调用方法的时候，方法本身不会被调用执行，即方法本身被略过了，取而代之的是方法的结果直接从缓存中找到并返回了. 现实中并不总是如此，有些情况下我们希望方法一定会被调用，因为其除了返回一个结果，还做了其他事情，例如记录日志，调用接口等，这个时候，我们可以用 @CachePut 注释，这个注释可以确保方法被执行，同时方法的返回值也被记录到缓存中. 12345678910111213141516@Cacheable(value=&quot;userCache&quot;) public User getUserById(String id) &#123; // 方法内部实现不考虑缓存逻辑，直接实现业务 return getUserFromDB(id); &#125; // 更新 userCache 缓存 @CachePut(value=&quot;userCache&quot;,key=&quot;#user.id&quot;) public User updateUser(User user) &#123; return updateUserToDB(user); &#125; private User updateUserToDB(User user) &#123; // do some query return user; &#125; 如上面的代码所示，我们首先用 getUserById 方法查询一个人的信息，这个时候会查询数据库一次，但是也记录到缓存中了。然后我们修改了用户信息，调用了 updateUser 方法，这个时候会执行数据库的更新操作且记录到缓存，我们再次修改信息并调用 updateUser 方法，然后通过 getUserById 方法查询，这个时候，由于缓存中已经有数据，所以不会查询数据库，而是直接返回最新的数据 三种方式的对比@Cacheable、@CachePut、@CacheEvict 注释介绍 @Cacheable 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存 @CachePut 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存，和 @Cacheable 不同的是，它每次都会触发真实方法的调用 @CachEvict 主要针对方法配置，能够根据一定的条件对缓存进行清空 基本原理一句话介绍就是Spring AOP的动态代理技术。 如果读者对Spring AOP不熟悉的话，可以去看看官方文档 注意和限制基于 proxy 的 spring aop 带来的内部调用问题上面介绍过 spring cache 的原理，即它是基于动态生成的 proxy 代理机制来对方法的调用进行切面，这里关键点是对象的引用问题. 如果对象的方法是内部调用（即 this 引用）而不是外部引用，则会导致 proxy 失效，那么我们的切面就失效，也就是说上面定义的各种注释包括 @Cacheable、@CachePut 和 @CacheEvict 都会失效，我们来演示一下。 123456789public Account getAccountByName2(String accountName) &#123; return this.getAccountByName(accountName); &#125; @Cacheable(value=&quot;accountCache&quot;)// 使用了一个缓存名叫 accountCache public Account getAccountByName(String accountName) &#123; // 方法内部实现不考虑缓存逻辑，直接实现业务 return getFromDB(accountName); &#125; 上面我们定义了一个新的方法 getAccountByName2，其自身调用了 getAccountByName 方法，这个时候，发生的是内部调用（this），所以没有走 proxy，导致 spring cache 失效 要避免这个问题，就是要避免对缓存方法的内部调用，或者避免使用基于 proxy 的 AOP 模式，可以使用基于 aspectJ 的 AOP 模式来解决这个问题。 @CacheEvict 的可靠性问题我们看到,@CacheEvict 注释有一个属性 beforeInvocation,缺省为 false,即缺省情况下，都是在实际的方法执行完成后，才对缓存进行清空操作。期间如果执行方法出现异常，则会导致缓存清空不被执行。我们演示一下 12345// 清空 accountCache 缓存 @CacheEvict(value=&quot;accountCache&quot;,allEntries=true) public void reload() &#123; throw new RuntimeException(); &#125; 我们的测试代码如下： 12345678accountService.getAccountByName(&quot;someone&quot;); accountService.getAccountByName(&quot;someone&quot;); try &#123; accountService.reload(); &#125; catch (Exception e) &#123; //... &#125; accountService.getAccountByName(&quot;someone&quot;); 注意上面的代码，我们在 reload 的时候抛出了运行期异常，这会导致清空缓存失败。上面的测试代码先查询了两次，然后 reload，然后再查询一次，结果应该是只有第一次查询走了数据库，其他两次查询都从缓存，第三次也走缓存因为 reload 失败了。 那么我们如何避免这个问题呢？我们可以用 @CacheEvict 注释提供的 beforeInvocation 属性，将其设置为 true，这样，在方法执行前我们的缓存就被清空了。可以确保缓存被清空。 非 public 方法问题和内部调用问题类似，非 public 方法如果想实现基于注释的缓存，必须采用基于 AspectJ 的 AOP 机制 Dummy CacheManager 的配置和作用有的时候，我们在代码迁移、调试或者部署的时候，恰好没有 cache 容器，比如 memcache 还不具备条件，h2db 还没有装好等，如果这个时候你想调试代码，岂不是要疯掉？这里有一个办法，在不具备缓存条件的时候，在不改代码的情况下，禁用缓存。 方法就是修改 spring的配置文件，设置一个找不到缓存就不做任何操作的标志位 1234567891011121314151617181920&lt;cache:annotation-driven /&gt; &lt;bean id=&quot;simpleCacheManager&quot; class=&quot;org.springframework.cache.support.SimpleCacheManager&quot;&gt; &lt;property name=&quot;caches&quot;&gt; &lt;set&gt; &lt;bean class=&quot;org.springframework.cache.concurrent.ConcurrentMapCacheFactoryBean&quot; p:name=&quot;default&quot; /&gt; &lt;/set&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;cacheManager&quot; class=&quot;org.springframework.cache.support.CompositeCacheManager&quot;&gt; &lt;property name=&quot;cacheManagers&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;simpleCacheManager&quot; /&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;fallbackToNoOpCache&quot; value=&quot;true&quot; /&gt; &lt;/bean&gt; 注意以前的 cacheManager 变为了 simpleCacheManager，且没有配置 accountCache 实例，后面的 cacheManager 的实例是一个 CompositeCacheManager，他利用了前面的 simpleCacheManager 进行查询，如果查询不到，则根据标志位 fallbackToNoOpCache 来判断是否不做任何缓存操作。 使用 guava cache12345678&lt;bean id=&quot;cacheManager&quot; class=&quot;org.springframework.cache.guava.GuavaCacheManager&quot;&gt; &lt;property name=&quot;cacheSpecification&quot; value=&quot;concurrencyLevel=4,expireAfterAccess=100s,expireAfterWrite=100s&quot; /&gt; &lt;property name=&quot;cacheNames&quot;&gt; &lt;list&gt; &lt;value&gt;dictTableCache&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 代码地址https://github.com/rollenholt/spring-cache-example 参考链接csdn的技术文:Redis 缓存 + Spring 的集成示例 Ibm的技术文:注释驱动的 Spring cache 缓存介绍开涛大神的技术博客:Spring Cache抽象详解 Spring @Cacheable 的key生成​ 自定义策略是指我们可以通过Spring的EL表达式来指定我们的key。这里的EL表达式可以使用方法参数及它们对应的属性。使用方法参数时我们可以直接使用“#参数名”或者“#p参数index”。下面是几个使用参数作为key的示例。 12345678910111213141516171819@Cacheable(value="users", key="#id")public User find(Integer id) &#123; return null;&#125;@Cacheable(value="users", key="#p0")public User find(Integer id) &#123; return null;&#125;@Cacheable(value="users", key="#user.id")public User find(User user) &#123; return null;&#125;@Cacheable(value="users", key="#p0.id")public User find(User user) &#123; return null;&#125; 除了上述使用方法参数作为key之外，Spring还为我们提供了一个root对象可以用来生成key。通过该root对象我们可以获取到以下信息。 属性名称 描述 示例 methodName 当前方法名 #root.methodName method 当前方法 #root.method.name target 当前被调用的对象 #root.target targetClass 当前被调用的对象的class #root.targetClass args 当前方法参数组成的数组 #root.args[0] caches 当前被调用的方法使用的Cache #root.caches[0].name]]></content>
      <categories>
        <category>java框架</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shiro授权-认证]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fshiro%2F%E6%8E%88%E6%9D%83-%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[简介maven 地址123456&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;web等依赖未包括 整体架构Shiro可以非常容易的开发出足够好的应用，其不仅可以用在JavaSE环境，也可以用在JavaEE环境。Shiro可以帮助我们完成：认证、授权、加密、会话管理、与Web集成、缓存等。 Authentication：身份认证/登录，验证用户是不是拥有相应的身份； Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情，常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限； Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通JavaSE环境的，也可以是如Web环境的； Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储 WebSupport：Web支持，可以非常容易的集成到Web环境； Caching：缓存，比如用户登录后，其用户信息、拥有的角色/权限不必每次去查，这样可以提高效率； Concurrency：shiro支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去； Testing：提供测试支持； RunAs：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问； RememberMe：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 认证授权核心Shiro不会去维护用户、维护权限；这些需要我们自己去设计/提供；然后通过相应的接口注入给Shiro即可。 Subject：主体，代表了当前“用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是Subject，如网络爬虫，机器人等；即一个抽象概念；所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager SecurityManager：安全管理器；即所有与安全有关的操作都会与SecurityManager交互；且它管理着所有Subject Realm：域，Shiro从从Realm获取安全数据（如用户、角色、权限），就是说SecurityManager要验证用户身份，那么它需要从Realm获取相应的用户进行比较以确定用户身份是否合法；也需要从Realm得到用户相应的角色/权限进行验证用户是否能进行操作；可以把Realm看成DataSource，即安全数据源。 应用代码通过Subject来进行认证和授权，而Subject又委托给SecurityManager；我们需要给Shiro的SecurityManager注入Realm，从而让SecurityManager能得到合法的用户及其权限进行判断。 Shiro架构 Subject：主体，可以看到主体可以是任何可以与应用交互的“用户”； SecurityManager：Shiro的心脏；所有具体的交互都通过SecurityManager进行控制；它管理着所有Subject、且负责进行认证和授权、及会话、缓存的管理。 Authenticator：认证器，负责主体认证的，这是一个扩展点，可以自定义实现 Authrizer：授权器 Realm：可以有1个或多个Realm，可以认为是安全实体数据源，即用于获取安全实体的；可以是JDBC实现，也可以是LDAP实现，或者内存实现等等；由用户提供；注意：Shiro不知道你的用户/权限存储在哪及以何种格式存储；所以我们一般在应用中都需要实现自己的Realm； SessionManager：Session呢需要有人去管理它的生命周期，这个组件就是SessionManager；而Shiro并不仅仅可以用在Web环境，也可以用在如普通的JavaSE环境、EJB等环境；所有呢，Shiro就抽象了一个自己的Session来管理主体与应用之间交互的数据；也方便实现自己的分布式会话（如把数据放到Memcached服务器）； SessionDAO：用于会话的CRUD，比如我们想把Session保存到数据库，那么可以实现自己的SessionDAO，通过如JDBC写到数据库；比如想把Session放到Memcached中，可以实现自己的MemcachedSessionDAO；另外SessionDAO中可以使用Cache进行缓存，以提高性能； CacheManager：缓存控制器，来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能 Cryptography：密码模块，Shiro提高了一些常见的加密组件用于如密码加密/解密的。 身份验证作用：验证身份信息，例如用户名密码。 principals：身份，即主体的标识属性，如用户名、邮箱等，唯一即可。 一个主体可以有多个principals，但只有一个Primary principals，一般是用户名/密码/手机号。 credentials：证明/凭证，即只有主体知道的安全值，如密码/数字证书等。 token封装了这两个参数12345public interface AuthenticationToken extends Serializable &#123; Object getPrincipal(); Object getCredentials();&#125; 例子1234567891011121314151617//1、获取 SecurityManager 工厂，此处使用 Ini 配置文件初始化SecurityManager Factory&lt;org.apache.shiro.mgt.SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;);//2、得到 SecurityManager 实例 并绑定给 SecurityUtilsorg.apache.shiro.mgt.SecurityManager securityManager = factory.getInstance();SecurityUtils.setSecurityManager(securityManager);//3、得到 Subject 及创建用户名/密码身份验证 Token（即用户身份/凭证）Subject subject = SecurityUtils.getSubject();UsernamePasswordToken token = new UsernamePasswordToken(&quot;zhang&quot;, &quot;123&quot;);try &#123; //4、登录，即身份验证 subject.login(token);&#125; catch (AuthenticationException e) &#123; //5、身份验证失败&#125;Assert.assertEquals(true, subject.isAuthenticated()); //断言用户已经登录//6、退出subject.logout(); SecurityManager通过工厂方法获得，单例的，再注册给SecurityUtils 通过 SecurityUtils 得到 Subject，其会自动绑定到当前线程，调用 subject.login()方法进行登录 如果身份验证失败请捕获 AuthenticationException 或 其 子类，常见的如： DisabledAccountException（禁用的帐号）、LockedAccountException（锁定的帐号）、 UnknownAccountException（错误的帐号）、ExcessiveAttemptsException（登录失败次数过 多）、IncorrectCredentialsException（错误的凭证）、ExpiredCredentialsException（过期的 凭证）等 调用 subject.logout 退出 身份认证流程 首先调用Subject.login(token)进行登录，其会自动委托给SecurityManager，调用之前必须通过SecurityUtils.setSecurityManager()设置； SecurityManager负责真正的身份验证逻辑；它会委托给Authenticator进行身份验证； Authenticator才是真正的身份验证者，ShiroAPI中核心的身份认证入口点，此处可以自定义插入自己的实现； Authenticator可能会委托给相应的AuthenticationStrategy进行多Realm身份验证，默认ModularRealmAuthenticator会调用AuthenticationStrategy进行多Realm身份验证； Authenticator会把相应的token传入Realm，从Realm获取身份验证信息，如果没有返回/抛出异常表示身份验证失败了。此处可以配置多个Realm，将按照相应的顺序及策略进行访问。 源码跟踪如下==这个可以先不看,等看了常见类的继承图就容易了==1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586//获取SubjectSecurityUtils.class public static Subject getSubject()&#123;&#125;//loginSubject.class public void login(AuthenticationToken token)&#123; //... Subject subject = this.securityManager.login(this, token); //... &#125; //校验SecurityUtils.class public Subject login(Subject subject, AuthenticationToken token) &#123; AuthenticationInfo info; try &#123; info = this.authenticate(token);//核心 &#125; catch (AuthenticationException var7) &#123; //... this.onFailedLogin(token, ae, subject); throw var7; &#125; Subject loggedIn = this.createSubject(token, info, subject); //会设置新subject的属性,并且通过subjectDao保存subject(默认会保存在session中,参考DefaultSubjectDAO.class) this.onSuccessfulLogin(token, info, loggedIn); return loggedIn; &#125; public AuthenticationInfo authenticate(AuthenticationToken token)&#123; return this.authenticator.authenticate(token);//authenticator对应上图 &#125;//认证器认证AbstractAuthenticator.class public final AuthenticationInfo authenticate(AuthenticationToken token)&#123; //... try&#123; AuthenticationInfo info = this.doAuthenticate(token);//doAuthenticate是抽象类,由子类实现 &#125;catch()&#123;...&#125; &#125;子类ModularRealmAuthenticator.class protected AuthenticationInfo doAuthenticate(AuthenticationToken authenticationToken)&#123; Collection&lt;Realm&gt; realms = this.getRealms(); //获取realm集合 return realms.size() == 1?this.doSingleRealmAuthentication((Realm)realms.iterator().next(), authenticationToken):this.doMultiRealmAuthentication(realms, authenticationToken); &#125; //do...RealmAuthentication函数 //查找Realm集合,然后调用每一个的getAuthenticationInfo方法,Realm由用户实现 protected AuthenticationInfo doSingleRealmAuthentication(Realm realm, AuthenticationToken token)&#123; AuthenticationInfo info = realm.getAuthenticationInfo(token); return info; &#125; protected AuthenticationInfo doMultiRealmAuthentication(Collection&lt;Realm&gt; realms, AuthenticationToken token) &#123; AuthenticationStrategy strategy = this.getAuthenticationStrategy(); AuthenticationInfo aggregate = strategy.beforeAllAttempts(realms, token); Iterator var5 = realms.iterator(); while(var5.hasNext()) &#123; Realm realm = (Realm)var5.next(); aggregate = strategy.beforeAttempt(realm, token, aggregate); if(realm.supports(token)) &#123; try &#123; info = realm.getAuthenticationInfo(token); &#125; catch (Throwable var11) &#123; &#125; aggregate = strategy.afterAttempt(realm, token, info, aggregate, t); &#125; else &#123; &#125; &#125; aggregate = strategy.afterAllAttempts(token, aggregate); return aggregate; &#125;//Realm校验返回AuthenticationInfoAuthenticatingRealm.class public final AuthenticationInfo getAuthenticationInfo(AuthenticationToken token)&#123; AuthenticationInfo info = getCachedAuthenticationInfo(token);//缓存 if (info == null) &#123; info = doGetAuthenticationInfo(token); if (token != null &amp;&amp; info != null) &#123; //添加缓存 cacheAuthenticationInfoIfPossible(token, info); &#125; &#125; else &#123;&#125; if (info != null) &#123; assertCredentialsMatch(token, info); //匹配token和info的凭证,这里用户可以自定义,默认匹配token和info的凭证是否完全相同,当然也有hash匹配这些 &#125; else &#123; &#125; return info; &#125;//返回Subject对象通过认证的info会存放到Subject对象里面 观察源码, shiro不止提供了多种认证器等,而且提供了多种监听器,如认证成功失败,认证前后等,参考AuthenticationStrategy.class.AuthenticationListener.class,用于判断在多realm下如何处理,分为3种(都是子类),后面会讲 很多类继承了很多接口,由于SecurityManager是最核心组件,用组合的方式组合其他组件 RealmRealm：域，Shiro 从从 Realm 获取安全数据（如用户、角色、权限） 接口123String getName(); //返回一个唯一的 Realm 名字 boolean supports(AuthenticationToken token); //判断此 Realm 是否支持此 Token AuthenticationInfo getAuthenticationInfo(AuthenticationToken token) throws AuthenticationException; //根据 Token 获取认证信息 自定义 Realm 实现12345678910111213141516171819202122232425public class MyRealm1 implements Realm &#123; @Override public String getName() &#123; return &quot;myrealm1&quot;; &#125; @Override public boolean supports(AuthenticationToken token) &#123; //仅支持 UsernamePasswordToken 类型的 Token return token instanceof UsernamePasswordToken; &#125; @Override public AuthenticationInfo getAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; String username = (String) token.getPrincipal(); //得到用户名 String password = new String((char[]) token.getCredentials()); //得到密码 if (!&quot;zhang&quot;.equals(username)) &#123; throw new UnknownAccountException(); //如果用户名错误 &#125; if (!&quot;123&quot;.equals(password)) &#123; throw new IncorrectCredentialsException(); //如果密码错误 &#125; //如果身份认证验证成功，返回一个 AuthenticationInfo 实现； return new SimpleAuthenticationInfo(username, password, getName()); &#125; &#125; Shiro 默认提供的 Realm一般继承 AuthorizingRealm（授权）即可；其继承了AuthenticatingRealm（即身份验证），而且也间接继承了 CachingRealm（带有缓存实现）。AuthorizingRealm的子类提供了通用的功能,如jdbcRealm方便使用数据库验证,PropertiesRealm用Properties配置文件做数据源,但一般继承AuthorizingRealm自己写 其他核心类结构==可以后面看== AuthenticationToken AuthenticationInfo 如果 Realm 是 AuthenticatingRealm 子类，则提供给 AuthenticatingRealm 内部使用的 CredentialsMatcher进行凭据验证；（如果没有继承它需要在自己的 Realm中自己实现验证） 提供给 SecurityManager 来创建 Subject（提供身份信息）； MergableAuthenticationInfo用于提供在多Realm时合并AuthenticationInfo的功能，主要合并Principal、如果是其他的如credentialsSalt，会用后边的信息覆盖前边的。一般返回 SimpleAuthenticationInfo 即可 PrincipalCollection 在 Shiro 中同时配置多个Realm，所以呢身份信息可能就有多个；因此其提供了PrincipalCollection 用于聚合这些身份信息.目前Shiro只提供了一个实现SimplePrincipalCollection,思考AuthenticationStrategy的合并Info AuthorizationInfo 当我们使用 AuthorizingRealm时，如果身份验证成功，在进行授权时就通过doGetAuthorizationInfo 方法获取角色/权限信息用于授权验证。 Subject Subject 是 Shiro 的核心对象，基本所有身份验证、授权都是通过 Subject 完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//身份信息获取 Object getPrincipal(); //Primary Principal PrincipalCollection getPrincipals(); // PrincipalCollection //身份验证 void login(AuthenticationToken token) throws AuthenticationException; boolean isAuthenticated(); boolean isRemembered(); // 角色授权验证boolean hasRole(String roleIdentifier); boolean[] hasRoles(List&lt;String&gt; roleIdentifiers); boolean hasAllRoles(Collection&lt;String&gt; roleIdentifiers); void checkRole(String roleIdentifier) throws AuthorizationException; void checkRoles(Collection&lt;String&gt; roleIdentifiers) throws AuthorizationException; void checkRoles(String... roleIdentifiers) throws AuthorizationException;//权限授权验证boolean isPermitted(String permission);boolean isPermitted(Permission permission);boolean[] isPermitted(String... permissions);boolean[] isPermitted(List&lt;Permission&gt; permissions);boolean isPermittedAll(String... permissions);boolean isPermittedAll(Collection&lt;Permission&gt; permissions);void checkPermission(String permission) throws AuthorizationException;void checkPermission(Permission permission) throws AuthorizationException;void checkPermissions(String... permissions) throws AuthorizationException;void checkPermissions(Collection&lt;Permission&gt; permissions) throws AuthorizationException;//会话Session getSession(); //相当于 getSession(true) Session getSession(boolean create);//退出void logout(); //RunAsvoid runAs(PrincipalCollection principals) throws NullPointerException, IllegalStateException; boolean isRunAs(); PrincipalCollection getPreviousPrincipals(); PrincipalCollection releaseRunAs(); //多线程 &lt;V&gt; V execute(Callable&lt;V&gt; callable) throws ExecutionException; void execute(Runnable runnable); &lt;V&gt; Callable&lt;V&gt; associateWith(Callable&lt;V&gt; callable); Runnable associateWith(Runnable runnable); //实现线程之间的 Subject 传播，因为 Subject 是线程绑定的；因此在多线程执行中需要传播 到相应的线程才能获取到相应的 Subject。最简单的办法就是通过 execute(runnable/callable 实例)直接调用；或者通过 associateWith(runnable/callable 实例)得到一个包装后的实例；它 们都是通过：1、把当前线程的 Subject 绑定过去；2、在线程执行结束后自动释放。 对于 Subject 我们一般这么使用： 身份验证（login） 授权（hasRole/isPermitted或 checkRole/checkPermission） 将相应的数据存储到会话（Session） 切换身份（RunAs）/多线程身份传播 退出 Authenticator 及 AuthenticationStrategySecurityManager 接口继承了 Authenticator，另外还有一个 ModularRealmAuthenticator 实现， 其委托给多个 Realm进行验证，验证规则通过AuthenticationStrategy接口指定，默认提供的实现： FirstSuccessfulStrategy：只要有一个Realm验证成功即可，只返回第一个Realm身份验证成功的认证信息，其他的忽略； AtLeastOneSuccessfulStrategy：只要有一个Realm验证成功即可，和FirstSuccessfulStrategy不同，返回所有Realm身份验证成功的认证信息； AllSuccessfulStrategy：所有Realm验证成功才算成功，且返回所有Realm身份验证成功的认证信息，如果有一个失败就失败了。 ModularRealmAuthenticator默认使用AtLeastOneSuccessfulStrategy策略。 授权 主体（Subject）:访问应用的用户 资源（Resource）:在应用中用户可以访问的任何东西，比如访问 JSP 页面 权限（Permission）:安全策略中的原子授权单位 角色（Role）:角色代表了操作集合,赋予用户角色而不是权限 授权方式Shiro 支持三种方式的授权： 编程式: 1234567Subject subject = SecurityUtils.getSubject(); if(subject.hasRole(“admin”)) &#123; //有权限 &#125; else &#123; //无权限 &#125;&#125; 注解式 1234@RequiresRoles(&quot;admin&quot;) public void hello() &#123; //有权限 &#125; JSP/GSP 标签 123&lt;shiro:hasRole name=&quot;admin&quot;&gt; &lt;!— 有权限 —&gt; &lt;/shiro:hasRole&gt; 基于角色的访问控制（隐式角色）直接用hasRole进行判断 Shiro 提供的 checkRole/checkRoles和hasRole/hasAllRoles不同的地方是它在判断为假的情 况下会抛出 UnauthorizedException 异常。 基于资源的访问控制（显示角色）Shiro 提供了 isPermitted 和isPermittedAll用于判断用户是否拥有某个权限或所有权限，也有checkPermission方法 用户——角色，角色——权限（资源：操作） Permission字符串通配符权限 规则：“资源标识符：操作：对象实例ID”,其默认支持通配符权限字符串，“:”表示资源/操作/实例的分割；“,”表示操作的分割； “*”表示任意资源/操作/实例 单个资源单个权限 1subject().checkPermissions(&quot;system:user:update&quot;); 单个资源多个权限 1subject().checkPermissions(&quot;system:user:update&quot;,&quot;system:user:delete&quot;); 单个资源全部权限 12subject().checkPermissions(&quot;system:user:create,delete,update:view&quot;)subject().checkPermissions(&quot;system:user:*&quot;); 所有资源全部权限 12role61=*:view也可以是:*:*:view 实例级别的权限 12345678910单个实例单个权限role71=user:view:1单个实例多个权限role72=&quot;user:update,delete:1&quot;单个实例所有权限role73=user:*:1所有实例单个权限role74=user:auth:*所有实例所有权限role71=user:*:* Shiro对权限字符串缺失部分的处理123前缀匹配。如“user:view”等价于“user:view:*”；而“organization”等价于“organization:*”或者“organization:*:*”。但是如“*:view”不能匹配“system:user:view”，需要使用“*:*:view”，即后缀匹配必须指定前缀（多个冒号就需要多个*来匹配） 授权流程 首先调用Subject.isPermitted/hasRole接口，其会委托给SecurityManager，而SecurityManager接着会委托给Authorizer； Authorizer是真正的授权者，如果我们调用如isPermitted(“user:view”)，其首先会通过PermissionResolver把字符串转换成相应的Permission实例； 在进行授权之前，其会调用相应的Realm获取Subject相应的角色/权限用于匹配传入的角色/权限； Authorizer会判断Realm的角色/权限是否和传入的匹配，如果有多个Realm，会委托给ModularRealmAuthorizer进行循环判断，如果匹配如isPermitted/hasRole会返回true，否则返回false表示授权失败。 ModularRealmAuthorizer进行多Realm匹配流程： 首先检查相应的Realm是否实现了实现了Authorizer； 如果实现了Authorizer，那么接着调用其相应的isPermitted/hasRole接口进行匹配； 如果有一个Realm匹配那么将返回true，否则返回false。 如果Realm进行授权的话，应该继承AuthorizingRealm，其流程是： 如果调用hasRole*，则直接获取AuthorizationInfo.getRoles()与传入的角色比较即可；如果调用如isPermitted(“user:view”)，首先通过PermissionResolver将权限字符串转换成相应的Permission实例，默认使用WildcardPermissionResolver，即转换为通配符的WildcardPermission； 通过AuthorizationInfo.getObjectPermissions()得到Permission实例集合；通过AuthorizationInfo.getStringPermissions()得到字符串集合并通过PermissionResolver解析为Permission实例；然后获取用户的角色，并通过RolePermissionResolver解析角色对应的权限集合（默认没有实现，可以自己提供）； 接着调用Permission.implies(Permissionp)逐个与传入的权限比较，如果有匹配的则返回true，否则false。 Authorizer、PermissionResolver及RolePermissionResolver PermissionResolver接口的意义在于告诉Shiro根据字符串的表现形式（表现特征），采用什么样的Permission进行匹配。Permission是用户自定义的 RolePermissionResolver因为角色是一组权限的集合，所以我们指定角色解析器的时候，须要告诉Shiro使用什么Permission 。 例子参考 http://blog.csdn.net/lw_power/article/details/52562754]]></content>
      <categories>
        <category>java框架</category>
        <category>shiro</category>
      </categories>
      <tags>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java多线程原子操作]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[#Atomic#java.util.concurrent 是基于Queue 的并发包，而Queue，很多情况下使用到了Atomic 操作。通常情况下，在Java里面在没有额外资源可以利用的情况下，只能使用加锁才能保证读-改-写这三个操作时“原子”的。则需要使用synchronized和volatile关键字，代价很高，还需要jni。 JDK 5.0后，则重复使用现代cpu特性降低锁消耗 ##java.util.concurrent.atomic.AtomicInteger## ###API### int addAndGet(int delta) //以原子方式将给定值与当前值相加。实际上就是等于线程安全版本的i=i+delta操作。 boolean compareAndSet(int expect, int update)//如果当前值== 预期值，则以原子方式将该值设置为给定的更新值。如果成功就返回true，否则返回false，并且不修改原值。 int decrementAndGet()//以原子方式将当前值减1。相当于线程安全版本的–i 操作。 int get()//获取当前值。 int getAndAdd(int delta)//以原子方式将给定值与当前值相加。相当于线程安全版本的t=i;i+=delta;return t;操作。 int getAndDecrement()以原子方式将当前值减1。相当于线程安全版本的i–操作。 int getAndIncrement()//以原子方式将当前值加1。相当于线程安全版本的i++操作。 int getAndSet(int newValue)//以原子方式设置为给定值， 并返回旧值。相当于线程安全版本的t=i;i=newValue;return t;操作。 int incrementAndGet()//以原子方式将当前值加1。相当于线程安全版本的++i 操作。 …AtomicInteger 和AtomicLong、AtomicBoolean、AtomicReference 差不多 ##AtomicIntegerArray/AtomicLongArray/AtomicReferenceArray## ###API###API和AtomicInteger 是类似的，这种通过方法、参数的名称就能够得到函数意义的写法是非常值得称赞 int get(int i)//获取位置i 的当前值。很显然， 由于这个是数组操作， 就有索引越界的问题 void set(int i, int newValue) void lazySet(int i, int newValue) int getAndSet(int i, int newValue) boolean compareAndSet(int i, int expect, int update) boolean weakCompareAndSet(int i, int expect, int update) int getAndIncrement(int i) int getAndDecrement(int i) int getAndAdd(int i, int delta) int incrementAndGet(int i) int decrementAndGet(int i) int addAndGet(int i, int delta) ##字段原子更新##AtomicIntegerFieldUpdaterAtomicLongFieldUpdaterAtomicReferenceFieldUpdater&lt;T,V&gt;是基于反射的原子更新字段的值。 约束： 字段是volatile类型 必须可以访问，同普通的反射 只能是实例变量 public class AtomicIntegerFieldUpdaterDemo { class DemoData{ public volatile int value1 = 1; } void main() { DemoData data = new DemoData(); AtomicIntegerFieldUpdater.newUpdater(DemoData.class, value1).getAndSet(data, 10)); } } ##指令重排序##Java 语言规范规定了JVM 线程内部维持顺序化语义，也就是说只要程序的最终结果等同于它在严格的顺序化环境下的结果，那么指令的执行顺序就可能与代码的顺序不一致。这个过程通过叫做指令的重排序。 意义: JVM 能够根据处理器的特性（CPU 的多级缓存系统、多核处理器等）适当的重新排序机器指令，使机器指令更符合CPU 的执行特点，最大限度的发挥机器的性能。 Happens-before法则如果动作B 要看到动作A 的执行结果（无论A/B 是否在同一个线程里面执行），那么A/B 就需要满足happens-before 关系。 JMM 动作（Java Memeory Model Action），Java 存储模型动作。一个动作（Action）包括：变量的读写、监视器加锁和释放锁、线程的start()和join() volatilevolatile 相当于synchronized 的弱实现，也就是说volatile 实现了类似synchronized 的语义，却又没有锁机制。 ###volatile语义### Java 存储模型不会对valatile 指令的操作进行重排序：这个保证对volatile 变量的操作时按照指令的出现顺序执行的。 volatile 变量不会被缓存在寄存器中（只有拥有线程可见）或者其他对CPU 不可见的地方，每次总是从主存中读取volatile 变量的结果。也就是说对于volatile 变量的修改，其它线程总是可见的，并且不是使用自己线程栈内部的变量。 尽管volatile 变量的特性不错，但是volatile 并不能保证线程安全的，也就是说volatile 字段的操作不是原子性的，volatile 变量只能保证可见性（一个线程修改后其它线程能够理解看到此变化后的结果），要想保证原子性，目前为止只能加锁！ ##锁机制##缺点： 在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题。 一个线程持有锁会导致其它所有需要此锁的线程挂起。 如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置，引起性能风险。 volatile 是不错的机制，但是volatile 不能保证原子性。因此对于同步最终还是要回到锁机制上来。 独占锁是一种悲观锁，synchronized 就是一种独占锁，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。而另一个更加有效的锁就是乐观锁。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。 CAS 操作上面的乐观锁用到的机制就是CAS，Compare and Swap。CAS 有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A 和内存值V 相同时，将内存值V 修改为B，否则什么都不做。 非阻塞算法（nonblocking algorithms）一个线程的失败或者挂起不应该影响其他线程的失败或挂起的算法。 public final int incrementAndGet() { for (;;) { int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; } } 整体的过程，利用CPU 的CAS 指令，同时借助JNI 来完成Java 的非阻塞算法。]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring整体框架]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Fspring%2Fspring%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[1 spring的框架介绍spring框架是一个分层架构，它包含一系列的功能要素，并被分为大约20个模块，如下图所示 这些模块被总结为以下几个部分： Core Container Core Container(核心容器)包含有Core、Beans、Context和Expression Language模块Core和Beans模块是框架的基础部分，提供IoC(转控制)和依赖注入特性。这里的基础概念是BeanFactory，它提供对Factory模式的经典实现来消除对程序性单例模式的需要，并真正地允许你从程序逻辑中分离出依赖关系和配置 Core模块主要包含Spring框架基本的核心工具类 Beans模块是所有应用都要用到的，它包含访问配置文件、创建和管理bean以及进行Inversion of Control/Dependency Injection(Ioc/DI)操作相关的所有类 Context模块构建于Core和Beans模块基础之上，提供了一种类似于JNDI注册器的框架式的对象访问方法。Context模块继承了Beans的特性，为Spring核心提供了大量扩展，添加了对国际化(如资源绑定)、事件传播、资源加载和对Context的透明创建的支持。ApplicationContext接口是Context模块的关键 Expression Language模块提供了一个强大的表达式语言用于在运行时查询和操纵对象，该语言支持设置/获取属性的值，属性的分配，方法的调用，访问数组上下文、容器和索引器、逻辑和算术运算符、命名变量以及从Spring的IoC容器中根据名称检索对象 Data Access/Integration JDBC模块提供了一个JDBC抽象层，它可以消除冗长的JDBC编码和解析数据库厂商特有的错误代码，这个模块包含了Spring对JDBC数据访问进行封装的所有类 ORM模块为流行的对象-关系映射API，如JPA、JDO、Hibernate、iBatis等，提供了一个交互层，利用ORM封装包，可以混合使用所有Spring提供的特性进行O/R映射，如前边提到的简单声明性事务管理 Web Web上下文模块建立在应用程序上下文模块之上，为基于Web的应用程序提供了上下文，所以Spring框架支持与Jakarta Struts的集成。Web模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。Web层包含了Web、Web-Servlet、Web-Struts和Web、Porlet模块 Web模块：提供了基础的面向Web的集成特性，例如，多文件上传、使用Servlet listeners初始化IoC容器以及一个面向Web的应用上下文，它还包含了Spring远程支持中Web的相关部分 Web-Servlet模块web.servlet.jar：该模块包含Spring的model-view-controller(MVC)实现，Spring的MVC框架使得模型范围内的代码和web forms之间能够清楚地分离开来，并与Spring框架的其他特性基础在一起 Web-Struts模块：该模块提供了对Struts的支持，使得类在Spring应用中能够与一个典型的Struts Web层集成在一起 Web-Porlet模块：提供了用于Portlet环境和Web-Servlet模块的MVC的实现 AOP AOP模块提供了一个符合AOP联盟标准的面向切面编程的实现，它让你可以定义例如方法拦截器和切点，从而将逻辑代码分开，降低它们之间的耦合性，利用source-level的元数据功能，还可以将各种行为信息合并到你的代码中 Spring AOP模块为基于Spring的应用程序中的对象提供了事务管理服务，通过使用Spring AOP，不用依赖EJB组件，就可以将声明性事务管理集成到应用程序中 Test Test模块支持使用Junit和TestNG对Spring组件进行测试 2 容器的基本实现2.1 容器示例1234567891011121314151617181920212223242526package superfan.springLearn.bean;import java.util.List;public class Person &#123; private String name; private int age; private String sex; private List&lt;Person&gt; family; public Person(String name, int age) &#123; super(); this.name = name; this.age = age; &#125; public void init() &#123; System.out.println("init"); &#125; public void destory() &#123; System.out.println("destory"); &#125; //getter and setter&#125; XML配置 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd"&gt; &lt;bean id="me" class="superfan.springLearn.bean.Person" scope="singleton" depends-on="date" lazy-init="false" init-method="init" destroy-method="destory"&gt; &lt;constructor-arg name="name" value="wang" /&gt; &lt;constructor-arg name="age" type="int" value="11"/&gt; &lt;property name="family"&gt; &lt;list&gt; &lt;ref bean="father" /&gt; &lt;ref bean="mother" /&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="date" class="java.util.Date" /&gt; &lt;bean id="father" class="superfan.springLearn.bean.Person"&gt; &lt;constructor-arg name="name" value="ff" /&gt; &lt;constructor-arg name="age" type="int" value="11" /&gt; &lt;/bean&gt; &lt;bean id="mother" class="superfan.springLearn.bean.Person"&gt; &lt;constructor-arg name="name" value="mm" /&gt; &lt;constructor-arg name="age" type="int" value="11" /&gt; &lt;/bean&gt; &lt;/beans&gt; 测试文件 12345678910public class BeanLoadTest &#123; @SuppressWarnings(&#123; "deprecation", "unused","resource" &#125;) @Test public void beanLoad() &#123; BeanFactory beanFactory = new ClassPathXmlApplicationContext("beans.xml"); System.out.println(beanFactory.getBean("me")); &#125;&#125; 2.2 Spring的结构组成beans包的层级结构beans包中的各个源码包的功能如下 src/main/java 用于展现Spring的主要逻辑 src/main/resources 用于存放系统的配置文件 src/test/java 用于对主要逻辑进行单元测试 src/test/resources 用于存放测试用的配置文件 2.3主要接口和类介绍2.3.1 主要的类由于spring功能强大，类和接口数量很多，这里只总结一小部分，是对整个框架的一种简单理解 Context 应用程序上下文，与main交互 BeanFactory Bean的工厂，存储实例的容器，内部是map实现，但注意保存的不是真正的对象，而是一个BeanDefine，及Bean的定义 DefaultListableBeanFactory BeanDefinition Bean的定义，可以理解为将XML文件的内容转换为内存对象，类似class文件转换的Class对象 AbstractBeanDefinition 内部属性为scope，lazyInit，initMethodName这些等，及XML的属性 BeanDefinitionReader 从文件中获取Bean对象，参考ClassLoader，参考tomcat的ClassLoader，支持多种形式 AbstractBeanDefinitionReader BeanDefinitionReader接口的抽象实现 XmlBeanDefinitionReader 读取XML文件的实现 DocumentLoader 由于是dom解析XML，定义的读取类 DefaultDocumentLoader 接口实现，依赖dom包，包括对XML验证的步骤 DefaultBeanDefinitionDocumentReader 该类不实现BeanDefinitionReader接口，从Document（XML）节点进行获取BeanDefinition BeanDefinitionParser BeanDefinition的解析器 BeanDefinitionParserDelegate DefaultBeanDefinitionDocumentReader的成员，解析任务委托给他，包括各种标签 Resource 对资源文件的封装，负责文件的读取，提供文件（可能来自网络）的基本信息。继承自InputStreamSource，提供getInputStream功能 ClassPathResource 获取classpath路径下的文件 ResourceLoader 加载资源类，与Resource 对应 2.3.2 注册大致流程 main函数调用Context获取BeanFactory对象 Context通过ResourceLoader将xml路径转化为Resource对象 Context创建BeanFactory并初始化 Context通过Reader来读取Resource并且注册到BeanFactory中 Reader通过DocumentLoader 获取xml文件的dom结构 DocumentReader通过dom结构获取BeanDefinition DocumentReader将dom结构交给BeanDefinitionParser进行解析各种标签等等，获取BeanDefinition 注册id，别名到BeanFactory 完成 2.4 核心类介绍2.4.1 DefaultListableBeanFactory 各个类和接口的作用： AliasRegistry：定义对alias的简单增删改等操作 SimpleAliasRegistry：主要使用map作为alias的缓存，并对接口AliasRegistry进行实现 SingletonBeanRegistry：定义对单例的注册及获取 BeanFactory：定义获取bean及bean的各种属性 DefaultSingletonBeanRegistry：对接口SingletonBeanRegistry各函数的实现 HierarchicalBeanFactory：继承BeanFactory，也就是在BeanFactory定义的功能的基础上增加了对parentFactory的支持 BeanDefinitionRegistry：定义对BeanDefinition的各种增删改操作 FactoryBeanRegistrySupport：在DefaultSingletonBeanRegistry基础上增加了对FactoryBean的特殊处理功能 ConfigurableBeanFactory：提供配置Factory的各种方法 ListableBeanFactory：根据各种条件获取bean的配置清单 AbstractBeanFactory：综合FactoryBeanRegistrySupport和ConfigurationBeanFactory的功能 AutowireCapableBeanFactory：提供创建bean、自动注入、初始化以及应用bean的后处理器 AbstractAutowireCapableBeanFactory：综合AbstractBeanFactory并对接口AutowireCapableBeanFactory进行实现 ConfigurableListableBeanFactory：BeanFactory配置清单，指定忽略类型及接口等 DefaultListableBeanFactory：综合上面所有功能，主要是对Bean注册后的处理 2.4.2 XmlBeanDefinitionReaderXML配置文件的读取是Spring中重要的功能，因为Spring的大部分功能都是以配置作为切入点的。 各个类的功能： ResourceLoader：定义资源加载器，主要应用于根据给定的资源文件地址返回对应的Resource BeanDefinitionReader：主要定义资源文件读取并转换为BeanDefinition的各个功能 EnvironmentCapable：定义获取Environment方法 DocumentLoader：定义从资源文件加载到转换为Document的功能 AbstractBeanDefinitionReader：对EnvironmentCapable、BeanDefinitionReader类定义的功能进行实现 BeanDefinitionDocumentReader：定义读取Document并注册BeanDefinition功能 BeanDefinitionParserDelegate：定义解析Element的各种方法 整个XML配置文件读取的大致流程，在XmlBeanDefinitionReader中主要包含以下几步处理 通过继承自AbstractBeanDefinitionReader中的方法，来使用ResourceLoader将资源文件路径转换为对应的Resource文件 通过DocumentLoader对Resource文件进行转换，将Resource文件转换为Document文件 通过实现接口BeanDefinitionDocumentReader的DefaultBeanDefinitionDocumentReader类对Document进行解析，并使用BeanDefinitionParserDelegate对Element进行解析 2.4.3 配置文件封装Spring的配置文件读取是通过ClassPathResource进行封装的，Spring对其内部使用到的资源实现了自己的抽象结构：Resource接口来封装底层资源 123456789101112131415161718192021222324public interface InputStreamSource &#123; InputStream getInputStream() throws IOException;&#125;public interface Resource extends InputStreamSource &#123; boolean exists(); boolean isReadable(); boolean isOpen(); URL getURL() throws IOException; URI getURI() throws IOException; File getFile() throws IOException; long lastModified() throws IOException; Resource createRelative(String var1) throws IOException; String getFilename(); String getDescription();&#125; InputStreamSource封装任何能返回InputStream的类，比如File、Classpath下的资源和Byte Array等， 它只有一个方法定义：getInputStream()，该方法返回一个新的InputStream对象 Resource接口抽象了所有Spring内部使用到的底层资源：File、URL、Classpath等。首先，它定义了3个判断当前资源状态的方法：存在性(exists)、可读性(isReadable)、是否处于打开状态(isOpen)。另外，Resource接口还提供了不同资源到URL、URI、File类型的转换，以及获取lastModified属性、文件名(不带路径信息的文件名，getFilename())的方法，为了便于操作，Resource还提供了基于当前资源创建一个相对资源的方法：createRelative()，还提供了getDescription()方法用于在错误处理中的打印信息 对不同来源的资源文件都有相应的Resource实现：文件(FileSystemResource)、Classpath资源(ClassPathResource)、URL资源(UrlResource)、InputStream资源(InputStreamResource)、Byte数组(ByteArrayResource)等，相关类图如下所示 2.5 bean注册源码追踪程序入口，以Context入口，常用 ClassPathXmlApplication 通过setConfigLocation将配置文件路径保存到本地，再通过调用refresh进行初始化。因为该类支持刷新，看它的继承结构。refresh等同于第一次加载 在 521行看到了关键的beanFactory，及DefaultListableBeanFactory 这里的130行简单的创建了个工厂，里面啥都没有，133行才是装载BeanFinition 这里我们看到了XMLReader，至于为什么是XMLReader，看类名是用XML读取，这里注意组合关系，BeanFactory不负责获取BeanDefinition，是由Context中让Reader去获取的，再填入BeanFactory Reader从循环各个Locations里获取beans 222行resourceLoader加载resource Xml用dom解析，获取dom树 解析为dom，包括了校验xml文件是否符合规范的步骤 通过DocumentReader来解析 148行委托给BeanDefinitionParserDelegate来解析 这里的175和178行分别对应解析spring标签和用户自定义标签 解析4个子标签import，alias，bean，。。。 305行获取BeanDefinition的封装类bdHolder 310行将获取到的BeanDefinition的封装类bdHolder向BeanFactory里注册，包括id，alias等 437行前获取id和alias 437获取BeanDefinition 最后封装为BeanDefinitionHolder，并返回 解析其他属性，如class，构造函数等 解析其他属性，如layz-init，init等等属性 310行向BeanFactory注册 注册BeanName和Alias 2.5 bean获取源码追踪]]></content>
      <categories>
        <category>java框架</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat Connection]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2FConnection%2F</url>
    <content type="text"><![CDATA[2 连接器Connect(连接器)负责接收外部连接请求，创建Request、Response对象用于请求的数据交换，并分配线程让Container处理该请求。 Connector可以支持多种协议的请求（如HTTP，AJP等）的请求，对Connector的配置在conf/server.xml中默认为： 12&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;&lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; 即在tomcat中支持两种协议的连接器：HTTP/1.1与AJP/1.3 2.1 关键属性和方法其中，ProtocalHandler是协议处理器接口，不同的协议各自实现。Service是由一个或多个Connector共享一个Container组成，对外部请求提供服务，service主要为了关联Connector和Container。关于Service，请移步。 Adapter是基于coyote的servlet容器的入口。 在Connector的代码中： 123456/** * Coyote Protocol handler class name. * Defaults to the Coyote HTTP/1.1 protocolHandler. */protected String protocolHandlerClassName = "org.apache.coyote.http11.Http11Protocol"; 可以看到，默认采用http1.1协议 2.1.1 构造方法123456789101112131415public Connector() &#123; this(null);&#125;public Connector(String protocol) &#123; setProtocol(protocol); // Instantiate protocol handler try &#123; Class&lt;?&gt; clazz = Class.forName(protocolHandlerClassName); // 实例化协议处理器 this.protocolHandler = (ProtocolHandler) clazz.newInstance(); &#125; catch (Exception e) &#123; log.error(sm.getString( "coyoteConnector.protocolHandlerInstantiationFailed"), e); &#125;&#125; 上述代码中根据协议名称构造Connector，继续进入setProtocol 123456789101112131415161718192021222324252627public void setProtocol(String protocol) &#123; if (AprLifecycleListener.isAprAvailable()) &#123; //这里条件不会成立 if ("HTTP/1.1".equals(protocol)) &#123; setProtocolHandlerClassName ("org.apache.coyote.http11.Http11AprProtocol"); &#125; else if ("AJP/1.3".equals(protocol)) &#123; setProtocolHandlerClassName ("org.apache.coyote.ajp.AjpAprProtocol"); &#125; else if (protocol != null) &#123; setProtocolHandlerClassName(protocol); &#125; else &#123; setProtocolHandlerClassName ("org.apache.coyote.http11.Http11AprProtocol"); &#125; &#125; else &#123; if ("HTTP/1.1".equals(protocol)) &#123; setProtocolHandlerClassName ("org.apache.coyote.http11.Http11Protocol"); &#125; else if ("AJP/1.3".equals(protocol)) &#123; setProtocolHandlerClassName ("org.apache.coyote.ajp.AjpProtocol"); &#125; else if (protocol != null) &#123; setProtocolHandlerClassName(protocol); &#125; &#125;&#125; 2.1.2 工作流程2.1.2.1 初始化和启动时序图： Tomcat初始化时会调用Bootstrap的Load方法，会调用Connector的构造方法 接着调用server.init(),调用链会走到Connector的initInternal()方法 123456789protected void initInternal() throws LifecycleException &#123; super.initInternal(); adapter = new CoyoteAdapter(this);//connector作为参数传入,设置到CoyoteAdapter的属性中 protocolHandler.setAdapter(adapter);//adapter设置到protocolHandler协议处理器的属性中 //……忽略非核心代码 protocolHandler.init(); //AbstractHttp11JsseProtocol.init()生成JSSEImplementation实例,接着调用AbstractProtocol.init()方法, 进而进入AbstractEndpoint.init()，再调用JIoEndpoint.bind()**核心方法，下面细说**&#125; JIoEndpoint.bind()中，会设置Acceptor线程数为1，设置最大连接数(default 200)，设置最大连接数的时候会初始化connectionLimitLatch（用于控制Connector的并发连接数，其值即为最大连接数），并初始化serverSocketFactory创建serverSocket。 Acceptor线程在start的时候细说。 12345protected void startInternal() throws LifecycleException &#123; //……忽略非核心代码 protocolHandler.start();//调用AbstractProtocol.start()进而进入AbstractEndpoint.start()在调用JIoEndpoint.startInternal()方法核心，见下文 &#125; 接着调用server.start(),调用链会走到Connector的startInternal()方法 JIoEndpoint.startInternal()所做的事情： 创建工作线程池: 其参数如下：corePoolSize=10，maximumPoolSize=200，keepAliveTime=60s 如果在init阶段未初始化ConnectionLatch，则此时会进行初始化 启动Acceptor线程(启动的个数在init中初始化了，default 1)监听的serverSocket上的请求 启动一个异步timeout线程 再附一张 JIoEndpoint.startInternal()的时序图 2.1.3 请求处理前文中说到Acceptor线程会监听Socket请求并转发给工作线程池，处理请求的即JIoEndpoint的内部类SocketProcessor。 SocketProcessor根据socket的状态进行第一层处理，另外SSL的握手也是由它负责，在处理时又调用handler.process(socket, SocketStatus.OPEN_READ);，Hanlder接口是每个具体的Endpoint的内部接口，一般由对应Protocol的一个Handler内部类实现，比如JIoEndponit的handler对应的就是Http11Protocol的Http11ConnectionHandler。 查看handler的调用链，会到adapter.service(request, response);，CoyoteAdapter完成http request的解析和分派，进而调用connector.getService().getContainer().getPipeline().getFirst().invoke(request, response);，这样，Connector就把请求传递到了Container，也就是Engine对应Pipeline的第一个Valve的invoke方法。 CoyoteAdapter对象负责将http request解析成HttpServletRequest对象，之后绑定相应的容器，然后从engine开始逐层调用valve直至该servlet。Http11Protocol 类完全路径org.apache.coyote.http11.Http11Protocol，这是支持HTTP的BIO实现。Http11Protocol包含了JIoEndpoint对象及Http11ConnectionHandler对象，她维护了一个Http11Processor对象池，Http11Processor对象会调用CoyoteAdapter完成HTTP Request的解析和分派。 Tomcat的Connector组件工作具体序列图： 2.2 协议处理器Connector支持多种协议的请求，必然需要进行协议的解析等处理，ProtocolHandler类层次结构为： 从类图中看到，协议上有三种不同的实现方式：NIO、APR、NIO2。 在Connector的代码中： 123456/** * Coyote Protocol handler class name. * Defaults to the Coyote HTTP/1.1 protocolHandler. */protected String protocolHandlerClassName = "org.apache.coyote.http11.Http11Protocol"; 协议处理器可以看为将endpoint和Processor关联起来，不同的协议对应不同的Processor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public abstract class AbstractProtocol&lt;S&gt; implements ProtocolHandler, MBeanRegistration &#123; private final AbstractEndpoint&lt;S&gt; endpoint; private final Set&lt;Processor&gt; waitingProcessors = Collections.newSetFromMap(new ConcurrentHashMap&lt;Processor, Boolean&gt;()); @Override public void init() throws Exception &#123; // 忽略其他 endpoint.init(); &#125; @Override public void start() throws Exception &#123; // 忽略其他 endpoint.start(); // Start async timeout thread asyncTimeout = new AsyncTimeout(); //处理超时的异步process对象 Thread timeoutThread = new Thread(asyncTimeout, getNameInternal() + "-AsyncTimeout"); timeoutThread.setDaemon(true); timeoutThread.start(); &#125; @Override public void pause() throws Exception &#123; // 忽略其他 endpoint.pause(); &#125; @Override public void resume() throws Exception &#123; // 忽略其他 endpoint.resume(); &#125; @Override public void stop() throws Exception &#123; // 忽略其他 if (asyncTimeout != null) &#123; asyncTimeout.stop(); &#125; endpoint.stop(); &#125; @Override public void destroy() &#123; // 忽略其他 endpoint.destroy(); &#125; // ------------------------------------------- Connection handler base class /** * ConnectionHandler 处理连接请求，提供Processors池化缓存，并在socket连接时与Processor绑定 * */ protected static class ConnectionHandler&lt;S&gt; implements AbstractEndpoint.Handler&lt;S&gt; &#123; private final Map&lt;S,Processor&gt; connections = new ConcurrentHashMap&lt;&gt;(); // 默认S 是 Socket private final RecycledProcessors recycledProcessors = new RecycledProcessors(this);//RecycledProcessors是一个栈缓存 @Override public SocketState process(SocketWrapperBase&lt;S&gt; wrapper, SocketEvent status) &#123; S socket = wrapper.getSocket(); Processor processor = connections.get(socket); try &#123; if (processor == null) &#123; processor = recycledProcessors.pop(); // 缓存中取出 &#125; if (processor == null) &#123; processor = getProtocol().createProcessor(); register(processor); &#125; // 绑定processor with the connection connections.put(socket, processor); SocketState state = SocketState.CLOSED; do &#123; state = processor.process(wrapper, status); release(processor); &#125; while ( state == SocketState.UPGRADING); if (state == SocketState...)&#123; &#125; else &#123; // Connection closed. OK to recycle the processor. Upgrade // processors are not recycled. connections.remove(socket); &#125; return state; &#125; // Make sure socket/processor is removed from the list of current // connections connections.remove(socket); release(processor); return SocketState.CLOSED; &#125; private void release(Processor processor) &#123; if (processor != null) &#123; processor.recycle(); if (!processor.isUpgrade()) &#123; recycledProcessors.push(processor); &#125; &#125; &#125; public void release(SocketWrapperBase&lt;S&gt; socketWrapper) &#123; S socket = socketWrapper.getSocket(); Processor processor = connections.remove(socket); release(processor); &#125; public final void pause() &#123; for (Processor processor : connections.values()) &#123; processor.pause(); &#125; &#125; &#125; &#125; 它的子类只是实现了不同协议对endpoint的设置，以及不同的Processor对象 2.2.1 endpoint 1234private XXX serverSock = null; //取决于实现时用的ServerSocket类别protected Acceptor[] acceptors; // 处理接受的请求protected SynchronizedStack&lt;SocketProcessorBase&lt;S&gt;&gt; processorCache;//处理器private Executor executor; //执行SocketProcessorBase的run 2.3 Adapter如果把整个tomcat内核最高抽象程度模块化，可以看成是由连接器Connector和容器Container组成，连接器负责HTTP请求接收及响应，生成请求对象及响应对象并交由容器处理，而容器则根据请求路径找到相应的servlet进行处理。请求响应对象从连接器传送到容器需要一个桥梁——CoyoteAdapter。 123456789public interface Adapter &#123; public void service(Request req, Response res) throws Exception; public boolean prepare(Request req, Response res) throws Exception; public boolean asyncDispatch(Request req,Response res, SocketEvent status) throws Exception; public void log(Request req, Response res, long time); public void checkRecycled(Request req, Response res); public String getDomain();&#125; 只有一个实现类 CoyoteAdapter 在service中，解析了Request后调用了connector.getService().getContainer().getPipeline().getFirst().invoke(request, response);CoyoteAdapter是将org.apache.coyote.Request适配成为org.apache.catalina.connector.Request 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class CoyoteAdapter implements Adapter&#123; public CoyoteAdapter(Connector connector) &#123; super(); this.connector = connector; &#125; @Override public void service(org.apache.coyote.Request req, org.apache.coyote.Response res) throws Exception &#123; Request request = (Request) req.getNote(ADAPTER_NOTES); Response response = (Response) res.getNote(ADAPTER_NOTES); if (request == null) &#123; // Create objects request = connector.createRequest(); request.setCoyoteRequest(req); response = connector.createResponse(); response.setCoyoteResponse(res); // Link objects request.setResponse(response); response.setRequest(request); // Set as notes req.setNote(ADAPTER_NOTES, request); res.setNote(ADAPTER_NOTES, response); // Set query string encoding req.getParameters().setQueryStringCharset(connector.getURICharset()); &#125; boolean async = false; boolean postParseSuccess = false; req.getRequestProcessor().setWorkerThreadName(THREAD_NAME.get()); try &#123; // Parse and set Catalina and configuration specific request parameters // 在解析HTTP报头之后执行必要的处理，以便将请求/响应对传递到容器管道的起始处理。 postParseSuccess = postParseRequest(req, request, res, response); if (postParseSuccess) &#123; //check valves if we support async request.setAsyncSupported( connector.getService().getContainer().getPipeline().isAsyncSupported()); // Calling the container connector.getService().getContainer().getPipeline().getFirst().invoke( request, response); &#125; if (request.isAsync()) &#123; //忽略非核心 &#125; else &#123; request.finishRequest(); response.finishResponse(); &#125; &#125; catch (IOException e) &#123; // Ignore &#125; finally &#123; //忽略非核心 req.getRequestProcessor().setWorkerThreadName(null); // Recycle the wrapper request and response if (!async) &#123; request.recycle(); response.recycle(); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat 安全性]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2F%E5%AE%89%E5%85%A8%E6%80%A7%2F</url>
    <content type="text"><![CDATA[10 安全性web应用程序的一些内容是受限的，tomcat通过配置web.xml来对这些内容进行访问控制。 servlet通过一个名为验证器的阀支持安全限制的。当servlet容器启动时，验证器阀会添加到Context容器的管道中。调用Wrapper阀之前会先调用验证器阀。验证器阀会调用Context的authenticate方法，Realm保存有效的用户名密码资源 10.1 领域用来对用户进行身份验证的组件。与Context是一对一关系，他保存了所有的有效的用户名和密码对 通过authenticate来进行进行认证 通过public boolean hasRole(Wrapper wrapper, Principal principal, String role);判断角色 CredentialHandler，This interface is used by the {@link Realm} to compare the user provided credentials with the credentials stored in the {@link Realm} for that user. 支持Realm的多种实现]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat session]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2FSession%2F</url>
    <content type="text"><![CDATA[9 Session管理Catalina通过Session管理器来管理建立的Session对象，该组件由org.apache.catalina.Manager接口提供 Session管理器与一个Context容器关联，且必须与一个Context容器关联。Session管理器负责创建、更新、销毁Session对象 默认情况下，Session对象放在内存中，但是，在Tomcat中，也可以做持久化。 9.1 Session对象在servlet编程方面中，Session对象由javax.servlet.http.HttpSession接口表示。Catalina的Session接口的实现是StandardSession类，但是为了安全起见，这里使用了一个外观类StandardSessionFacade 9.1.1 Session接口session保存了客户端用户请求和Context之间的状态信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public interface Session &#123; public static final String SESSION_CREATED_EVENT = "createSession"; public static final String SESSION_DESTROYED_EVENT = "destroySession"; public static final String SESSION_ACTIVATED_EVENT = "activateSession"; public static final String SESSION_PASSIVATED_EVENT = "passivateSession"; /** 认证类型 */ public String getAuthType(); public void setAuthType(String authType); /** 创建时间 */ public long getCreationTime(); public long getCreationTimeInternal(); public void setCreationTime(long time); /** id */ public String getId(); public String getIdInternal(); /** 设置id并通知监听器新的session创建了 */ public void setId(String id); /** 设置id并，可选通知监听器新的session创建了 */ public void setId(String id, boolean notify); public String getIdInternal(); /** 获取最新的请求访问时间（绑定在该Session），请求完成时间 */ public long getThisAccessedTime(); public long getThisAccessedTimeInternal(); /** 获取最新的请求访问时间（绑定在该Session），请求开始时间 */ public long getLastAccessedTime(); public long getLastAccessedTimeInternal(); /** 获取空闲时间 */ public long getIdleTime(); public long getIdleTimeInternal(); /** Manager */ public Manager getManager(); public void setManager(Manager manager); /** 最大有限时间间隔，单位second */ public int getMaxInactiveInterval(); public void setMaxInactiveInterval(int interval); public void setNew(boolean isNew); /** 认证凭证 */ public Principal getPrincipal(); public void setPrincipal(Principal principal); public HttpSession getSession(); /** 有效性 */ public void setValid(boolean isValid); public boolean isValid(); /** Update the accessed time */ public void access(); public void addSessionListener(SessionListener listener); public void endAccess(); public void expire(); /** 绑定的对象 */ public Object getNote(String name); public Iterator&lt;String&gt; getNoteNames(); public void recycle(); public void removeNote(String name); public void removeSessionListener(SessionListener listener); public void setNote(String name, Object value); public void tellChangedSessionId(String newId, String oldId, boolean notifySessionListeners, boolean notifyContainerListeners); public boolean isAttributeDistributable(String name, Object value);&#125; 上面有四个SessionEvent常量，代表4个事件，用于Session事件监听 123public interface SessionListener extends EventListener &#123; public void sessionEvent(SessionEvent event);&#125; 一堆过期，访问等时间 XXXNote，Note表示代表绑定在Session上的对象 isAttributeDistributable表明还支持分布式 XXXInternal与XXX区别在于XXXInternal不做有效性校验 Session对象存在于SessionManager中，而SessionManager关联在Context中。SessionManager通过getLastAccessedTime判断有效性，通过setValid设置有效性。每访问一个Session时，会调用其access来修改Session的最后访问时间。最后SessionManager会调用expire设置为过期，可以通过getSession获取一个经过Session外观类包装的HttpSession对象 9.1.2 StandardSessionStandardSession是Session接口的实现 1public class StandardSession implements HttpSession, Session, Serializable 支持序列化，以便支持分布式Session和Session在不同JVM的传输 构造器 必须绑定个SessionManager 1public StandardSession(Manager manager) Session属性map 1protected ConcurrentMap&lt;String, Object&gt; attributes = new ConcurrentHashMap&lt;&gt;(); 外观类 1protected transient StandardSessionFacade facade = null; notesMap ，这里与Session属性map有区别，StandardSession实现了HttpSession接口，attributes用于该接口，而notes用于容器和管理器的一些内容 1protected transient Map&lt;String, Object&gt; notes = new Hashtable&lt;&gt;(); getSession返回外观类 12345678910111213141516171819public HttpSession getSession() &#123; if (facade == null)&#123; if (SecurityUtil.isPackageProtectionEnabled())&#123; final StandardSession fsession = this; facade = AccessController.doPrivileged( new PrivilegedAction&lt;StandardSessionFacade&gt;()&#123; @Override public StandardSessionFacade run()&#123; return new StandardSessionFacade(fsession); &#125; &#125;); &#125; else &#123; facade = new StandardSessionFacade(this); &#125; &#125; return (facade);&#125; Session长时间未访问会设为过期，这个时间是maxInactiveInterval决定，为负则永不过期。expire用来设置过期 如果失效则立即返回 设置 expiring = true 通知相应监听器，context和session级别的 SessionManager删除该session 回收，解除所有绑定，并设置 expiring = false和isValid =false 9.1.3 HttpSessionservlet容器使用此接口在HTTP客户端和HTTP服务器之间创建会话。 会话在指定的时间段内持续，跨越来自用户的多个连接或页面请求。 会话通常对应一个用户，谁可以访问一个网站多次。 服务器可以通过许多方式维护会话，例如使用Cookie或重写URL。 1234567891011121314151617181920212223242526272829303132333435public interface HttpSession &#123; public long getCreationTime(); public String getId(); public long getLastAccessedTime(); public ServletContext getServletContext(); public void setMaxInactiveInterval(int interval); public int getMaxInactiveInterval(); @Deprecated public HttpSessionContext getSessionContext(); public Object getAttribute(String name); @Deprecated public Object getValue(String name); public Enumeration&lt;String&gt; getAttributeNames(); @Deprecated public String[] getValueNames(); public void setAttribute(String name, Object value); @Deprecated public void putValue(String name, Object value); public void removeAttribute(String name); @Deprecated public void removeValue(String name); /** * 设置失效 */ public void invalidate(); /** * Returns &lt;code&gt;true&lt;/code&gt; if the client does not yet know about the * session or if the client chooses not to join the session. For example, if * the server used only cookie-based sessions, and the client had disabled * the use of cookies, then a session would be new on each request. */ public boolean isNew();&#125; 注意@Deprecated的方法，被XXXAttribute替代 如果不支持session则isNew永远返回true 9.1.4 StandardSessionFacade这个类是个外观类，代理了真正的StandardSession，不提供get方法，防止向下转型StandardSession访问敏感方法，如expire等 1234567public class StandardSessionFacade implements HttpSession &#123; private final HttpSession session; public StandardSessionFacade(HttpSession session) &#123; this.session = session; &#125; //...........&#125; 9.2 Manager用来管理Session实例 提供了关联Context的方法 Session的创建删除查找 设置Session的过期时间，最大活跃数等属性 load，upload持久化方法 ManagerBase提供了常用功能的实现，有两个子类，是StandardManager和PersistentManagerBase类 当Catalina运行是StandardManager将Session保存在内存中，关闭时存储到文件中，当再次启动时，将载入这些Session对象 PersistentManagerBase支持其他持久化方式的基类 9.2.1 ManagerBase123456789101112131415161718public abstract class ManagerBase extends LifecycleMBeanBase implements Manager&#123; private Context context; protected SessionIdGenerator sessionIdGenerator = null; protected volatile int sessionMaxAliveTime; protected final Deque&lt;SessionTiming&gt; sessionCreationTiming = new LinkedList&lt;&gt;(); protected final Deque&lt;SessionTiming&gt; sessionExpirationTiming = new LinkedList&lt;&gt;(); protected final AtomicLong expiredSessions = new AtomicLong(0); protected int maxActiveSessions = -1; protected volatile int maxActive=0; protected long sessionCounter=0; private Pattern sessionAttributeNamePattern; private Pattern sessionAttributeValueClassNamePattern; private boolean warnOnSessionAttributeFilterFailure; protected Map&lt;String, Session&gt; sessions = new ConcurrentHashMap&lt;&gt;();&#125; ManagerBase继承自LifecycleMBeanBase，也是tomcat的组件 一些Manager需要的状态量 sessionAttributeValueClassNamePattern等用作Session对属性的过滤器 sessions保存Session对象 sessionCreationTiming和sessionExpirationTiming还不懂 9.2.2.1 Tomcat的Session过期处理策略tomcat容器实现类都继承了ContainerBase类,容器在启动的时候都会调用ContainerBase类的threadStart()方法,threadStart()方法如下: 12345678910111213protected void threadStart() &#123; if (thread != null) return; if (backgroundProcessorDelay &lt;= 0) return; threadDone = false; String threadName = "ContainerBackgroundProcessor[" + toString() + "]"; thread = new Thread(new ContainerBackgroundProcessor(), threadName); thread.setDaemon(true); thread.start(); &#125; StandardHost,StandardContext,StandardWrapper类的backgroundProcessorDelay的值都为-1,所以ContainerBackgroundProcessor线程都没有启动.而StandardEngine类在创建的时候 将backgroundProcessorDelay 的值设置为10.所以ContainerBackgroundProcessor线程在StandardEngine里启动,ContainerBackgroundProcessor线程代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142protected class ContainerBackgroundProcessor implements Runnable &#123; public void run() &#123; while (!threadDone) &#123; try &#123; Thread.sleep(backgroundProcessorDelay * 1000L); &#125; catch (InterruptedException e) &#123; ; &#125; if (!threadDone) &#123; Container parent = (Container) getMappingObject();//得到StandardEngine ClassLoader cl = Thread.currentThread().getContextClassLoader(); if (parent.getLoader() != null) &#123; cl = parent.getLoader().getClassLoader(); &#125; processChildren(parent, cl); &#125; &#125; &#125; protected void processChildren(Container container, ClassLoader cl) &#123; try &#123; if (container.getLoader() != null) &#123; Thread.currentThread().setContextClassLoader (container.getLoader().getClassLoader()); &#125; container.backgroundProcess(); &#125; catch (Throwable t) &#123; log.error("Exception invoking periodic operation: ", t); &#125; finally &#123; Thread.currentThread().setContextClassLoader(cl); &#125; Container[] children = container.findChildren(); for (int i = 0; i &lt; children.length; i++) &#123; if (children[i].getBackgroundProcessorDelay() &lt;= 0) &#123; processChildren(children[i], cl); &#125; &#125; &#125; &#125; ContainerBackgroundProcessor每隔10秒执行一下线程,线程递归调用StandardEngine,StandardHost,StandardContext,StandardWrapper类的backgroundProcess()方法(backgroundProcess()方法在ContainerBase类中) backgroundProcess()方法如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void backgroundProcess() &#123; if (!started) return; if (cluster != null) &#123; try &#123; cluster.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.cluster", cluster), e); &#125; &#125; if (loader != null) &#123; try &#123; loader.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.loader", loader), e); &#125; &#125; if (manager != null) &#123;//manager的实现类StandardManager try &#123; manager.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.manager", manager), e); &#125; &#125; if (realm != null) &#123; try &#123; realm.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.realm", realm), e); &#125; &#125; Valve current = pipeline.getFirst(); while (current != null) &#123; try &#123; current.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.valve", current), e); &#125; current = current.getNext(); &#125; lifecycle.fireLifecycleEvent(Lifecycle.PERIODIC_EVENT, null); &#125; 在调用StandContext类的backgroundProcess()方法时, 会调用StandardManager的backgroundProcess()方法,该方法如下: 12345public void backgroundProcess() &#123; count = (count + 1) % processExpiresFrequency; if (count == 0) processExpires(); &#125; processExpiresFrequency的默认值为6,所以每6*10秒会调一次processExpires()方法,processExpires()方法如下: 12345678910111213141516171819public void processExpires() &#123; long timeNow = System.currentTimeMillis(); Session sessions[] = findSessions(); int expireHere = 0 ; if(log.isDebugEnabled()) log.debug("Start expire sessions " + getName() + " at " + timeNow + " sessioncount " + sessions.length); for (int i = 0; i &lt; sessions.length; i++) &#123; if (sessions[i]!=null &amp;&amp; !sessions[i].isValid()) &#123;//判断session是否有效,无效则清除 expireHere++; &#125; &#125; long timeEnd = System.currentTimeMillis(); if(log.isDebugEnabled()) log.debug("End expire sessions " + getName() + " processingTime " + (timeEnd - timeNow) + " expired sessions: " + expireHere); processingTime += ( timeEnd - timeNow ); &#125; 该方法主要判断session是否过期,过期则清除. 整个过程看起来有点复杂,简单地来说,就是tomcat服务器在启动的时候初始化了一个守护线程,定期6*10秒去检查有没有Session过期.过期则清除. 9.2.3 StandardManagerStandardManager主要提供了持久化功能，在canalina启动或关闭时，会调用它的stop和start方法（继承至LifecycleMBeanBase），关闭时会在work目录下生成个SESSION.ser文件 1234public class StandardManager extends ManagerBase &#123; protected String pathname = "SESSIONS.ser"; &#125; 主要定义了load，unload， stopInternal，startInternal方法 9.2.4 PersistentManagerBase是所有持久化Session管理器的父类 123456public abstract class PersistentManagerBase extends ManagerBase implements StoreManager&#123; /** * Store object which will manage the Session store. */ protected Store store = null;&#125; 对Session的增删改查都是通过Story接口 9.2.4.1 StoreManager接口1234public interface StoreManager extends DistributedManager &#123; Store getStore(); void removeSuper(Session session);&#125; 9.2.4.2 Store接口 12345678910111213public interface Store &#123; public Manager getManager(); public void setManager(Manager manager); public int getSize() throws IOException; public void addPropertyChangeListener(PropertyChangeListener listener); public String[] keys() throws IOException; public Session load(String id) throws ClassNotFoundException, IOException; public void remove(String id) throws IOException; public void clear() throws IOException; public void removePropertyChangeListener(PropertyChangeListener listener); public void save(Session session) throws IOException;&#125; StoreBase 子类StoreBase也是组件，通过processExpires处理过期数据 1public abstract class StoreBase extends LifecycleBase implements Store FileStore FileStore是文件处理，下面是save和remove操作，一个id对应一个文件 1234567891011121314151617181920212223public void save(Session session) throws IOException &#123; // Open an output stream to the specified pathname, if any File file = file(session.getIdInternal()); if (file == null) &#123; return; &#125; if (manager.getContext().getLogger().isDebugEnabled()) &#123; manager.getContext().getLogger().debug(sm.getString(getStoreName() + ".saving", session.getIdInternal(), file.getAbsolutePath())); &#125; try (FileOutputStream fos = new FileOutputStream(file.getAbsolutePath()); ObjectOutputStream oos = new ObjectOutputStream(new BufferedOutputStream(fos))) &#123; ((StandardSession)session).writeObjectData(oos); &#125;&#125;public void remove(String id) throws IOException &#123; File file = file(id); if (file == null) &#123; return; &#125; file.delete();&#125; JdbcStore 通过jdbc的方式，以remove为例 123456789101112131415161718192021222324252627282930313233public void remove(String id) throws IOException &#123; synchronized (this) &#123; int numberOfTries = 2; while (numberOfTries &gt; 0) &#123; Connection _conn = getConnection(); if (_conn == null) &#123; return; &#125; try &#123; remove(id, _conn); // Break out after the finally block numberOfTries = 0; &#125; catch (SQLException e) &#123; if (dbConnection != null) close(dbConnection); &#125; finally &#123; release(_conn); &#125; numberOfTries--; &#125; &#125;&#125;private void remove(String id, Connection _conn) throws SQLException &#123; if (preparedRemoveSql == null) &#123; String removeSql = "DELETE FROM " + sessionTable + " WHERE " + sessionIdCol + " = ? AND " + sessionAppCol + " = ?"; preparedRemoveSql = _conn.prepareStatement(removeSql); &#125; preparedRemoveSql.setString(1, id); preparedRemoveSql.setString(2, getName()); preparedRemoveSql.execute();&#125; ​]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat 生命周期]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2F%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[6 生命周期Catalina包含很多组件，当Catalina启动的时候，这些组件会一起启动，关闭一样。通过实现org.apache.catalina.Lifecycle。 6.1 LifecycleCatalina的组件实现该接口，父组件负责启动/关闭子组件。这种模式使子组件均在父组件的监控下，Catalina启动类可以直接启动和关闭所有组件。 几乎所有组件都实现了该方法 6.1.1 Lifecycle的周期循环图大写字母是状态，start()等是触发函数 12345678910111213141516171819202122232425262728 start() ----------------------------- | | | init() |NEW -»-- INITIALIZING || | | | ------------------«-----------------------| | |auto | | || | \|/ start() \|/ \|/ auto auto stop() || | INITIALIZED --»-- STARTING_PREP --»- STARTING --»- STARTED --»--- || | | | || |destroy()| | || --»-----«-- ------------------------«-------------------------------- ^| | | || | \|/ auto auto start() || | STOPPING_PREP ----»---- STOPPING ------»----- STOPPED -----»-----| \|/ ^ | ^| | stop() | | || | -------------------------- | || | | | || | | destroy() destroy() | || | FAILED ----»------ DESTROYING ---«----------------- || | ^ | || | destroy() | |auto || --------»----------------- \|/ || DESTROYED || || stop() |----»-----------------------------»------------------------------ 主要有以下事件： 12345678910111213141516public interface Lifecycle &#123; public static final String BEFORE_INIT_EVENT = "before_init"; public static final String AFTER_INIT_EVENT = "after_init"; public static final String START_EVENT = "start"; public static final String BEFORE_START_EVENT = "before_start"; public static final String AFTER_START_EVENT = "after_start"; public static final String STOP_EVENT = "stop"; public static final String BEFORE_STOP_EVENT = "before_stop"; public static final String AFTER_STOP_EVENT = "after_stop"; public static final String AFTER_DESTROY_EVENT = "after_destroy"; public static final String BEFORE_DESTROY_EVENT = "before_destroy"; public static final String PERIODIC_EVENT = "periodic"; //定期 public static final String CONFIGURE_START_EVENT = "configure_start"; public static final String CONFIGURE_STOP_EVENT = "configure_stop"; //....&#125; 主要有以下事件触发方法： 1234567public interface Lifecycle &#123; public void init() throws LifecycleException; public void start() throws LifecycleException; public void stop() throws LifecycleException; public void destroy() throws LifecycleException; //....&#125; 6.1.2 组件状态状态与Lifecycle的事件常量是关联的，注意区分状态和事件，事件用于监听器。 但我觉得有冗余，后期估计会改 1234public interface Lifecycle &#123; public LifecycleState getState(); public String getStateName();&#125; LifecycleState对应6.1的生命周期图中的各个状态，LifecycleState将Lifecycle中的状态独立到这里，和Lifecycle对比，多了New和FAILED，少了PERIODIC_EVENT，CONFIGURE_START_EVENT，CONFIGURE_STOP_EVENT 1234567891011121314151617181920212223242526272829public enum LifecycleState &#123; NEW(false, null), INITIALIZING(false, Lifecycle.BEFORE_INIT_EVENT), INITIALIZED(false, Lifecycle.AFTER_INIT_EVENT), STARTING_PREP(false, Lifecycle.BEFORE_START_EVENT), STARTING(true, Lifecycle.START_EVENT), STARTED(true, Lifecycle.AFTER_START_EVENT), STOPPING_PREP(true, Lifecycle.BEFORE_STOP_EVENT), STOPPING(false, Lifecycle.STOP_EVENT), STOPPED(false, Lifecycle.AFTER_STOP_EVENT), DESTROYING(false, Lifecycle.BEFORE_DESTROY_EVENT), DESTROYED(false, Lifecycle.AFTER_DESTROY_EVENT), FAILED(false, null); private final boolean available; private final String lifecycleEvent; private LifecycleState(boolean available, String lifecycleEvent) &#123; this.available = available; this.lifecycleEvent = lifecycleEvent; &#125; public boolean isAvailable() &#123; return available; &#125; public String getLifecycleEvent() &#123; return lifecycleEvent; &#125;&#125; 6.2 生命周期监听器12345public interface Lifecycle &#123; public void addLifecycleListener(LifecycleListener listener); public LifecycleListener[] findLifecycleListeners(); public void removeLifecycleListener(LifecycleListener listener);&#125; 每个周期都会有相应的监听器 123public interface LifecycleListener &#123; public void lifecycleEvent(LifecycleEvent event);&#125; The listener will be fired after the associated state change has taken place.这里的fire应该指的是触发 6.2.1 LifecyclSupport老版本的管理LifecycleListener的类，已被抛弃 6.2.2 LifecycleBaseLifecycleBase直接实现Lifecycle接口, 除了ClassLoader组件其他组件都是其子类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public abstract class LifecycleBase implements Lifecycle &#123; private final List&lt;LifecycleListener&gt; lifecycleListeners = new CopyOnWriteArrayList&lt;&gt;(); public void addLifecycleListener(LifecycleListener listener) &#123; lifecycleListeners.add(listener); &#125; public LifecycleListener[] findLifecycleListeners() &#123; return lifecycleListeners.toArray(new LifecycleListener[0]); &#125; public void removeLifecycleListener(LifecycleListener listener) &#123; lifecycleListeners.remove(listener); &#125; public void removeLifecycleListener(LifecycleListener listener) &#123; lifecycleListeners.remove(listener); &#125; protected void fireLifecycleEvent(String type, Object data) &#123; LifecycleEvent event = new LifecycleEvent(this, type, data); for (LifecycleListener listener : lifecycleListeners) &#123; listener.lifecycleEvent(event); &#125; &#125; //下面是周期方法 public final synchronized void init() throws LifecycleException &#123; if (!state.equals(LifecycleState.NEW)) &#123; invalidTransition(Lifecycle.BEFORE_INIT_EVENT); &#125; try &#123; setStateInternal(LifecycleState.INITIALIZING, null, false); initInternal(); setStateInternal(LifecycleState.INITIALIZED, null, false); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); setStateInternal(LifecycleState.FAILED, null, false); throw new LifecycleException( sm.getString("lifecycleBase.initFail",toString()), t); &#125; &#125; public final synchronized void start() throws LifecycleException &#123; //略 类似init &#125; public final synchronized void stop() throws LifecycleException &#123; //略 类似init &#125; public final synchronized void destory() throws LifecycleException &#123; //略 类似init &#125; private synchronized void setStateInternal(LifecycleState state, Object data, boolean check) throws LifecycleException &#123; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("lifecycleBase.setState", this, state)); &#125; if (check) &#123; //校验 &#125; this.state = state; String lifecycleEvent = state.getLifecycleEvent(); if (lifecycleEvent != null) &#123; fireLifecycleEvent(lifecycleEvent, data); &#125; &#125;&#125; fireLifecycleEvent用来触发监听器的事件 init，start，stop，destory是4个主要的周期方法 以init为例，除开状态校验，重点是： 123setStateInternal(LifecycleState.INITIALIZING, null, false);initInternal();setStateInternal(LifecycleState.INITIALIZED, null, false); 其中setStateInternal修改状态并通过fireLifecycleEvent触发事件，initInternal()由子类实现，子类不需要负责监听器的触发。 start，stop，destory和init类似，自己看源码，同样有XXXInternal() 注意这里并非触发了所有事件，如Lifecycle.START_EVENT，因为没有通过setStateInternal设置对应状态LifecycleState.STARTING，LifecycleState.STARTING一般在子类stopInternal中call 思考：子类并不是重写stop等，而是重写stopInternal()，这样能够有效调用生命周期内的监听。但实现子类的时候要注意不要继承错了 6.3 总结tomcat是基于组件开发的，而所有的组件都是实现了Lifecycle接口，都是有生命周期的。 对容器而言，通过向父容器添加监听器可以在父容器启动时启动自己 当然父容器可以调用子容器的start启动，只要启动顶层组件，子组件们会自动初始化完成，关闭也同理 Lifecycle的监听能都有效的提高tomcat的扩展性]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat 服务器组件和服务组件]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[14 服务器组件和服务组件14.1 服务器组件org.capache.catalina.Server接口实例表示Catalina的整个servlet引擎，囊括了所有的组件。使用一种优雅的方法启动/关闭整个系统。 Server元素表示整个Catalina servlet容器。 它的属性代表了整个servlet容器的特性。 一个服务器可能包含一个或多个服务，以及顶层的一组命名资源。 通常，此接口的实现也将实现生命周期，以便在调用start（）和stop（）方法时，所有已定义的服务也将启动或停止。 在这之间，实现必须在由port属性指定的端口号上打开服务器套接字。 当连接被接受时，第一行被读取并与指定的关闭命令进行比较。 如果命令匹配，则关闭 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public interface Server extends Lifecycle &#123; // ------------------------------------------------------------- Properties public NamingResourcesImpl getGlobalNamingResources(); public void setGlobalNamingResources (NamingResourcesImpl globalNamingResources); public javax.naming.Context getGlobalNamingContext(); public int getPort(); /** * Set the port number we listen to for shutdown commands. */ public void setPort(int port); /** * the address on which we listen to for shutdown commands. */ public String getAddress(); public void setAddress(String address); /** * the shutdown command we are waiting for. */ public String getShutdown(); public void setShutdown(String shutdown); public void setParentClassLoader(ClassLoader parent); /** * the outer Catalina startup/shutdown component */ public Catalina getCatalina(); public void setCatalina(Catalina catalina); /** * the configured base (instance) directory. 为空返回getCatalinaHome */ public File getCatalinaBase(); public void setCatalinaBase(File catalinaBase); public File getCatalinaHome(); public void setCatalinaHome(File catalinaHome); // --------------------------------------------------------- Public Methods /** * Add a new Service to the set of defined Services. */ public void addService(Service service); public Service findService(String name); public Service[] findServices(); public void removeService(Service service); /** * 等待shutdown command */ public void await(); /** * the token necessary for operations on the associated JNDI naming * context. */ public Object getNamingToken();&#125; 14.2 StandardServer类服务器接口的标准实现 属性为接口状态属性 stopAwait 关闭socketServer和停主线程 注意这里只是关闭了socketServer和停止线程，没有指明之后的操作，如stop() XXXInternal略 await 等到收到正确的关机命令，然后返回。 这使主线程保持活动，监听http连接的线程池是守护进程线程。 注意这里只是等待接受shutdown命令，匹配成功关闭socketServer和break循环，没有指明之后的操作，如stop()，与stopAwait 区别在于stopAwait 是由其他线程终止调用await 方法的线程 port==-2 负值 , 不要等待端口 - 嵌入式tomcat或者我们不喜欢用端口 port==-1 通过死循环观察stopAwait变量 开启socketServer，while(!stopAwait)不停接受连接，读取请求，与shutdown字符串匹配，相等则break 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276public final class StandardServer extends LifecycleMBeanBase implements Server &#123; // ----------------------------------------------------- Instance Variables private javax.naming.Context globalNamingContext = null; private NamingResourcesImpl globalNamingResources = null; private final NamingContextListener namingContextListener; /** * The port number on which we wait for shutdown commands. */ private int port = 8005; /** * The address on which we wait for shutdown commands. */ private String address = "localhost"; private Service services[] = new Service[0]; /** * The shutdown command string we are looking for. */ private String shutdown = "SHUTDOWN"; private volatile boolean stopAwait = false; private Catalina catalina = null; private ClassLoader parentClassLoader = null; private volatile Thread awaitThread = null; /** * Server socket that is used to wait for the shutdown command. */ private volatile ServerSocket awaitSocket = null; private File catalinaHome = null; private File catalinaBase = null; private final Object namingToken = new Object(); // ------------------------------------------------------------- Properties //getter and setter // --------------------------------------------------------- Server Methods public void stopAwait() &#123; stopAwait=true; Thread t = awaitThread; if (t != null) &#123; ServerSocket s = awaitSocket; if (s != null) &#123; awaitSocket = null; try &#123; s.close(); &#125; catch (IOException e) &#123; // Ignored &#125; &#125; t.interrupt(); try &#123; t.join(1000); &#125; catch (InterruptedException e) &#123; // Ignored &#125; &#125; &#125; /** * Wait until a proper shutdown command is received, then return. * This keeps the main thread alive * the thread pool listening for http connections is daemon threads. */ @Override public void await() &#123; // Negative values - don't wait on port - tomcat is embedded or we just don't like ports if( port == -2 ) &#123; // undocumented yet - for embedding apps that are around, alive. return; &#125; if( port==-1 ) &#123; try &#123; awaitThread = Thread.currentThread(); while(!stopAwait) &#123; try &#123; Thread.sleep( 10000 ); &#125; catch( InterruptedException ex ) &#123; &#125; &#125; &#125; finally &#123; awaitThread = null; &#125; return; &#125; // Set up a server socket to wait on try &#123; awaitSocket = new ServerSocket(port, 1, InetAddress.getByName(address)); &#125; catch (IOException e) &#123; log.error("StandardServer.await: create[" + address + ":" + port + "]: ", e); return; &#125; try &#123; awaitThread = Thread.currentThread(); // Loop waiting for a connection and a valid command while (!stopAwait) &#123; ServerSocket serverSocket = awaitSocket; if (serverSocket == null) &#123; break; &#125; // Wait for the next connection Socket socket = null; StringBuilder command = new StringBuilder(); try &#123; InputStream stream; try &#123; socket = serverSocket.accept(); stream = socket.getInputStream(); &#125; catch (SocketTimeoutException ste) &#123; continue; &#125; catch (IOException e) &#123; break; &#125; // Read a set of characters from the socket int expected = 1024; // Cut off to avoid DoS attack while (expected &gt; 0) &#123; int ch = -1; try &#123; ch = stream.read(); &#125; catch (IOException e) &#123; ch = -1; &#125; // Control character or EOF (-1) terminates loop if (ch &lt; 32 || ch == 127) &#123; break; &#125; command.append((char) ch); expected--; &#125; &#125; finally &#123; // Close the socket now that we are done with it try &#123; if (socket != null) &#123; socket.close(); &#125; &#125; catch (IOException e) &#123;&#125; &#125; // Match against our command string boolean match = command.toString().equals(shutdown); if (match) &#123; break; &#125; &#125; &#125; finally &#123; ServerSocket serverSocket = awaitSocket; awaitThread = null; awaitSocket = null; // Close the server socket and return if (serverSocket != null) &#123; try &#123; serverSocket.close(); &#125; catch (IOException e) &#123; // Ignore &#125; &#125; &#125; &#125; // --------------------------------------------------------- Public Methods @Override protected void startInternal() throws LifecycleException &#123; fireLifecycleEvent(CONFIGURE_START_EVENT, null); setState(LifecycleState.STARTING); globalNamingResources.start(); // Start our defined Services synchronized (servicesLock) &#123; for (int i = 0; i &lt; services.length; i++) &#123; services[i].start(); &#125; &#125; &#125; @Override protected void stopInternal() throws LifecycleException &#123; setState(LifecycleState.STOPPING); fireLifecycleEvent(CONFIGURE_STOP_EVENT, null); // Stop our defined Services for (int i = 0; i &lt; services.length; i++) &#123; services[i].stop(); &#125; globalNamingResources.stop(); stopAwait(); &#125; @Override protected void initInternal() throws LifecycleException &#123; super.initInternal(); onameStringCache = register(new StringCache(), "type=StringCache"); // Register the MBeanFactory MBeanFactory factory = new MBeanFactory(); factory.setContainer(this); onameMBeanFactory = register(factory, "type=MBeanFactory"); // Register the naming resources globalNamingResources.init(); // Populate the extension validator with JARs from common and shared // class loaders if (getCatalina() != null) &#123; ClassLoader cl = getCatalina().getParentClassLoader(); while (cl != null &amp;&amp; cl != ClassLoader.getSystemClassLoader()) &#123; if (cl instanceof URLClassLoader) &#123; URL[] urls = ((URLClassLoader) cl).getURLs(); for (URL url : urls) &#123; if (url.getProtocol().equals("file")) &#123; try &#123; File f = new File (url.toURI()); if (f.isFile() &amp;&amp; f.getName().endsWith(".jar")) &#123; ExtensionValidator.addSystemResource(f); &#125; &#125; catch (URISyntaxException e) &#123; // Ignore &#125; catch (IOException e) &#123; // Ignore &#125; &#125; &#125; &#125; cl = cl.getParent(); &#125; &#125; // Initialize our defined Services for (int i = 0; i &lt; services.length; i++) &#123; services[i].init(); &#125; &#125; @Override protected void destroyInternal() throws LifecycleException &#123; // Destroy our defined Services for (int i = 0; i &lt; services.length; i++) &#123; services[i].destroy(); &#125; globalNamingResources.destroy(); unregister(onameMBeanFactory); unregister(onameStringCache); super.destroyInternal(); &#125;&#125; 14.3 Service接口Service是包含一个Container或多个连接器，这些连接器共享一个Container来处理它们的传入请求。 例如，这种安排允许非SSL和SSL连接器共享相同的网络应用程序。 12345678910111213141516171819202122public interface Service extends Lifecycle &#123; // ------------------------------------------------------------- Properties public Engine getContainer(); public void setContainer(Engine engine); public String getName(); public void setName(String name); public Server getServer(); public void setServer(Server server); public ClassLoader getParentClassLoader(); public void setParentClassLoader(ClassLoader parent); public String getDomain(); // --------------------------------------------------------- Public Methods public void addConnector(Connector connector); public Connector[] findConnectors(); public void removeConnector(Connector connector); public void addExecutor(Executor ex); public Executor[] findExecutors(); public Executor getExecutor(String name); public void removeExecutor(Executor ex); Mapper getMapper();&#125; 14.4 StandardServiceService的标准实现 14.4.1 Container和ConnectorContainer只能有一个，Connector有多个，保证了一个Container能处理多种请求，如http，https这些 12protected Connector connectors[] = new Connector[0];private Engine engine = null; 14.4.1.1 add set 操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * 添加并启动connector */@Overridepublic void addConnector(Connector connector) &#123; synchronized (connectorsLock) &#123; connector.setService(this); Connector results[] = new Connector[connectors.length + 1]; System.arraycopy(connectors, 0, results, 0, connectors.length); results[connectors.length] = connector; connectors = results; if (getState().isAvailable()) &#123; try &#123; connector.start(); &#125; catch (LifecycleException e) &#123; log.error(sm.getString("standardService.connector.startFailed",connector), e); &#125; &#125; // Report this property change to interested listeners support.firePropertyChange("connector", null, connector); &#125;&#125;/** * 设置新的Engine * 启动Engine * 刷新mapperListener * 关闭旧的Engine */@Overridepublic void setContainer(Engine engine) &#123; Engine oldEngine = this.engine; if (oldEngine != null) &#123; oldEngine.setService(null); &#125; this.engine = engine; if (this.engine != null) &#123; this.engine.setService(this); &#125; if (getState().isAvailable()) &#123; if (this.engine != null) &#123; try &#123; this.engine.start(); &#125; catch (LifecycleException e) &#123; log.warn(sm.getString("standardService.engine.startFailed"), e); &#125; &#125; // Restart MapperListener to pick up new engine. try &#123; mapperListener.stop(); &#125; catch (LifecycleException e) &#123; log.warn(sm.getString("standardService.mapperListener.stopFailed"), e); &#125; try &#123; mapperListener.start(); &#125; catch (LifecycleException e) &#123; log.warn(sm.getString("standardService.mapperListener.startFailed"), e); &#125; if (oldEngine != null) &#123; try &#123; oldEngine.stop(); &#125; catch (LifecycleException e) &#123; log.warn(sm.getString("standardService.engine.stopFailed"), e); &#125; &#125; &#125; // Report this property change to interested listeners support.firePropertyChange("container", oldEngine, this.engine);&#125; 14.4.1.2 周期方法stop，init，destory，start调用子组件的stop，init，destory，start方法，源码略，子组件有： 1234protected Connector connectors[] = new Connector[0];protected final ArrayList&lt;Executor&gt; executors = new ArrayList&lt;&gt;();private Engine engine = null;protected final MapperListener mapperListener = new MapperListener(this); 14.4.1.3 Mapper123class StandardService&#123; protected final Mapper mapper = new Mapper();&#125; Mapper，实现servlet API映射规则。映射规则保存在service中实现而不是在Container中 Mapper组件的核心功能是提供请求路径的路由映射，根据某个请求路径通过计算得到相应的Servlet（Wrapper）。 123456789101112131415161718192021222324252627282930public final class Mapper &#123; volatile MappedHost[] hosts = new MappedHost[0]; /** * 支持分布式请求映射 */ private final Map&lt;Context, ContextVersion&gt; contextObjectToContextVersionMap = new ConcurrentHashMap&lt;&gt;(); //---------------------------------------------inner class type protected static final class MappedHost extends MapElement&lt;Host&gt; &#123; public volatile ContextList contextList; &#125; protected static final class ContextList &#123; public final MappedContext[] contexts; &#125; protected static final class MappedContext extends MapElement&lt;Void&gt; &#123; public volatile ContextVersion[] versions; &#125; protected static final class ContextVersion extends MapElement&lt;Context&gt; &#123; public final String path; public final WebResourceRoot resources; public String[] welcomeResources; //wellcome资源 public MappedWrapper defaultWrapper = null; public MappedWrapper[] exactWrappers = new MappedWrapper[0]; //精确匹配 public MappedWrapper[] wildcardWrappers = new MappedWrapper[0]; //表达式匹配 public MappedWrapper[] extensionWrappers = new MappedWrapper[0]; //扩展匹配 &#125; protected static class MappedWrapper extends MapElement&lt;Wrapper&gt; &#123; &#125;&#125; Mapper只要包含一个Host数组即可完成所有组件关系的映射。在tomcat启动时将所有Host容器和它的名字组成Host映射模型添加到Mapper对象中，每个Host下的Context容器和它的名字组成Context映射模型添加到对应的Host下，每个Context下的Wrapper容器和它的名字组成的Wrapper映射模型添加到对应的Context下。Mapper组件提供了对Host映射、Context映射、Wrapper映射的添加和移除的方法，在tomcat容器中添加或移除相应的容器时都要调用相应的方法维护这些映射关系。Mapper组件为了提高查找速度和效率，使用了二分搜索法查找，所以在添加时应按照字典序把Host、Context、Wrapper等映射排好序。 14.4.1.4 路由映射的建立MapperListener负责建立映射关系，实现了ContainerListener接口，支持动态刷新.StandardService中有个MapperListener，在容器更新时更新Mapper 1234567891011121314151617181920public class StandardService&#123; protected final MapperListener mapperListener = new MapperListener(this); protected void startInternal() throws LifecycleException &#123; // start engine // start any Executors // start mapper listener mapperListener.start(); // start our defined Connectors &#125; @Override public void setContainer(Engine engine) &#123; // set engine // start engine // Restart MapperListener to pick up new engine. mapperListener.stop(); mapperListener.start(); // stop old engine // Report this property change to interested listeners &#125;&#125; MapperListener实现 startInternal会初始化Mapper并添加监听 stopInternal只是删除监听 containerEvent和lifecycleEvent根据监听的容器的变化更新Mapper的状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198public class MapperListener extends LifecycleMBeanBase implements ContainerListener, LifecycleListener &#123; private final Mapper mapper; //service的mapper，用于对请求进行路由 private final Service service; //所属的service对象 public MapperListener(Service service) &#123; this.service = service; this.mapper = service.getMapper(); &#125; /** * 启动，mapper中加入engine，host，context，Wrapper * 向中engine，host，context，Wrapper注册监听 */ @Override public void startInternal() throws LifecycleException &#123; setState(LifecycleState.STARTING); Engine engine = service.getContainer(); if (engine == null) &#123; return; &#125; findDefaultHost(); //mapper添加默认host addListeners(engine); //engine及子（host，context，Wrapper）递归添加监听 Container[] conHosts = engine.findChildren(); for (Container conHost : conHosts) &#123; Host host = (Host) conHost; if (!LifecycleState.NEW.equals(host.getState())) &#123; registerHost(host); //mapper中加入host，context，Wrapper &#125; &#125; &#125; /** * stop， * 向中engine，host，context，Wrapper监听 */ @Override public void stopInternal() throws LifecycleException &#123; setState(LifecycleState.STOPPING); Engine engine = service.getContainer(); if (engine == null) &#123; return; &#125; removeListeners(engine); &#125; /** * 监听容器的lifecycleEvent * 实时更新Mapper */ @Override public void lifecycleEvent(LifecycleEvent event) &#123; if (event.getType().equals(Lifecycle.AFTER_START_EVENT)) &#123; Object obj = event.getSource(); if (obj instanceof Wrapper) &#123; Wrapper w = (Wrapper) obj; // Only if the Context has started. If it has not, then it will // have its own "after_start" event later. if (w.getParent().getState().isAvailable()) &#123; registerWrapper(w); &#125; &#125; else if (obj instanceof Context) &#123; Context c = (Context) obj; // Only if the Host has started. If it has not, then it will // have its own "after_start" event later. if (c.getParent().getState().isAvailable()) &#123; registerContext(c); &#125; &#125; else if (obj instanceof Host) &#123; registerHost((Host) obj); &#125; &#125; else if (event.getType().equals(Lifecycle.BEFORE_STOP_EVENT)) &#123; Object obj = event.getSource(); if (obj instanceof Wrapper) &#123; unregisterWrapper((Wrapper) obj); &#125; else if (obj instanceof Context) &#123; unregisterContext((Context) obj); &#125; else if (obj instanceof Host) &#123; unregisterHost((Host) obj); &#125; &#125; &#125; /** * 监听容器的ContainerEvent * 实时更新Mapper */ @Override public void containerEvent(ContainerEvent event) &#123; if (Container.ADD_CHILD_EVENT.equals(event.getType())) &#123; Container child = (Container) event.getData(); addListeners(child); // If child is started then it is too late for life-cycle listener // to register the child so register it here if (child.getState().isAvailable()) &#123; if (child instanceof Host) &#123; registerHost((Host) child); &#125; else if (child instanceof Context) &#123; registerContext((Context) child); &#125; else if (child instanceof Wrapper) &#123; // Only if the Context has started. If it has not, then it // will have its own "after_start" life-cycle event later. if (child.getParent().getState().isAvailable()) &#123; registerWrapper((Wrapper) child); &#125; &#125; &#125; &#125; else if (Container.REMOVE_CHILD_EVENT.equals(event.getType())) &#123; Container child = (Container) event.getData(); removeListeners(child); // No need to unregister - life-cycle listener will handle this when // the child stops &#125; else if (Host.ADD_ALIAS_EVENT.equals(event.getType())) &#123; // Handle dynamically adding host aliases mapper.addHostAlias(((Host) event.getSource()).getName(), event.getData().toString()); &#125; else if (Host.REMOVE_ALIAS_EVENT.equals(event.getType())) &#123; // Handle dynamically removing host aliases mapper.removeHostAlias(event.getData().toString()); &#125; else if (Wrapper.ADD_MAPPING_EVENT.equals(event.getType())) &#123; // Handle dynamically adding wrappers Wrapper wrapper = (Wrapper) event.getSource(); Context context = (Context) wrapper.getParent(); String contextPath = context.getPath(); if ("/".equals(contextPath)) &#123; contextPath = ""; &#125; String version = context.getWebappVersion(); String hostName = context.getParent().getName(); String wrapperName = wrapper.getName(); String mapping = (String) event.getData(); boolean jspWildCard = ("jsp".equals(wrapperName) &amp;&amp; mapping.endsWith("/*")); mapper.addWrapper(hostName, contextPath, version, mapping, wrapper, jspWildCard, context.isResourceOnlyServlet(wrapperName)); &#125; else if (Wrapper.REMOVE_MAPPING_EVENT.equals(event.getType())) &#123; // Handle dynamically removing wrappers Wrapper wrapper = (Wrapper) event.getSource(); Context context = (Context) wrapper.getParent(); String contextPath = context.getPath(); if ("/".equals(contextPath)) &#123; contextPath = ""; &#125; String version = context.getWebappVersion(); String hostName = context.getParent().getName(); String mapping = (String) event.getData(); mapper.removeWrapper(hostName, contextPath, version, mapping); &#125; else if (Context.ADD_WELCOME_FILE_EVENT.equals(event.getType())) &#123; // Handle dynamically adding welcome files Context context = (Context) event.getSource(); String hostName = context.getParent().getName(); String contextPath = context.getPath(); if ("/".equals(contextPath)) &#123; contextPath = ""; &#125; String welcomeFile = (String) event.getData(); mapper.addWelcomeFile(hostName, contextPath, context.getWebappVersion(), welcomeFile); &#125; else if (Context.REMOVE_WELCOME_FILE_EVENT.equals(event.getType())) &#123; // Handle dynamically removing welcome files Context context = (Context) event.getSource(); String hostName = context.getParent().getName(); String contextPath = context.getPath(); if ("/".equals(contextPath)) &#123; contextPath = ""; &#125; String welcomeFile = (String) event.getData(); mapper.removeWelcomeFile(hostName, contextPath, context.getWebappVersion(), welcomeFile); &#125; else if (Context.CLEAR_WELCOME_FILES_EVENT.equals(event.getType())) &#123; // Handle dynamically clearing welcome files Context context = (Context) event.getSource(); String hostName = context.getParent().getName(); String contextPath = context.getPath(); if ("/".equals(contextPath)) &#123; contextPath = ""; &#125; mapper.clearWelcomeFiles(hostName, contextPath, context.getWebappVersion()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java jni]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%9F%BA%E7%A1%80%2Fjni%2F</url>
    <content type="text"><![CDATA[#JNI调用#JNI是Java Native Interface的缩写，它提供了若干的API实现了Java和其他语言的通信（主要是C&amp;C++）。使用java与本地已编译的代码交互，通常会丧失平台可移植性。但是，有些情况下这样做是可以接受的，甚至是必须的。例如，使用一些旧的库，与硬件、操作系统进行交互，或者为了提高程序的性能。 ##副作用##一旦使用JNI，JAVA程序就丧失了JAVA平台的两个优点： 程序不再跨平台。要想跨平台，必须在不同的系统环境下重新编译本地语言部分。 程序不再是绝对安全的，本地代码的不当使用可能导致整个程序崩溃。一个通用规则是，你应该让本地方法集中在少数几个类当中。这样就降低了JAVA和C之间的耦合性 流程 用javac时 javac -d . HelloWorld.java在eclipse中用javah时加包名cl -I%java_home%/include -I%java_home%/include/win32 -I -LD simple_HelloWorld.cpp -Fehellodll.dll]]></content>
      <categories>
        <category>java基础</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发简介]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2Fjava%E5%B9%B6%E5%8F%91(think%20in%20java)%2F</url>
    <content type="text"><![CDATA[并发的多面性并发通常提高运行在单处理器上的程序的性能,如果没有任务阻塞,单处理器上的并发就没有意义 阻塞阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，CPU不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。 同步，就是调用一个功能，该功能没有结束前，一直等结果。 异步，就是调用一个功能，不需要知道该功能结果，该功能有结果后通知（回调通知） 阻塞，就是调用（函数），（函数）没有接收完数据或者没有得到结果之前，不会返回。 非阻塞，就是调用（函数），（函数）立即返回，通过select通知调用者。 定义任务实现Runnable接口并编写run方法 Thread.yield() :是对线程调度器(java线程一部分,可以将cpu从一个线程转移给另一个线程)的一种建议,表明我已经执行完生命周期中最重要的一部分. Thread将Runnable对象提交给Thread来驱动,通过start()启动任务[run()],不过start()是立即返回的 Executor管理Thread,简化并发编程 ExecutorService exec=Executor.newCachedThreadPool(); //基于缓存,创建所需数量相同的线程 ExecutorService exec=Executor.newFixedThreadPool(); //线程的数量有限 ExecutorService exec=Executor.newSingleThreadExecutor(); //线程的数量为一 exec.execute(new Runnable(){...}); exec.shutdown() //禁止提交新任务,但是旧任务会执行完 所有的线程池都会尽可能的复用在线程数量有限且用完,新任务会进行排队 Callable完成任务有返回值 exec.submit(new Callable(){...}); submit()返回Future对象,IsDown()查询是否完成,直接调用get()会发生阻塞 休眠sleep(),使任务终止执行给定的时间sleep()会抛出InterruptedException异常,在run()中被捕获,因为异常不能跨线程传递,必须本地处理 优先级调度器倾向于让优先级高的线程先执行,优先级低的也可以得到执行,只是频率低 getThread(); setThread(); Thread.MAX_PRIORITY;Thread.NORM_PRIORITY;Thread.MAX_PRIORITY; 让步yield() 让当前线程回到可执行状态，以便让具有相同优先级的线程进入执行状态，但不是绝对的。因为虚拟机可能会让该线程重新进入执行状态。 后台线程setDaemon(true); 必须在线程启动前调用setDaemon(true)才能设置为后台线程当最后一个非后台线程终止时,后台线程会突然终止 加入线程如果某个线程在另一个线程t上调用t.join();此线程将被挂起,直到t执行完才恢复.可以加上时间参数 共享受限资源synchronized所有的对象都自动含有单一的锁,当在对象上调用其任意synchronized方法的时候,此对象会被加锁,此时该对象的其他synchronized方法只有等到前一个方法调用完毕并释放锁后才能调用. java.util.concurrent.Lock比synchronized复杂,但是给了你更细粒度的控制力.你可以尝试获取锁,不成功可以去做其他事,而不至于阻塞. 原子性与易变性原子操作是不能被线程调度机制中断的操作. 原子性指事务的一个完整操作。操作成功则提交，失败则回滚，即一件事要做就做完整，要么就什么都不做 volatile保证应用中的可视性,如果这个域发生了写操作,那么所有的读操作都可以看到这个修改,volatile域会立即写到主存里,而写操作也发生在主存里 原子类AtomicInteger/AtomicLong/AtomicReference…参考另一篇 临界区有时你只希望多个线程同时访问方法内部的部分代码而不是访问整个方法,使用synchronized建立,synchronized用来指定某个对象,此对象的锁来做同步控制 synchronized(syncObject){ //this code can be accessed by only one task at a time } 使用Lock对象建立临界区这种方法可以显著提高访问性能 线程本地存储根除对变量的控制.线程本地存储是一种自动化机制,可以为使用相同变量的每个不同的线程创建不同的存储java.lang.ThreadLocal ThreadLocal对象通常当作静态域存储,只能通过get和set方法访问该对象的内容.get返回与线程相关联的副本,set将数值插入其线程存储对象中,并返回原有对象 终结线程 使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。 使用stop方法强行终止线程（这个方法不推荐使用，因为stop和suspend、resume一样，也可能发生不可预料的结果，不释放锁）。 使用interrupt方法中断线程。 在阻塞时终结线程状态 新建:线程创建时,只会短暂的处于该状态,它已经分配了必备的资源,并进行了初始化,此线程已经有资格获取cpu 就绪:只要调度器把时间片分给线程,线程就可以运行 阻塞:线程能够运行,但是有个条件阻止它运行 死亡:不再可调度 一个任务进入阻塞状态,可能因为: 调用sleep(millisecinds); 调用wait() 等待某个输入输出完成 调用同步控制方法,等待锁 stop()方法被废弃,因为其不释放锁 中断如果一个线程已经被阻塞,修改中断位会抛出InterruptedException异常,而interrupted()离开run()但不抛出异常 interrupt方法是唯一能将中断状态设置为true的方法 Java中断机制是一种协作机制，也就是说通过中断并不能直接终止另一个线程，而需要被中断的线程自己处理中断。 检查中断interrupted() //检查中断并返回状态,并清除中断位 interrupt() //中断,并设为true 线程之间的协作wait()/notifyAll()wait()会在等待外部世界产生变化的时候将任务挂起,只有在notify()或者notifyAll()发生时,这个任务才会被唤醒并检查变化 调用sleep()和yield()时锁并没有释放,当调用wait()时,线程挂起,锁被释放. wait() &lt;==&gt; 挂起该线程+释放锁+唤醒其他线程 在实际中可能会有多个任务处于wait()中,所以notifyAll()更安全 notifyAll()只会唤醒等待该锁的所有wait()线程 何时可以用notify: 所有任务必须等待同一个条件 当条件发生变化时,只有一个任务起作用 使用显示的Lock和Condition参考其他]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面向对象]]></title>
    <url>%2F2018%2F03%2F16%2Fpython%E5%9F%BA%E7%A1%80%2Fpython%20%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[1 面向对象编程 类和实例 12345class Student(object): def __init__(self, name, score): self.name = name self.score = scor 有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去 数据封装 访问限制 实例的变量名如果以__开头，就变成了一个私有变量（private） 需要注意的是，在Python中，变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量，所以，不能用__name__、__score__这样的变量名。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量 python _、__和__xx__的区别 &quot;_&quot;单下划线 在解释器中：在这种情况下，“_”代表交互式解释器会话中上一条执行的语句的结果。 以下划线“_”为前缀的名称（如_spam）应该被视为API中非公开的部分（不管是函数、方法还是数据成员） &quot;__&quot;双下划线 __spam这种形式（至少两个前导下划线，最多一个后续下划线）的任何标识符将会被“_classname__spam”这种形式原文取代，在这里“classname”是去掉前导下划线的当前类名。 名称前后的双下划线 变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量 继承和多态 12class Class(superClass): pass 获取对象信息 type() 123456789101112&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue&gt;&gt;&gt; type(123)==intTrue&gt;&gt;&gt; type(&apos;abc&apos;)==type(&apos;123&apos;)True&gt;&gt;&gt; type(abs)&lt;class &apos;builtin_function_or_method&apos;&gt; isinstance() 12&gt;&gt;&gt; isinstance(h, Dog)True dir() 12&gt;&gt;&gt; dir(&apos;ABC&apos;)[&apos;__add__&apos;, &apos;__class__&apos;,..., &apos;__subclasshook__&apos;, &apos;capitalize&apos;, &apos;casefold&apos;,..., &apos;zfill&apos;] 仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态： 123456789101112131415&gt;&gt;&gt; class MyObject(object):... def __init__(self):... self.x = 9... def power(self):... return self.x * self.x...&gt;&gt;&gt; obj = MyObject()&gt;&gt;&gt; hasattr(obj, &apos;x&apos;) # 有属性&apos;x&apos;吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, &apos;y&apos;) # 有属性&apos;y&apos;吗？False&gt;&gt;&gt; setattr(obj, &apos;y&apos;, 19) # 设置一个属性&apos;y&apos; 实例属性和类属性 给实例绑定属性的方法是通过实例变量，或者通过self变量： 123456class Student(object): def __init__(self, name): self.name = names = Student(&apos;Bob&apos;)s.score = 90 Student类本身绑定属性直接在class中定义属性 12class Student(object): name = &apos;Student&apos; 当我们定义了一个类属性后，这个属性虽然归类所有，但类的所有实例都可以访问到。 12345678910111213141516&gt;&gt;&gt; class Student(object):... name = &apos;Student&apos;...&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; print(Student.name) # 打印类的name属性Student&gt;&gt;&gt; s.name = &apos;Michael&apos; # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 如果删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 2 面向对象高级2.1 使用__slots__正常情况下，当我们定义了一个class，创建了一个class的实例后，我们可以给该实例绑定任何属性和方法，这就是动态语言的灵活性 12class Student(object): pass 然后，尝试给实例绑定一个属性： 1234&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = &apos;Michael&apos; # 动态给实例绑定一个属性&gt;&gt;&gt; print(s.name)Michael 还可以尝试给实例绑定一个方法： 12345678&gt;&gt;&gt; def set_age(self, age): # 定义一个函数作为实例方法... self.age = age...&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 给实例绑定一个方法&gt;&gt;&gt; s.set_age(25) # 调用实例方法&gt;&gt;&gt; s.age # 测试结果25 注意，以上都是给实例绑定的方法 为了给所有实例都绑定方法，可以给class绑定方法： 1234&gt;&gt;&gt; def set_score(self, score):... self.score = score...&gt;&gt;&gt; Student.set_score = set_score 使用__slots____slots__变量，来限制该class实例能添加的属性，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： 12class Student(object): __slots__ = (&apos;name&apos;, &apos;age&apos;) # 用tuple定义允许绑定的属性名称 在子类中也定义__slots__，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。 2.2 @propertyPython内置的@property装饰器就是负责把一个方法变成属性调用的： 12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('。。。') if value &lt; 0 or value &gt; 100: raise ValueError('。。。') self._score = value getter方法变成属性，只需要加上@property此时，@property本身又创建了另一个装饰器`@score.setter`，负责把一个setter方法变成属性赋值 2.3 多重继承12class Bat(Mammal, Flyable): pass 2.4 定制类__slots__这种形如__xxx__的变量或者函数名就要注意，这些在Python中是有特殊用途的 __slots__ __len__()方法我们也知道是为了能让class作用于len()函数。 __str__ 类似toString() __repr__() 返回程序开发者看到的字符串,&lt;__main__.Student object at 0x109afb310&gt;,__repr__ = __str__ __iter__用于for ... in循环 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration() return self.a # 返回下一个值 __getitem__由于类list访问，通过[]下标 12345678910111213141516171819class Fib(object): def __getitem__(self, n): if isinstance(n, int): # n是索引 a, b = 1, 1 for x in range(n): a, b = b, a + b return a if isinstance(n, slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a, b = 1, 1 L = [] for x in range(stop): if x &gt;= start: L.append(a) a, b = b, a + b return L __getattr__避免调用不存在的score属性 123def __getattr__(self, attr): if attr==&apos;score&apos;: return 99 __call__ 对象看成函数 123456class Student(object): def __init__(self, name): self.name = name def __call__(self): print(&apos;My name is %s.&apos; % self.name) 调用方式如下： 123&gt;&gt;&gt; s = Student(&apos;Michael&apos;)&gt;&gt;&gt; s() # self参数不要传入My name is Michael. 通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。 2.3 枚举123from enum import EnumMonth = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) 12for name, member in Month.__members__.items(): print(name, &apos;=&gt;&apos;, member, &apos;,&apos;, member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： 1234567891011from enum import Enum, unique@uniqueclass Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。 2.4 使用元类2.4.1 type()动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。 type()函数可以查看一个类型或变量的类型，Hello是一个class，它的类型就是type，而h是一个实例，它的类型就是class Hello。 type()函数既可以返回一个对象的类型，又可以创建出新的类型，要创建一个class对象，type()函数依次传入3个参数： class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 2.4.2 metaclassmetaclass，直译为元类，metaclass允许你创建类或者修改类。 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。]]></content>
      <categories>
        <category>python</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础]]></title>
    <url>%2F2018%2F03%2F16%2Fpython%E5%9F%BA%E7%A1%80%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1 python介绍优点： 简单优雅 完善的基础代码库 缺点： 代码不能加密，发布Python程序，实际上就是发布源代码，凡是编译型的语言，都没有这个问题，而解释型的语言，则必须把源码发布出去。 运行速度慢，Python是解释型语言，你的代码在执行时会一行一行地翻译成CPU能理解的机器码 1.1 python解释器 CPython 当我们从Python官方网站下载并安装好Python 3.x后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的 IPython IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强 pypy PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 Jython Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 Python的解释器很多，但使用最广泛的还是CPython。如果要和Java或.Net平台交互，最好的办法不是用Jython或IronPython，而是通过网络调用来交互，确保各程序之间的独立性。 2 基础Python的语法比较简单，采用缩进方式 123456# print absolute value of an integer:a = 100if a &gt;= 0: print(a)else: print(-a) 以#开头的语句是注释，注释是给人看的，可以是任意内容，解释器会忽略掉注释。其他每一行都是一个语句，当语句以冒号:结尾时，缩进的语句视为代码块。 2.1 数据类型和变量 整数 浮点数 字符串 字符串是以单引号&#39;或双引号&quot;括起来的任意文本，比如&#39;abc&#39;，&quot;xyz&quot;等等。请注意，&#39;&#39;或&quot;&quot;本身只是一种表示方式，不是字符串的一部分，因此，字符串&#39;abc&#39;只有a，b，c这3个字符。如果&#39;本身也是一个字符，那就可以用&quot;&quot;括起来， 转义字符\ 为了简化，Python还允许用r&#39;&#39;表示&#39;&#39;内部的字符串默认不转义 Python允许用在交互命令行中&#39;&#39;&#39;...&#39;&#39;&#39;的格式表示多行内容，...是提示符，不是代码的一部分，所以在py文件中不需要 ...字符 1234567891011&gt;&gt;&gt; print('''line1...line2...line3''')line1line2line3# 在py文件中print('''line1line2line3''') 布尔值 True、False（请注意大小写） 布尔值可以用and、or和not运算。 空值 None 变量 变量名必须是大小写英文、数字和_的组合，且不能用数字开头 常量 在Python中，通常用全部大写的变量名表示常量，但事实上仍然是一个变量，Python根本没有任何机制保证不会被改变 在Python中，有两种除法，一种除法是/： 12345678&gt;&gt;&gt; 10 / 33.3333333333333335# /除法计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数：# //，称为地板除，两个整数的除法仍然是整数：&gt;&gt;&gt; 10 // 33&gt;&gt;&gt; 10 % 31 注意：Python的整数没有大小限制，而某些语言的整数根据其存储长度是有大小限制的，例如Java对32位整数的范围限制在-2147483648-2147483647。 Python的浮点数也没有大小限制，但是超出一定范围就直接表示为inf（无限大）。 2.2 字符串和编码2.2.1 字符编码Unicode把所有语言都统一到一套编码里，常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。 如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。 所以，出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间： 字符 ASCII Unicode UTF-8 A 01000001 00000000 01000001 01000001 中 x 01001110 00101101 11100100 10111000 10101101 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件： 2.2.3 Python的字符串对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符： 12345678&gt;&gt;&gt; ord(&apos;A&apos;)65&gt;&gt;&gt; ord(&apos;中&apos;)20013&gt;&gt;&gt; chr(66)&apos;B&apos;&gt;&gt;&gt; chr(25991)&apos;文&apos; 如果知道字符的整数编码，还可以用十六进制这么写str： 12&gt;&gt;&gt; &apos;\u4e2d\u6587&apos;&apos;中文&apos; 两种写法完全是等价的。 Python对bytes类型的数据用带b前缀的单引号或双引号表示： 1x = b&apos;ABC&apos; bytes的每个字符都只占用一个字节。 以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如： 12345678&gt;&gt;&gt; &apos;ABC&apos;.encode(&apos;ascii&apos;)b&apos;ABC&apos;&gt;&gt;&gt; &apos;中文&apos;.encode(&apos;utf-8&apos;)b&apos;\xe4\xb8\xad\xe6\x96\x87&apos;&gt;&gt;&gt; &apos;中文&apos;.encode(&apos;ascii&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 0-1: ordinal not in range(128) 纯英文的str可以用ASCII编码为bytes，内容是一样的，含有中文的str可以用UTF-8编码为bytes。在bytes中，无法显示为ASCII字符的字节，用\x##显示。 反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法： 1234&gt;&gt;&gt; b&apos;ABC&apos;.decode(&apos;ascii&apos;)&apos;ABC&apos;&gt;&gt;&gt; b&apos;\xe4\xb8\xad\xe6\x96\x87&apos;.decode(&apos;utf-8&apos;)&apos;中文&apos; 如果bytes中只有一小部分无效的字节，可以传入errors=&#39;ignore&#39;忽略错误的字节： 12&gt;&gt;&gt; b&apos;\xe4\xb8\xad\xff&apos;.decode(&apos;utf-8&apos;, errors=&apos;ignore&apos;)&apos;中&apos; len(str)计算长度： len(bytes)计算字节数： 由于Python源代码也是一个文本文件，所以，当你的源代码中包含中文的时候，在保存源代码时，就需要务必指定保存为UTF-8编码。当Python解释器读取源代码时，为了让它按UTF-8编码读取，我们通常在文件开头写上这两行： 1234#!/usr/bin/env python3# -*- coding: utf-8 -*-第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码。 2.2.4 格式化1234&gt;&gt;&gt; &apos;Hello, %s&apos; % &apos;world&apos;&apos;Hello, world&apos;&gt;&gt;&gt; &apos;Hi, %s, you have $%d.&apos; % (&apos;Michael&apos;, 1000000)&apos;Hi, Michael, you have $1000000.&apos; 常见的占位符有： 占位符 替换内容 %d 整数 %f 浮点数 %s 字符串 %x 十六进制整数 %转义，用%%来表示一个%： 精度控制 12345round(2.645, 2) # 2.65 不一定四舍五入，因为计算机表示浮点数不是完全的，如1.22=1.220000000001round(2.5) # 2 .5类型靠向偶数("%.2f" % 2.635) # 2.63 ("%6.2f" % 2.645) # 2.65 6表示一个显示多少字符，超出前面补空格，不足忽略6 2.2.5 format()用传入的参数依次替换字符串内的占位符{0}、{1}…… 12&gt;&gt;&gt; &apos;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&apos;.format(&apos;小明&apos;, 17.125)&apos;Hello, 小明, 成绩提升了 17.1%&apos; 2.3 list和tuple2.3.1 list123&gt;&gt;&gt; classmates = [&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;]&gt;&gt;&gt; classmates[&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;] 12&gt;&gt;&gt; len(classmates)3 用索引来访问list中每一个位置的元素，记得索引是从0开始的： 12&gt;&gt;&gt; classmates[0]&apos;Michael&apos; 用-n做索引，直接获取倒数第n个元素： 1234追加：classmates.append(&apos;Adam&apos;) //入栈插入：classmates.insert(1, &apos;Jack&apos;)删除末尾：classmates.pop() //出栈删除：classmates.pop(1) list里面的元素的数据类型也可以不同，比如： 1&gt;&gt;&gt; L = [&apos;Apple&apos;, 123, True] 2.3.2 tuple一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改 classmates = (&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;) 获取元素的方法和list是一样的 不可变的tuple有什么意义？因为tuple不可变，所以代码更安全。如果可能，能用tuple代替list就尽量用tuple。 tuple的陷阱 123&gt;&gt;&gt; t = (1)&gt;&gt;&gt; t1 123&gt;&gt;&gt; t = (1,)&gt;&gt;&gt; t(1,) 后来看一个“可变的”tuple： 12345&gt;&gt;&gt; t = (&apos;a&apos;, &apos;b&apos;, [&apos;A&apos;, &apos;B&apos;])&gt;&gt;&gt; t[2][0] = &apos;X&apos;&gt;&gt;&gt; t[2][1] = &apos;Y&apos;&gt;&gt;&gt; t(&apos;a&apos;, &apos;b&apos;, [&apos;X&apos;, &apos;Y&apos;]) 2.4 条件判断12345678if &lt;条件判断1&gt;: &lt;执行1&gt;elif &lt;条件判断2&gt;: &lt;执行2&gt;elif &lt;条件判断3&gt;: &lt;执行3&gt;else: &lt;执行4&gt; 12if x: print(&apos;True&apos;) 只要x是非零数值、非空字符串、非空list等，就判断为True，否则为False。 2.5 循环Python的循环有两种，一种是for…in循环 1234567names = ['Michael', 'Bob', 'Tracy']for name in names: print(name) fruits = ['banana', 'apple', 'mango']for index in range(len(fruits)): print '当前水果 :', fruits[index] 1range(start, stop[, step]) 参数说明： start: 计数从 start 开始。默认是从 0 开始。例如range（5）等价于range（0， 5）; end: 计数到 end 结束，但不包括 end。例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5 step：步长，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1) 第二种循环是while循环，只要条件满足，就不断循环，条件不满足时退出循环。 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print(sum) break continue 2.6 dict和set2.6.1 dict123&gt;&gt;&gt; d = &#123;&apos;Michael&apos;: 95, &apos;Bob&apos;: 75, &apos;Tracy&apos;: 85&#125;&gt;&gt;&gt; d[&apos;Michael&apos;]95 如果key不存在，dict就会报错，要避免key不存在的错误，有两种办法，一是通过in判断key是否存在： 12&gt;&gt;&gt; &apos;Thomas&apos; in dFalse 二是通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value： 123&gt;&gt;&gt; d.get(&apos;Thomas&apos;)&gt;&gt;&gt; d.get(&apos;Thomas&apos;, -1)-1 删除：pop(key) 2.6.2 set要创建一个set，需要提供一个list作为输入集合： 12345678910&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125;s.add(4)s.remove(4)s1 &amp; s2s1 | s2s1 - s2 3 函数函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”： 123&gt;&gt;&gt; a = abs # 变量a指向abs函数&gt;&gt;&gt; a(-1) # 所以也可以通过a调用abs函数1 3.1 定义函数12345def my_abs(x): if x &gt;= 0: return x else: return -x 如果你已经把my_abs()的函数定义保存为abstest.py文件了，那么，可以在该文件的当前目录下启动Python解释器，用from abstest import my_abs来导入my_abs()函数，注意abstest是文件名（不含.py扩展名）： 如果想定义一个什么事也不做的空函数，可以用pass语句： 12def nop(): pass isinstance()参数检查 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(&apos;bad operand type&apos;) if x &gt;= 0: return x else: return -x 返回多个值 123456import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny 但其实这只是一种假象，Python函数返回的仍然是单一值：返回值是一个tuple 123&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(r)(151.96152422706632, 70.0) 2.2 函数的参数2.2.1 位置参数123456def power(x, n): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 修改后的power(x, n)函数有两个参数：x和n，这两个参数都是位置参数，调用函数时，传入的两个值按照位置顺序依次赋给参数x和n。 2.2.2 默认参数12def power(x, n=2): ... 注意： 必选参数在前，默认参数在后 如何设置默认参数。 当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 可见，默认参数降低了函数调用的难度，而一旦需要更复杂的调用时，又可以传递更多的参数来实现。无论是简单调用还是复杂调用，函数只需要定义一个。 定义默认参数要牢记一点：默认参数必须指向不变对象！ 2.2.3 可变参数所以，我们把函数的参数改为可变参数： 12345def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 1234&gt;&gt;&gt; calc(1, 2)5&gt;&gt;&gt; calc()0 在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去： 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 2.2.4 关键字参数可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 12def person(name, age, **kw): print(&apos;name:&apos;, name, &apos;age:&apos;, age, &apos;other:&apos;, kw) 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： 123456&gt;&gt;&gt; person(&apos;Michael&apos;, 30)name: Michael age: 30 other: &#123;&#125;&gt;&gt;&gt; person(&apos;Bob&apos;, 35, city=&apos;Beijing&apos;)name: Bob age: 35 other: &#123;&apos;city&apos;: &apos;Beijing&apos;&#125;&gt;&gt;&gt; person(&apos;Adam&apos;, 45, gender=&apos;M&apos;, job=&apos;Engineer&apos;)name: Adam age: 45 other: &#123;&apos;gender&apos;: &apos;M&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; 当然，上面复杂的调用可以用简化的写法： 123&gt;&gt;&gt; extra = &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125;&gt;&gt;&gt; person(&apos;Jack&apos;, 24, **extra)name: Jack age: 24 other: &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; **extra表示把extra这个dict的所有key-value用关键字参数传入到函数的**kw参数，kw将获得一个dict，注意kw获得的dict是extra的一份拷贝，对kw的改动不会影响到函数外的extra。 2.2.5 命名关键字参数限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下： 12def person(name, age, *, city, job): print(name, age, city, job) 和关键字参数**kw不同，命名关键字参数需要一个特殊分隔符*，*后面的参数被视为命名关键字参数。 12&gt;&gt;&gt; person(&apos;Jack&apos;, 24, city=&apos;Beijing&apos;, job=&apos;Engineer&apos;)Jack 24 Beijing Engineer 如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了： 12def person(name, age, *args, city, job): print(name, age, args, city, job) 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错： 1234&gt;&gt;&gt; person(&apos;Jack&apos;, 24, &apos;Beijing&apos;, &apos;Engineer&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: person() takes 2 positional arguments but 4 were given 由于调用时缺少参数名city和job，Python解释器把这4个参数均视为位置参数，但person()函数仅接受2个位置参数。 命名关键字参数可以有缺省值，从而简化调用： 12def person(name, age, *, city=&apos;Beijing&apos;, job): print(name, age, city, job) 使用命名关键字参数时，要特别注意，如果没有可变参数，就必须加一个*作为特殊分隔符。如果缺少*，Python解释器将无法识别位置参数和命名关键字参数： 2.2.6 参数组合在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 比如定义一个函数，包含上述若干种参数： 12345def f1(a, b, c=0, *args, **kw): print(&apos;a =&apos;, a, &apos;b =&apos;, b, &apos;c =&apos;, c, &apos;args =&apos;, args, &apos;kw =&apos;, kw)def f2(a, b, c=0, *, d, **kw): print(&apos;a =&apos;, a, &apos;b =&apos;, b, &apos;c =&apos;, c, &apos;d =&apos;, d, &apos;kw =&apos;, kw) 最神奇的是通过一个tuple和dict，你也可以调用上述函数： 12345678&gt;&gt;&gt; args = (1, 2, 3, 4)&gt;&gt;&gt; kw = &#123;&apos;d&apos;: 99, &apos;x&apos;: &apos;#&apos;&#125;&gt;&gt;&gt; f1(*args, **kw)a = 1 b = 2 c = 3 args = (4,) kw = &#123;&apos;d&apos;: 99, &apos;x&apos;: &apos;#&apos;&#125;&gt;&gt;&gt; args = (1, 2, 3)&gt;&gt;&gt; kw = &#123;&apos;d&apos;: 88, &apos;x&apos;: &apos;#&apos;&#125;&gt;&gt;&gt; f2(*args, **kw)a = 1 b = 2 c = 3 d = 88 kw = &#123;&apos;x&apos;: &apos;#&apos;&#125; 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。 2.3 递归函数3 高级特性3.1 切片123L[0:3] //取前3个元素L[:3]L[-2:] 3.2 迭代当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，而我们不太关心该对象究竟是list还是其他数据类型。如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断： 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 字典： 1234567&gt;&gt;&gt; d = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3&#125;&gt;&gt;&gt; for key in d:... print(key)for value in d.values()for k, v in d.items() Python内置的enumerate函数可以把一个list变成索引-元素对 12for i, value in enumerate([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]):... print(i, value) 123456&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 3.2 列表生成式列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 生成[1x1, 2x2, 3x3, ..., 10x10] 123&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x) 1[x * x for x in range(1, 11)] for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 1&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0] 还可以使用两层循环，可以生成全排列： 12345678910&gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;][&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;]&gt;&gt;&gt; d = &#123;&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; &#125;&gt;&gt;&gt; [k + &apos;=&apos; + v for k, v in d.items()][&apos;y=B&apos;, &apos;x=A&apos;, &apos;z=C&apos;]&gt;&gt;&gt; L = [&apos;Hello&apos;, &apos;World&apos;, &apos;IBM&apos;, &apos;Apple&apos;]&gt;&gt;&gt; [s.lower() for s in L][&apos;hello&apos;, &apos;world&apos;, &apos;ibm&apos;, &apos;apple&apos;] 3.3 生成器通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 在Python中，这种一边循环一边计算的机制，称为生成器：generator。 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 我们可以直接打印出list的每一个元素，generator通过next()函数获得generator的下一个返回值： 1234567&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4... 不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象： 1234567&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:... print(n)... 014 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return &apos;done&apos; generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return &apos;done&apos; 如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator： generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 3.4 迭代器直接作用于for循环的数据类型有以下几种： 集合数据类型，如list、tuple、dict、set、str等； generator，包括生成器和带yield的generator function。 可以使用isinstance()判断一个对象是否是Iterable对象： 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的 4 函数式编程4.1 高阶函数变量可以指向函数 函数名也是变量 既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。 12def add(x, y, f): return f(x) + f(y) 4.1.1 mapreduce123456&gt;&gt;&gt; def f(x):... return x * x...&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; list(r)[1, 4, 9, 16, 25, 36, 49, 64, 81] map()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。 reduce把一个函数作用在一个序列[x1, x2, x3, ...]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是： 1reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 4.1.2 filterPython内建的filter()函数用于过滤序列。 和map()类似，filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 例如，在一个list中，删掉偶数，只保留奇数，可以这么写： 12345def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 结果: [1, 5, 9, 15] 4.1.3 sortedPython内置的sorted()函数就可以对list进行排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21])[-21, -12, 5, 9, 36] 此外，sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] 要进行反向排序，不必改动key函数，可以传入第三个参数reverse=True： 12&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower, reverse=True)[&apos;Zoo&apos;, &apos;Credit&apos;, &apos;bob&apos;, &apos;about&apos;] 4.2 返回函数高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。 通常情况下，求和的函数是这样定义的： 12345def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数： 1234567def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum 当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt; 调用函数f时，才真正计算求和的结果： 12&gt;&gt;&gt; f()25 在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。 请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数： 1234&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f1==f2False f1()和f2()的调用结果互不影响。 4.3 匿名函数Python中，对匿名函数提供了有限支持。还是以map()函数为例，计算f(x)=x2时，除了定义一个f(x)的函数外，还可以直接传入匿名函数： 12&gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[1, 4, 9, 16, 25, 36, 49, 64, 81] 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： 4.4 装饰器在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下： 12345def log(func): def wrapper(*args, **kw): print(&apos;call %s():&apos; % func.__name__) return func(*args, **kw) return wrapper 123@logdef now(): print(&apos;2015-3-25&apos;) 由于log()是一个decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本： 1234567def log(text): def decorator(func): def wrapper(*args, **kw): print(&apos;%s %s():&apos; % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个3层嵌套的decorator用法如下： 123@log(&apos;execute&apos;)def now(): print(&apos;2015-3-25&apos;) 执行结果如下： 123&gt;&gt;&gt; now()execute now():2015-3-25 4.5 偏函数int()函数可以把字符串转换为整数，当仅传入字符串时，int()函数默认按十进制转换： 12&gt;&gt;&gt; int(&apos;12345&apos;)12345 但int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换： 1234&gt;&gt;&gt; int(&apos;12345&apos;, base=8)5349&gt;&gt;&gt; int(&apos;12345&apos;, 16)74565 假设要转换大量的二进制字符串，每次都传入int(x, base=2)非常麻烦，于是，我们想到，可以定义一个int2()的函数，默认把base=2传进去： 12def int2(x, base=2): return int(x, base) functools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2： 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2(&apos;1000000&apos;)64&gt;&gt;&gt; int2(&apos;1010101&apos;)85 所以，简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 5 模块在Python中，一个.py文件就称之为一个模块（Module）。 为了避免模块名冲突，Python又引入了按目录来组织模块的方法，称为包（Package）。 1234mycompany├─ __init__.py├─ abc.py└─ xyz.py abc.py模块的名字就变成了mycompany.abc 每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。 5.1 使用模块1234567891011121314151617181920#!/usr/bin/env python3# -*- coding: utf-8 -*-' a test module '__author__ = 'Michael Liao'import sysdef test(): args = sys.argv if len(args)==1: print('Hello, world!') elif len(args)==2: print('Hello, %s!' % args[1]) else: print('Too many arguments!')if __name__=='__main__': test() 第1行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行 第2行注释表示.py文件本身使用标准UTF-8编码 第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释； 第6行使用__author__变量把作者写进去 导入模块： 1import sys 12if __name__==&apos;__main__&apos;: test() 当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败 作用域是通过_前缀来实现私用函数和变量。 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等； 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名； 类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等； 之所以我们说，private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。 5.2 安装第三方模块在Python中，安装第三方模块，是通过包管理工具pip完成的。 1pip install modleName 安装常用模块 直接使用Anaconda 模块搜索路径默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中]]></content>
      <categories>
        <category>python</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python IO&并发&正则]]></title>
    <url>%2F2018%2F03%2F16%2Fpython%E5%9F%BA%E7%A1%80%2Fpython%20IO%26%E5%B9%B6%E5%8F%91%26%E6%AD%A3%E5%88%99%2F</url>
    <content type="text"><![CDATA[文件读写要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符： 1f = open(&apos;/Users/michael/test.txt&apos;, &apos;r&apos;) 如果文件不存在，open()函数就会抛出一个IOError的错误 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： 1&gt;&gt;&gt; f.read() 最后一步是调用close()方法关闭文件 1&gt;&gt;&gt; f.close() 123456try: f = open(&apos;/path/to/file&apos;, &apos;r&apos;) print(f.read())finally: if f: f.close() Python引入了with语句来自动帮我们调用close()方法： 12with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f: print(f.read()) read()会一次性读取文件的全部内容 read(size) readline()可以每次读取一行内容 readlines()一次读取所有内容并按行返回list 二进制文件前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用&#39;rb&#39;模式打开文件即可： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.jpg&apos;, &apos;rb&apos;)&gt;&gt;&gt; f.read()b&apos;\xff\xd8\xff\xe1\x00\x18Exif\x00\x00...&apos; # 十六进制表示的字节 字符编码要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数，例如，读取GBK编码的文件： 12&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;)&gt;&gt;&gt; f.read() open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略： 1&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;, errors=&apos;ignore&apos;) 写文件写文件和读文件是一样的，唯一区别是调用open()函数时，传入标识符&#39;w&#39;或者&#39;wb&#39;表示写文本文件或写二进制文件： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.txt&apos;, &apos;w&apos;)&gt;&gt;&gt; f.write(&apos;Hello, world!&apos;)&gt;&gt;&gt; f.close() StringIO12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('hello')5&gt;&gt;&gt; f.write(' ')1&gt;&gt;&gt; f.write('world!')6&gt;&gt;&gt; print(f.getvalue())hello world! 1234567891011&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO('Hello!\nHi!\nGoodbye!')&gt;&gt;&gt; while True:... s = f.readline()... if s == '':... break... print(s.strip())...Hello!Hi!Goodbye! BytesIO123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write('中文'.encode('utf-8'))6&gt;&gt;&gt; print(f.getvalue())b'\xe4\xb8\xad\xe6\x96\x87' 1234&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO(b'\xe4\xb8\xad\xe6\x96\x87')&gt;&gt;&gt; f.read()b'\xe4\xb8\xad\xe6\x96\x87' 操作文件和目录Python内置的os模块也可以直接调用操作系统提供的接口函数。 123&gt;&gt;&gt; import os&gt;&gt;&gt; os.name # 操作系统类型&apos;posix&apos; 12345678910&gt;&gt;&gt; os.uname()posix.uname_result(sysname='Darwin', nodename='MichaelMacPro.local', release='14.3.0', version='Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64', machine='x86_64')&gt;&gt;&gt; os.environenviron(&#123;'VERSIONER_PYTHON_PREFER_32_BIT': 'no', 'TERM_PROGRAM_VERSION': '326', 'LOGNAME': 'michael', 'USER': 'michael', 'PATH': '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin', ...&#125;)&gt;&gt;&gt; os.environ.get('PATH')'/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin'&gt;&gt;&gt; os.environ.get('x', 'default')'default' 操作文件和目录的函数一部分放在os模块中，一部分放在os.path模块中， 12345678910# 查看当前目录的绝对路径:&gt;&gt;&gt; os.path.abspath('.')'/Users/michael'# 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:&gt;&gt;&gt; os.path.join('/Users/michael', 'testdir')'/Users/michael/testdir'# 然后创建一个目录:&gt;&gt;&gt; os.mkdir('/Users/michael/testdir')# 删掉一个目录:&gt;&gt;&gt; os.rmdir('/Users/michael/testdir') 但是复制文件的函数在os模块中不存在！原因是复制文件并非由操作系统提供的系统调用。 幸运的是shutil模块提供了copyfile()的函数，你还可以在shutil模块中找到很多实用函数，它们可以看做是os模块的补充。 12345&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isdir(x)]['.lein', '.local', '.m2', '.npm', '.ssh', '.Trash', '.vim', 'Applications', 'Desktop', ...]&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']['apis.py', 'config.py', 'models.py', 'pymonitor.py', 'test_db.py', 'urls.py', 'wsgiapp.py'] 序列化Python提供了pickle模块来实现序列化。 123&gt;&gt;&gt; import pickle&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; pickle.dumps(d) 123&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;wb&apos;)&gt;&gt;&gt; pickle.dump(d, f)&gt;&gt;&gt; f.close() 12345&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;rb&apos;)&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d&#123;&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;&#125; JSON JSON类型 Python类型 {} dict [] list “string” str 1234.56 int或float true/false True/False null None 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; json.dumps(d)&apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos; 123&gt;&gt;&gt; json_str = &apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos;&gt;&gt;&gt; json.loads(json_str)&#123;&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;&#125; Python的对象序列化：需要为类专门写一个转换函数，再把函数传进去即可 12345678def student2dict(std): return &#123; 'name': std.name, 'age': std.age, 'score': std.score &#125;&gt;&gt;&gt; print(json.dumps(s, default=student2dict))&#123;"age": 20, "name": "Bob", "score": 88&#125; 进程和线程多进程Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid)) multiprocessingmultiprocessing模块就是跨平台版本的多进程模块。 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.') Pool如果要启动大量的子进程，可以用进程池的方式批量创建子进程： 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。 子进程很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。 subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。 12345678910import subprocessprint('$ nslookup www.python.org')r = subprocess.call(['nslookup', 'www.python.org'])print('Exit code:', r)# 运行结果：$ nslookup www.python.orgServer: 192.168.19.4Address: 192.168.19.4#53 如果子进程还需要输入，则可以通过communicate()方法输入： 1234567import subprocessprint('$ nslookup')p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b'set q=mx\npython.org\nexit\n')print(output.decode('utf-8'))print('Exit code:', p.returncode) 进程间通信multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。 123456789101112# 父进程创建Queue，并传给各个子进程：q = Queue()pw = Process(target=write, args=(q,))pr = Process(target=read, args=(q,))# 启动子进程pw，写入:pw.start()# 启动子进程pr，读取:pr.start()# 等待pw结束:pw.join()# pr进程里是死循环，无法等待其结束，只能强行终止:pr.terminate() 多线程Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name)print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name) 主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2…… Lock创建一个锁就是通过threading.Lock()来实现： 12345678lock = threading.Lock()lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 多核CPU12345678910import threading, multiprocessingdef loop(): x = 0 while True: x = x ^ 1for i in range(multiprocessing.cpu_count()): t = threading.Thread(target=loop) t.start() 动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%。 因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 ThreadLocal12# 创建全局ThreadLocal对象:local_school = threading.local 进程 vs. 线程 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。 多进程模式的缺点是创建进程的代价大 多线程模式通常比多进程快一点，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。 线程切换 分布式进程在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。 Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。 12345678910111213141516171819202122232425262728293031323334353637# task_master.pyimport random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 放几个任务进去:for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 从result队列读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') 正则表达式 \d可以匹配一个数字， \w可以匹配一个字母或数字 \s可以匹配一个空格（也包括Tab等空白符） &#39;00\d&#39;可以匹配&#39;007&#39;，但无法匹配&#39;00A&#39;； &#39;\d\d\d&#39;可以匹配&#39;010&#39;； .可以匹配任意字符 *表示任意个字符（包括0个） +表示至少一个字符 ?表示0个或1个字符 {n}表示n个字符 {n,m}表示n-m个字符 \d{3}\s+\d{3,8}以任意个空格隔开的带区号的电话号码。 []表示范围 [0-9a-zA-Z\_]可以匹配一个数字、字母或者下划线； A|B可以匹配A或B， (P|p)ython可以匹配&#39;Python&#39;或者python ^表示行的开头 $表示行的结束 re模块12345&gt;&gt;&gt; import re&gt;&gt;&gt; re.match(r&apos;^\d&#123;3&#125;\-\d&#123;3,8&#125;$&apos;, &apos;010-12345&apos;)&lt;_sre.SRE_Match object; span=(0, 9), match=&apos;010-12345&apos;&gt;&gt;&gt;&gt; re.match(r&apos;^\d&#123;3&#125;\-\d&#123;3,8&#125;$&apos;, &apos;010 12345&apos;)&gt;&gt;&gt; 123456&gt;&gt;&gt; &apos;a b c&apos;.split(&apos; &apos;)[&apos;a&apos;, &apos;b&apos;, &apos;&apos;, &apos;&apos;, &apos;c&apos;]&gt;&gt;&gt; re.split(r&apos;\s+&apos;, &apos;a b c&apos;)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]&gt;&gt;&gt; re.split(r&apos;[\s\,]+&apos;, &apos;a,b, c d&apos;)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;] 分组123456789&gt;&gt;&gt; m = re.match(r'^(\d&#123;3&#125;)-(\d&#123;3,8&#125;)$', '010-12345')&gt;&gt;&gt; m&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; m.group(0)'010-12345'&gt;&gt;&gt; m.group(1)'010'&gt;&gt;&gt; m.group(2)'12345' ()表示的就是要提取的分组（Group） 贪婪匹配正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符 12&gt;&gt;&gt; re.match(r&apos;^(\d+)(0*)$&apos;, &apos;102300&apos;).groups()(&apos;102300&apos;, &apos;&apos;) 由于\d+采用贪婪匹配，直接把后面的0全部匹配了 加个?就可以让\d+采用非贪婪匹配： 12&gt;&gt;&gt; re.match(r&apos;^(\d+?)(0*)$&apos;, &apos;102300&apos;).groups()(&apos;1023&apos;, &apos;00&apos;) 编译当我们在Python中使用正则表达式时，re模块内部会干两件事情： 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配： 12345678&gt;&gt;&gt; import re# 编译:&gt;&gt;&gt; re_telephone = re.compile(r&apos;^(\d&#123;3&#125;)-(\d&#123;3,8&#125;)$&apos;)# 使用：&gt;&gt;&gt; re_telephone.match(&apos;010-12345&apos;).groups()(&apos;010&apos;, &apos;12345&apos;)&gt;&gt;&gt; re_telephone.match(&apos;010-8086&apos;).groups()(&apos;010&apos;, &apos;8086&apos;) 字符 说明 \ 将下一字符标记为特殊字符、文本、反向引用或八进制转义符。例如，”n”匹配字符”n”。”\n”匹配换行符。序列”\\“匹配”\”，”\(“匹配”(“。 ^ 匹配输入字符串开始的位置。如果设置了 RegExp 对象的 Multiline 属性，^ 还会与”\n”或”\r”之后的位置匹配。 $ 匹配输入字符串结尾的位置。如果设置了 RegExp 对象的 Multiline 属性，$ 还会与”\n”或”\r”之前的位置匹配。 * 零次或多次匹配前面的字符或子表达式。例如，zo 匹配”z”和”zoo”。 等效于 {0,}。 + 一次或多次匹配前面的字符或子表达式。例如，”zo+”与”zo”和”zoo”匹配，但与”z”不匹配。+ 等效于 {1,}。 ? 零次或一次匹配前面的字符或子表达式。例如，”do(es)?”匹配”do”或”does”中的”do”。? 等效于 {0,1}。 {n} n 是非负整数。正好匹配 n 次。例如，”o{2}”与”Bob”中的”o”不匹配，但与”food”中的两个”o”匹配。 {n,} n是非负整数。至少匹配 n次。例如，”o{2,}”不匹配”Bob”中的”o”，而匹配”foooood”中的所有 o。”o{1,}”等效于”o+”。”o{0,}”等效于”o*”。 {n,m} M和 n 是非负整数，其中 n &lt;= m。匹配至少 n 次，至多 m 次。例如，”o{1,3}”匹配”fooooood”中的头三个 o。’o{0,1}’ 等效于 ‘o?’。注意：您不能将空格插入逗号和数字之间。 ? 当此字符紧随任何其他限定符（、+、?、{n}、{n,}、{n,m}）之后时，匹配模式是”非贪心的”。”非贪心的”模式匹配搜索到的、尽可能短的字符串，而默认的”贪心的”模式匹配搜索到的、尽可能长的字符串。例如，在字符串”oooo”中，”o+?”只匹配单个”o”，而”o+”匹配所有”o”。 . 匹配除”\r\n”之外的任何单个字符。若要匹配包括”\r\n”在内的任意字符，请使用诸如”[\s\S]”之类的模式。 (pattern) 匹配 pattern 并捕获该匹配的子表达式。可以使用 $0…$9 属性从结果”匹配”集合中检索捕获的匹配。若要匹配括号字符 ( )，请使用”(“或者”)“。 (?:pattern) 匹配 pattern 但不捕获该匹配的子表达式，即它是一个非捕获匹配，不存储供以后使用的匹配。这对于用”or”字符 (\ ) 组合模式部件的情况很有用。例如，’industr(?:y\ ies) 是比 ‘industry\ industries’ 更经济的表达式。 (?=pattern) 执行正向预测先行搜索的子表达式，该表达式匹配处于匹配 pattern 的字符串的起始点的字符串。它是一个非捕获匹配，即不能捕获供以后使用的匹配。例如，’Windows (?=95\ 98\ NT\ 2000)’ 匹配”Windows 2000”中的”Windows”，但不匹配”Windows 3.1”中的”Windows”。预测先行不占用字符，即发生匹配后，下一匹配的搜索紧随上一匹配之后，而不是在组成预测先行的字符后。 (?!pattern) 执行反向预测先行搜索的子表达式，该表达式匹配不处于匹配 pattern 的字符串的起始点的搜索字符串。它是一个非捕获匹配，即不能捕获供以后使用的匹配。例如，’Windows (?!95\ 98\ NT\ 2000)’ 匹配”Windows 3.1”中的 “Windows”，但不匹配”Windows 2000”中的”Windows”。预测先行不占用字符，即发生匹配后，下一匹配的搜索紧随上一匹配之后，而不是在组成预测先行的字符后。 x\ y 匹配 x 或 y。例如，’z\ food’ 匹配”z”或”food”。’(z\ f)ood’ 匹配”zood”或”food”。 [xyz] 字符集。匹配包含的任一字符。例如，”[abc]”匹配”plain”中的”a”。 [^xyz] 反向字符集。匹配未包含的任何字符。例如，”[^abc]”匹配”plain”中”p”，”l”，”i”，”n”。 [a-z] 字符范围。匹配指定范围内的任何字符。例如，”[a-z]”匹配”a”到”z”范围内的任何小写字母。 [^a-z] 反向范围字符。匹配不在指定的范围内的任何字符。例如，”[^a-z]”匹配任何不在”a”到”z”范围内的任何字符。 \b 匹配一个字边界，即字与空格间的位置。例如，”er\b”匹配”never”中的”er”，但不匹配”verb”中的”er”。 \B 非字边界匹配。”er\B”匹配”verb”中的”er”，但不匹配”never”中的”er”。 \cx 匹配 x 指示的控制字符。例如，\cM 匹配 Control-M 或回车符。x 的值必须在 A-Z 或 a-z 之间。如果不是这样，则假定 c 就是”c”字符本身。 \d 数字字符匹配。等效于 [0-9]。 \D 非数字字符匹配。等效于 [^0-9]。 \f 换页符匹配。等效于 \x0c 和 \cL。 \n 换行符匹配。等效于 \x0a 和 \cJ。 \r 匹配一个回车符。等效于 \x0d 和 \cM。 \s 匹配任何空白字符，包括空格、制表符、换页符等。与 [ \f\n\r\t\v] 等效。 \S 匹配任何非空白字符。与 [^ \f\n\r\t\v] 等效。 \t 制表符匹配。与 \x09 和 \cI 等效。 \v 垂直制表符匹配。与 \x0b 和 \cK 等效。 \w 匹配任何字类字符，包括下划线。与”[A-Za-z0-9_]”等效。 \W 与任何非单词字符匹配。与”[^A-Za-z0-9_]”等效。 \xn 匹配 n，此处的 n 是一个十六进制转义码。十六进制转义码必须正好是两位数长。例如，”\x41”匹配”A”。”\x041”与”\x04”&amp;”1”等效。允许在正则表达式中使用 ASCII 代码。 *num* 匹配 num，此处的 num 是一个正整数。到捕获匹配的反向引用。例如，”(.)\1”匹配两个连续的相同字符。 *n* 标识一个八进制转义码或反向引用。如果 *n 前面至少有 n 个捕获子表达式，那么 n 是反向引用。否则，如果 n是八进制数 (0-7)，那么 n* 是八进制转义码。 *nm* 标识一个八进制转义码或反向引用。如果 *nm 前面至少有 nm 个捕获子表达式，那么 nm 是反向引用。如果 \nm前面至少有 n 个捕获，则 n 是反向引用，后面跟有字符 m。如果两种前面的情况都不存在，则 \nm 匹配八进制值nm，其中 n 和 m* 是八进制数字 (0-7)。 \nml 当 n 是八进制数 (0-3)，m 和 l 是八进制数 (0-7) 时，匹配八进制转义码 nml。 \un 匹配 n，其中 n 是以四位十六进制数表示的 Unicode 字符。例如，\u00A9 匹配版权符号 (©)。]]></content>
      <categories>
        <category>python</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat servlet容器]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2Fservlet%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[5 servlet容器容器有4种，都继承自org.apache.catalina.Container接口 5.1 Container以下结合容器接口代码，拆分了接口 4种容器： Engine 表示整个Catalina servlet引擎 Host 表示包含一个或个Context的虚拟主机 Context 表示一个web应用，包含多个Wrapper Wrapper 表示一个独立的servlet 这4种容器都有各自实现的类，分别是 StandardXXX，在org.apache.catalina.core内 每一个上层容器都包含多个下层子容器 1234567/** 父子容器关系，接口 */public void addChild(Container child);public void removeChild(Container child);public Container findChild(String name);public Container[] findChildren();public Container getParent();public void setParent(Container container); 容器可以包含一些组件，如载入器、记录器、管理器、域和Resource，暂且可以忽略 12345678public Log getLogger();public void setRealm(Realm realm);public Realm getRealm();public ClassLoader getParentClassLoader();public void setParentClassLoader(ClassLoader parent);public AccessLog getAccessLog();public void setCluster(Cluster cluster);public Cluster getCluster(); 在部署应用时，可以通过配置server.xml来决定使用哪种容器。这是通过管道（pipeline）和阀值（valve）的实现。 5.1.1 Lifecycle接口Container 继承自 Lifecycle，所以每一个Container都是一个组件，有组件的周期 ，下面是Lifecycle的接口 Lifecycle：Catalina components may implement this interface in order to provide a consistent mechanism to start and stop the component. 代表了一个组件的周期 1234567891011121314151617181920212223242526272829&gt; start()&gt; -----------------------------&gt; | |&gt; | init() |&gt; NEW -»-- INITIALIZING |&gt; | | | | ------------------«-----------------------&gt; | | |auto | | |&gt; | | \|/ start() \|/ \|/ auto auto stop() |&gt; | | INITIALIZED --»-- STARTING_PREP --»- STARTING --»- STARTED --»--- |&gt; | | | | |&gt; | |destroy()| | |&gt; | --»-----«-- ------------------------«-------------------------------- ^&gt; | | | |&gt; | | \|/ auto auto start() |&gt; | | STOPPING_PREP ----»---- STOPPING ------»----- STOPPED -----»-----&gt; | \|/ ^ | ^&gt; | | stop() | | |&gt; | | -------------------------- | |&gt; | | | | |&gt; | | | destroy() destroy() | |&gt; | | FAILED ----»------ DESTROYING ---«----------------- |&gt; | | ^ | |&gt; | | destroy() | |auto |&gt; | --------»----------------- \|/ |&gt; | DESTROYED |&gt; | |&gt; | stop() |&gt; ----»-----------------------------»------------------------------&gt; 5.1.2 ContainerBase LifecycleBase和其接口在第六章描述 LifecycleMBeanBase实现了JmxEnable和Container接口 5.1.2.1 Container接口主要对父子容器的操作，监听，以及管道，阀等组件 5.1.2.2 JmxEnable接口用于JMX的管理 12345public interface JmxEnabled extends MBeanRegistration &#123; String getDomain(); void setDomain(String domain); ObjectName getObjectName();&#125; 5.1.2.3 ContainerBase 实现ContainerBase 主要处理4个公共组件，cluster，childContainer，pipeLine和Realm，对于childContainer使用线程池操作，线程池在init时创建，意味着每个容器都有个线程池 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 @Override protected void initInternal() throws LifecycleException &#123; BlockingQueue&lt;Runnable&gt; startStopQueue = new LinkedBlockingQueue&lt;&gt;(); startStopExecutor = new ThreadPoolExecutor( getStartStopThreadsInternal(), getStartStopThreadsInternal(), 10, TimeUnit.SECONDS, startStopQueue, new StartStopThreadFactory(getName() + "-startStop-")); startStopExecutor.allowCoreThreadTimeOut(true); super.initInternal(); &#125; @Override protected synchronized void startInternal() throws LifecycleException &#123; logger = null; getLogger(); Cluster cluster = getClusterInternal(); if (cluster instanceof Lifecycle) &#123; ((Lifecycle) cluster).start(); &#125; Realm realm = getRealmInternal(); if (realm instanceof Lifecycle) &#123; ((Lifecycle) realm).start(); &#125; // Start our child containers, if any Container children[] = findChildren(); List&lt;Future&lt;Void&gt;&gt; results = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; children.length; i++) &#123; results.add(startStopExecutor.submit(new StartChild(children[i]))); &#125; boolean fail = false; for (Future&lt;Void&gt; result : results) &#123; try &#123; result.get(); &#125; catch (Exception e) &#123; log.error(sm.getString("containerBase.threadedStartFailed"), e); fail = true; &#125; &#125; if (fail) &#123; throw new LifecycleException( sm.getString("containerBase.threadedStartFailed")); &#125; // Start the Valves in our pipeline (including the basic), if any if (pipeline instanceof Lifecycle) ((Lifecycle) pipeline).start(); setState(LifecycleState.STARTING); threadStart();// 用thread启动处理backgroundProcessor()方法 &#125;stopInternal类似startInternal 5.2 管道任务联想tomcat的过滤器，一次调用是经过了多个过滤器的，过滤器类似valve，pipeline是装载valve的容器，通常是链表。 pipeline：管道包含该servlet容器将要调用的任务。 valve：一个阀表示一个具体的任务。 在ContainerBase（四大组件的基类）中包含有一个pipeline和HashMap&lt;String, Container&gt; children 的子容器 12345678910111213141516171819202122package org.apache.catalina;public interface Pipeline &#123; public Valve getBasic(); public void setBasic(Valve valve); public void addValve(Valve valve); public Valve[] getValves(); public void removeValve(Valve valve); public Valve getFirst(); public boolean isAsyncSupported(); public Container getContainer(); public void setContainer(Container container); public void findNonAsyncValves(Set&lt;String&gt; result);&#125;package org.apache.catalina;public interface Valve &#123; public Valve getNext(); public void setNext(Valve valve); public void backgroundProcess(); public void invoke(Request request, Response response) throws IOException, ServletException; public boolean isAsyncSupported();&#125; 在调用了servlet的执行方法时，实际上是执行了pipeline中的valve。注意pipeline中包含一个baseValve，不同于valve[]。 5.2.1 ValveContext 实现阀的遍历在tomcat8.5中被淘汰。。。。。。 参考深入剖析Tomcat一书，在低版本中，通过ValveContext实现遍历，ContainerBase和pipeline都有public void invoke(Request request, Response response)的方法签名，ValveContext是Pipeline的内部类，也有该签名，通过层层调用，最后由ValveContext遍历。同时会发现ContainerBase和pipeline也删除了该签名方法 淘汰原因：只想让Container和Pipeline只做容器，而不处理，将处理抽象出来 5.2.2 Contained接口123456package org.apache.catalina;public interface Contained &#123; Container getContainer(); void setContainer(Container container);&#125; ValveBase和PipelineBase均实现该接口 5.3 Wrapper容器Wrapper内部处理流程 5.3.1 Wrapper接口Wrapper实现类主要负责管理基础Servlet类的servlet的生命周期，是最低级的容器。 Wrapper的父容器只能是Context，对子容器操作是会抛异常 为啥是最低的，参看 StandardXXX.java 5.3.1.1 获取servlet：12public void load() throws ServletException;public Servlet allocate() throws ServletException; allocate()：分配一个已准备好调用其service()方法的Servlet的初始化实例。 load()：如果尚未有一个已初始化的实例，则加载并初始化此Servlet（一对一）的实例。可以在服务器启动时预加载部署描述符中标记的Servlet。 5.3.2 StandardWrapperStandardWrapper主要负责载入它所代表的servlet类，并进行实例化，但是它并不调用servlet的service方法，该方法由StandardWrapperValve对象完成（作为基础阀） 当第一次请求某个servlet时，StandardWrapper载入servlet类，由于会动态载入，因此必须知道完全限定名，可以调用setServletClass设置。 5.3.2.1 方法调用序列方法的调用是通过管道调用的，基础阀最后调用，基础阀会调用子容器的管道 5.3.2.2 load如果还没有至少一个初始化实例，则加载并初始化此servlet的一个实例。 load调用了loadServlet() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public synchronized Servlet loadServlet() throws ServletException &#123; // Nothing to do if we already have an instance or an instance pool if (!singleThreadModel &amp;&amp; (instance != null)) return instance; PrintStream out = System.out; if (swallowOutput) &#123; SystemLogHandler.startCapture(); &#125; Servlet servlet; try &#123; long t1=System.currentTimeMillis(); // Complain if no servlet class has been specified if (servletClass == null) &#123; unavailable(null); throw new ServletException (sm.getString("standardWrapper.notClass", getName())); &#125; InstanceManager instanceManager = ((StandardContext)getParent()).getInstanceManager(); try &#123; servlet = (Servlet) instanceManager.newInstance(servletClass); &#125; catch (ClassCastException e) &#123; unavailable(null); //... &#125; catch (Throwable e) &#123; unavailable(null); //... &#125; if (multipartConfigElement == null) &#123; MultipartConfig annotation = servlet.getClass().getAnnotation(MultipartConfig.class); if (annotation != null) &#123; multipartConfigElement = new MultipartConfigElement(annotation); &#125; &#125; processServletSecurityAnnotation(servlet.getClass()); if (servlet instanceof ContainerServlet) &#123; ((ContainerServlet) servlet).setWrapper(this); &#125; classLoadTime=(int) (System.currentTimeMillis() -t1); if (servlet instanceof SingleThreadModel) &#123; if (instancePool == null) &#123; instancePool = new Stack&lt;&gt;(); &#125; singleThreadModel = true; &#125; initServlet(servlet); fireContainerEvent("load", this); loadTime=System.currentTimeMillis() -t1; &#125; finally &#123; if (swallowOutput) &#123; //log &#125; &#125; return servlet;&#125; 5.3.2.2 allocate分配一个已经准备好调用service（）方法的Servlet的初始化实例。 如果servlet类没有实现SingleThreadModel，则（仅）已初始化的实例可能会立即返回。 如果servlet类实现了SingleThreadModel，那么Wrapper实现必须确保这个实例在被deallocate（）调用解除分配之前不会被再次分配。 SingleThreadModel防止一个servlet的service同时被多个线程调用，通过synchronized和池化处理。请注意，SingleThreadModel不能解决所有线程安全问题。 例如，即使在使用SingleThreadModel servlet的时候，会话属性和静态变量仍然可以同时被多个线程上的多个请求访问。该接口已被淘汰 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Overridepublic Servlet allocate() throws ServletException &#123; // If we are currently unloading this servlet, throw an exception if (unloading) &#123; throw new ServletException(sm.getString("standardWrapper.unloading", getName())); &#125; boolean newInstance = false; // If not SingleThreadedModel, return the same instance every time if (!singleThreadModel) &#123; // Load and initialize our instance if necessary if (instance == null || !instanceInitialized) &#123; synchronized (this) &#123; if (instance == null) &#123; try &#123; // Note: We don't know if the Servlet implements // SingleThreadModel until we have loaded it. instance = loadServlet(); newInstance = true; if (!singleThreadModel) &#123; countAllocated.incrementAndGet(); &#125; &#125; catch (ServletException e) &#123; //... &#125; &#125; if (!instanceInitialized) &#123; initServlet(instance); &#125; &#125; &#125; if (singleThreadModel) &#123; if (newInstance) &#123; // Have to do this outside of the sync above to prevent a // possible deadlock synchronized (instancePool) &#123; instancePool.push(instance); nInstances++; &#125; &#125; &#125; else &#123; // For new instances, count will have been incremented at the // time of creation if (!newInstance) &#123; countAllocated.incrementAndGet(); &#125; return instance; &#125; &#125; synchronized (instancePool) &#123; while (countAllocated.get() &gt;= nInstances) &#123; // Allocate a new instance if possible, or else wait if (nInstances &lt; maxInstances) &#123; try &#123; instancePool.push(loadServlet()); nInstances++; &#125; catch (ServletException e) &#123; //... &#125; &#125; else &#123; try &#123; instancePool.wait(); &#125; catch (InterruptedException e) &#123; // Ignore &#125; &#125; &#125; countAllocated.incrementAndGet(); return instancePool.pop(); &#125;&#125; countAllocated.incrementAndGet() 用来计算分配次数 对于非singleThreadModel采用单例模式，但由于第一次加载不知道singleThreadModel，要特殊处理 对于singleThreadModel采用对象池，利用了wait，signal 对于singleThreadModel的回收 1234567891011121314@Overridepublic void deallocate(Servlet servlet) throws ServletException &#123; // If not SingleThreadModel, no action is required if (!singleThreadModel) &#123; countAllocated.decrementAndGet(); return; &#125; // Unlock and free this instance synchronized (instancePool) &#123; countAllocated.decrementAndGet(); instancePool.push(servlet); instancePool.notify(); &#125;&#125; 5.3.2.2 ServletConfig与初始化一个servlet配置对象，由servlet容器在初始化期间将信息传递给servlet。servlet初始化需要一个ServletConfig servlet.init(facade);为了安全传入个外观类， 初始化时示例：init() in GenericServlet 123&gt; if (getServletConfig().getInitParameter("input") != null)&gt; input = Integer.parseInt(getServletConfig().getInitParameter("input"));&gt; ​ 123456public interface ServletConfig &#123; public String getServletName(); public ServletContext getServletContext(); public String getInitParameter(String name); public Enumeration&lt;String&gt; getInitParameterNames();&#125; 在StandardWrapper中 12345678910111213141516171819202122232425262728293031// wrapper name同servletNamepublic String getServletName() &#123; return (getName());&#125;@Overridepublic ServletContext getServletContext() &#123; if (parent == null) return (null); else if (!(parent instanceof Context)) return (null); else return (((Context) parent).getServletContext());&#125;@Overridepublic String getInitParameter(String name) &#123; return (findInitParameter(name));&#125; @Overridepublic Enumeration&lt;String&gt; getInitParameterNames() &#123; parametersLock.readLock().lock(); try &#123; return Collections.enumeration(parameters.keySet()); &#125; finally &#123; parametersLock.readLock().unlock(); &#125;&#125; ​ 5.4.3 StandardWrapperFacade只是用来构建Servlet的，简化版StandardWrapper 123456789101112131415161718192021222324252627282930public final class StandardWrapperFacade implements ServletConfig &#123; public StandardWrapperFacade(StandardWrapper config) &#123; super(); this.config = config; &#125; private final ServletConfig config; private ServletContext context = null; @Override public String getServletName() &#123; return config.getServletName(); &#125; @Override public ServletContext getServletContext() &#123; if (context == null) &#123; context = config.getServletContext(); if (context instanceof ApplicationContext) &#123; context = ((ApplicationContext) context).getFacade(); &#125; &#125; return (context); &#125; @Override public String getInitParameter(String name) &#123; return config.getInitParameter(name); &#125; @Override public Enumeration&lt;String&gt; getInitParameterNames() &#123; return config.getInitParameterNames(); &#125;&#125; 5.4.4 StandardWrapperValveStandardWrapperValve是StandardWrapper的基础阀，wrapper的最后一个valve，要完成两个操作： 执行与该servlet关联的全部过滤器 调用servlet的service方法 StandardWrapperValve的实现主要是invoke方法 调用StandardWrapper的allocate获取servlet 调用ApplicationFilterFactory.createFilterChain创建过滤器链 调用过滤器链的doFilter，其中包括servlet的service方法 释放filterChain 调用Wrapper的deallocate 若servlet不会再用，调用Wrapper的unload() 源码分部解析 校验父容器的有效性，以及Wrapper分配servlet，处理该过程的异常，异常会response.sendError，发送错误相应给客户端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public final void invoke(Request request, Response response) throws IOException, ServletException &#123; // Initialize local variables we may need boolean unavailable = false; Throwable throwable = null; // This should be a Request attribute... long t1=System.currentTimeMillis(); requestCount.incrementAndGet(); StandardWrapper wrapper = (StandardWrapper) getContainer(); Servlet servlet = null; Context context = (Context) wrapper.getParent(); // Check for the application being marked unavailable if (!context.getState().isAvailable()) &#123; response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, sm.getString("standardContext.isUnavailable")); unavailable = true; &#125; // Check for the servlet being marked unavailable if (!unavailable &amp;&amp; wrapper.isUnavailable()) &#123; container.getLogger().info(sm.getString("standardWrapper.isUnavailable", wrapper.getName())); long available = wrapper.getAvailable(); if ((available &gt; 0L) &amp;&amp; (available &lt; Long.MAX_VALUE)) &#123; response.setDateHeader("Retry-After", available); response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, sm.getString("standardWrapper.isUnavailable", wrapper.getName())); &#125; else if (available == Long.MAX_VALUE) &#123; response.sendError(HttpServletResponse.SC_NOT_FOUND, sm.getString("standardWrapper.notFound", wrapper.getName())); &#125; unavailable = true; &#125; // Allocate a servlet instance to process this request try &#123; if (!unavailable) &#123; servlet = wrapper.allocate(); &#125; &#125; catch (UnavailableException e) &#123; container.getLogger().error( sm.getString("standardWrapper.allocateException", wrapper.getName()), e); long available = wrapper.getAvailable(); if ((available &gt; 0L) &amp;&amp; (available &lt; Long.MAX_VALUE)) &#123; response.setDateHeader("Retry-After", available); response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, sm.getString("standardWrapper.isUnavailable", wrapper.getName())); &#125; else if (available == Long.MAX_VALUE) &#123; response.sendError(HttpServletResponse.SC_NOT_FOUND, sm.getString("standardWrapper.notFound", wrapper.getName())); &#125; &#125; catch (ServletException e) &#123; container.getLogger().error(sm.getString("standardWrapper.allocateException", wrapper.getName()), StandardWrapper.getRootCause(e)); throwable = e; exception(request, response, e); &#125; catch (Throwable e) &#123; ExceptionUtils.handleThrowable(e); container.getLogger().error(sm.getString("standardWrapper.allocateException", wrapper.getName()), e); throwable = e; exception(request, response, e); servlet = null; &#125; ​ 创建过滤器链 123456789MessageBytes requestPathMB = request.getRequestPathMB();DispatcherType dispatcherType = DispatcherType.REQUEST;if (request.getDispatcherType()==DispatcherType.ASYNC) dispatcherType = DispatcherType.ASYNC;request.setAttribute(Globals.DISPATCHER_TYPE_ATTR,dispatcherType);request.setAttribute(Globals.DISPATCHER_REQUEST_PATH_ATTR, requestPathMB);// Create the filter chain for this requestApplicationFilterChain filterChain = ApplicationFilterFactory.createFilterChain(request, wrapper, servlet); dofilter 在filterChain.doFilter时有很多异常，还分为异步同步处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475try &#123; if ((servlet != null) &amp;&amp; (filterChain != null)) &#123; // Swallow output if needed if (context.getSwallowOutput()) &#123; try &#123; SystemLogHandler.startCapture(); if (request.isAsyncDispatching()) &#123; request.getAsyncContextInternal().doInternalDispatch(); &#125; else &#123; filterChain.doFilter(request.getRequest(), response.getResponse()); &#125; &#125; finally &#123; String log = SystemLogHandler.stopCapture(); if (log != null &amp;&amp; log.length() &gt; 0) &#123; context.getLogger().info(log); &#125; &#125; &#125; else &#123; if (request.isAsyncDispatching()) &#123; request.getAsyncContextInternal().doInternalDispatch(); &#125; else &#123; filterChain.doFilter (request.getRequest(), response.getResponse()); &#125; &#125; &#125;&#125; catch (ClientAbortException e) &#123; throwable = e; exception(request, response, e);&#125; catch (IOException e) &#123; container.getLogger().error(sm.getString( "standardWrapper.serviceException", wrapper.getName(), context.getName()), e); throwable = e; exception(request, response, e);&#125; catch (UnavailableException e) &#123; container.getLogger().error(sm.getString( "standardWrapper.serviceException", wrapper.getName(), context.getName()), e); // throwable = e; // exception(request, response, e); wrapper.unavailable(e); long available = wrapper.getAvailable(); if ((available &gt; 0L) &amp;&amp; (available &lt; Long.MAX_VALUE)) &#123; response.setDateHeader("Retry-After", available); response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, sm.getString("standardWrapper.isUnavailable", wrapper.getName())); &#125; else if (available == Long.MAX_VALUE) &#123; response.sendError(HttpServletResponse.SC_NOT_FOUND, sm.getString("standardWrapper.notFound", wrapper.getName())); &#125; // Do not save exception in 'throwable', because we // do not want to do exception(request, response, e) processing&#125; catch (ServletException e) &#123; Throwable rootCause = StandardWrapper.getRootCause(e); if (!(rootCause instanceof ClientAbortException)) &#123; container.getLogger().error(sm.getString( "standardWrapper.serviceExceptionRoot", wrapper.getName(), context.getName(), e.getMessage()), rootCause); &#125; throwable = e; exception(request, response, e);&#125; catch (Throwable e) &#123; ExceptionUtils.handleThrowable(e); container.getLogger().error(sm.getString( "standardWrapper.serviceException", wrapper.getName(), context.getName()), e); throwable = e; exception(request, response, e);&#125; 释放过滤器 1234// Release the filter chain (if any) for this requestif (filterChain != null) &#123; filterChain.release();&#125; 回收servlet 参考Wrapper中的实现， 1234567891011121314// Deallocate the allocated servlet instancetry &#123; if (servlet != null) &#123; wrapper.deallocate(servlet); &#125;&#125; catch (Throwable e) &#123; ExceptionUtils.handleThrowable(e); container.getLogger().error(sm.getString("standardWrapper.deallocateException", wrapper.getName()), e); if (throwable == null) &#123; throwable = e; exception(request, response, e); &#125;&#125; 若servlet不会再用，调用Wrapper的unload()，getAvailable代表下次可用时间，后面的time提供统计信息 12345678910111213141516171819202122// If this servlet has been marked permanently unavailable,// unload it and release this instancetry &#123; if ((servlet != null) &amp;&amp; (wrapper.getAvailable() == Long.MAX_VALUE)) &#123; wrapper.unload(); &#125;&#125; catch (Throwable e) &#123; ExceptionUtils.handleThrowable(e); container.getLogger().error(sm.getString("standardWrapper.unloadException", wrapper.getName()), e); if (throwable == null) &#123; throwable = e; exception(request, response, e); &#125;&#125;long t2=System.currentTimeMillis();long time=t2-t1;processingTime += time;if( time &gt; maxTime) maxTime=time;if( time &lt; minTime) minTime=time; 5.4.4 过滤器5.4.4.1 FilterDeforg.apache.tomcat.util.descriptor.web.FilterDef表示一个Filter的定义，as represented in a &lt;filter&gt; element in the deployment descriptor parameters表示该过滤器的初始化参数 5.4.4.2 FilterConfig过滤器配置对象，由servlet容器在初始化期间将信息传递给过滤器。实现了Def和Context的关联 ApplicationFilterConfig是其直接实现类 构造函数由Context和FilterDef组成 instanceManager管理器，创建和销毁Filter实例 getFilter用instanceManager创建，再初始化 初始化用filter.init(this); 5.4.4.3 Filter过滤器是一个对资源请求（servlet或静态内容）或资源响应执行过滤任务的对象。过滤器在doFilter方法中执行过滤。 每个过滤器都可以通过一个FilterConfig对象获取其初始化参数，例如，可以使用对ServletContext的引用来加载过滤任务所需的资源。 过滤器是在Web应用程序的部署描述符中配置的 举例： Authentication Filters Logging and Auditing Filters Image conversion Filters Data compression Filters Encryption Filters Tokenizing Filters Filters that trigger resource access events XSL/T filters Mime-type chain Filter 123456public interface Filter &#123; public void init(FilterConfig filterConfig) throws ServletException; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException; public void destroy();&#125; doFilter方法的典型实现将遵循以下模式： 检查请求 包装请求对象,筛选内容或标题 自定义包装响应对象 a）使用FilterChain对象chain.doFilter调用链中的下一个实体， b）或者不将请求/响应对传递给过滤器链中的下一个实体以阻止请求处理 在调用过滤器链中的下一个实体之后，在响应上直接设置header。 5.4.4.4 FilterChain123public interface FilterChain &#123; public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException;&#125; ApplicationFilterChain是其实现子类 doFilter最后会调用servlet的servlet方法 lastServicedRequest和lastServicedResponse是线程本地变量，在一次请求中的线程是不会变的。 FilterChain保存的链是ApplicationFilterConfig对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091@Overridepublic void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException &#123; if( Globals.IS_SECURITY_ENABLED ) &#123; final ServletRequest req = request; final ServletResponse res = response; try &#123; java.security.AccessController.doPrivileged( new java.security.PrivilegedExceptionAction&lt;Void&gt;() &#123; @Override public Void run() throws ServletException, IOException &#123; internalDoFilter(req,res); return null; &#125; &#125; ); &#125; catch( PrivilegedActionException pe) &#123; //... &#125; &#125; else &#123; internalDoFilter(request,response); &#125;&#125;private void internalDoFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException &#123; // Call the next filter if there is one if (pos &lt; n) &#123; ApplicationFilterConfig filterConfig = filters[pos++]; try &#123; Filter filter = filterConfig.getFilter(); if (request.isAsyncSupported() &amp;&amp; "false".equalsIgnoreCase( filterConfig.getFilterDef().getAsyncSupported())) &#123; request.setAttribute(Globals.ASYNC_SUPPORTED_ATTR, Boolean.FALSE); &#125; if( Globals.IS_SECURITY_ENABLED ) &#123; final ServletRequest req = request; final ServletResponse res = response; Principal principal = ((HttpServletRequest) req).getUserPrincipal(); Object[] args = new Object[]&#123;req, res, this&#125;; SecurityUtil.doAsPrivilege ("doFilter", filter, classType, args, principal); &#125; else &#123; filter.doFilter(request, response, this); &#125; &#125; catch (IOException | ServletException | RuntimeException e) &#123; //... &#125; return; &#125; // We fell off the end of the chain -- call the servlet instance try &#123; if (ApplicationDispatcher.WRAP_SAME_OBJECT) &#123; lastServicedRequest.set(request); lastServicedResponse.set(response); &#125; if (request.isAsyncSupported() &amp;&amp; !servletSupportsAsync) &#123; request.setAttribute(Globals.ASYNC_SUPPORTED_ATTR, Boolean.FALSE); &#125; // Use potentially wrapped request from this point if ((request instanceof HttpServletRequest) &amp;&amp; (response instanceof HttpServletResponse) &amp;&amp; Globals.IS_SECURITY_ENABLED ) &#123; final ServletRequest req = request; final ServletResponse res = response; Principal principal = ((HttpServletRequest) req).getUserPrincipal(); Object[] args = new Object[]&#123;req, res&#125;; SecurityUtil.doAsPrivilege("service", servlet, classTypeUsedInService, args, principal); &#125; else &#123; servlet.service(request, response); &#125; &#125; catch (IOException | ServletException | RuntimeException e) &#123; //... &#125; finally &#123; if (ApplicationDispatcher.WRAP_SAME_OBJECT) &#123; lastServicedRequest.set(null); lastServicedResponse.set(null); &#125; &#125;&#125; 5.4.5.5 过滤器链的创建利用ApplicationFilterFactory创建 filter配置实例：url-pattern是必选的 FilterConfig对象保存在context的filterConfigs中，通过匹配DispatcherType和servlet-name和url匹配，在33和49行。FilterMap和xml的属性基本一样 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public static ApplicationFilterChain createFilterChain(ServletRequest request, Wrapper wrapper, Servlet servlet) &#123; if (servlet == null) return null; // Create and initialize a filter chain object ApplicationFilterChain filterChain = null; //略去处理 filterChain = new ApplicationFilterChain(); filterChain.setServlet(servlet); filterChain.setServletSupportsAsync(wrapper.isAsyncSupported()); // Acquire the filter mappings for this Context StandardContext context = (StandardContext) wrapper.getParent(); FilterMap filterMaps[] = context.findFilterMaps(); // If there are no filter mappings, we are done if ((filterMaps == null) || (filterMaps.length == 0)) return (filterChain); // Acquire the information we will need to match filter mappings DispatcherType dispatcher = (DispatcherType) request.getAttribute(Globals.DISPATCHER_TYPE_ATTR); String requestPath = null; Object attribute = request.getAttribute(Globals.DISPATCHER_REQUEST_PATH_ATTR); if (attribute != null)&#123; requestPath = attribute.toString(); &#125; String servletName = wrapper.getName(); // Add the relevant path-mapped filters to this filter chain for (int i = 0; i &lt; filterMaps.length; i++) &#123; if (!matchDispatcher(filterMaps[i] ,dispatcher)) &#123; continue; &#125; if (!matchFiltersURL(filterMaps[i], requestPath)) continue; ApplicationFilterConfig filterConfig = (ApplicationFilterConfig) context.findFilterConfig(filterMaps[i].getFilterName()); if (filterConfig == null) &#123; // FIXME - log configuration problem continue; &#125; filterChain.addFilter(filterConfig); &#125; // Add filters that match on servlet name second for (int i = 0; i &lt; filterMaps.length; i++) &#123; if (!matchDispatcher(filterMaps[i] ,dispatcher)) &#123; continue; &#125; if (!matchFiltersServlet(filterMaps[i], servletName)) continue; ApplicationFilterConfig filterConfig = (ApplicationFilterConfig) context.findFilterConfig(filterMaps[i].getFilterName()); if (filterConfig == null) &#123; // FIXME - log configuration problem continue; &#125; filterChain.addFilter(filterConfig); &#125; // Return the completed filter chain return filterChain;&#125; 思考 既然SingleThreadModel无法保证线程安全，那如何处理？ 编写无状态的代码 servlet和filter关系 filter过滤servlet，但两者都是独立的，创建FilterChain都是动态的，用完销毁，但filter和servlet对象都缓存在FilterConfig和Wrapper中，创建FilterChain的效率是很高的 5.4 Context容器Context容器代表一个应用程序的 5.4.1 Context接口Context实例代表一个具体的web应用程序，其中包含一个或者多个Wrapper实例。但是Context还支持其他组件，如Sessiong管理器，ClassLoader等 主要定义了一些子组件，监听器，状态变量的增删改查等 注意这里的监听器是String类型，实际上是class name的 servlet名称映射 1234567private HashMap&lt;String, ApplicationFilterConfig&gt; filterConfigs = new HashMap&lt;&gt;(); //名称-Conf映射，filter缓存在ApplicationFilterConfig中public void addFilterDef(FilterDef filterDef);//&lt;String, FilterDef&gt;，Def与xml配置对应public void addFilterMap(FilterMap filterMap);//数组实现，一个FilterMap保存了一个filterName和一系列对应的servletName的关系。创建filter链的时候通过filterMap内部的url或servletName匹配，匹配成功再通过filterName和filterConfigs获取ApplicationFilterConfig，ApplicationFilterConfig中持有filter对象public void addServletMappingDecoded(String pattern, String name);//url pattern与servletName的映射public void addServletMappingDecoded(String pattern, String name, boolean jspWildcard); ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328public interface Context extends Container, ContextBind &#123; // ----------------------------------------------------- Manifest Constants /** * 事件常量 */ public static final String ADD_WELCOME_FILE_EVENT = "addWelcomeFile"; public static final String REMOVE_WELCOME_FILE_EVENT = "removeWelcomeFile"; public static final String CLEAR_WELCOME_FILES_EVENT = "clearWelcomeFiles"; public static final String CHANGE_SESSION_ID_EVENT = "changeSessionId"; // ------------------------------------------------------------- Properties /** * 是否允许解析Mutipart */ public boolean getAllowCasualMultipartParsing(); public void setAllowCasualMultipartParsing(boolean allowCasualMultipartParsing); /** * app监听器 */ public Object[] getApplicationEventListeners(); public void setApplicationEventListeners(Object listeners[]); /** * Lifecycle监听器 */ public Object[] getApplicationLifecycleListeners(); public void setApplicationLifecycleListeners(Object listeners[]); /** * 获取编码 */ public String getCharset(Locale locale); /** * URL of the XML descriptor */ public URL getConfigFile(); public void setConfigFile(URL configFile); /** * 是否配置过 */ public boolean getConfigured(); public void setConfigured(boolean configured); /** * 是否用cookies */ public boolean getCookies(); public void setCookies(boolean cookies); /** * cookies name */ public String getSessionCookieName(); public void setSessionCookieName(String sessionCookieName); public boolean getUseHttpOnly(); public void setUseHttpOnly(boolean useHttpOnly); /** * Cookie域，可用于跨域访问 */ public String getSessionCookieDomain(); public void setSessionCookieDomain(String sessionCookieDomain); /** * path表示cookie所在的目录 */ public String getSessionCookiePath(); public void setSessionCookiePath(String sessionCookiePath); public boolean getCrossContext(); public void setCrossContext(boolean crossContext); /** * 发布名 */ public String getAltDDName(); public void setAltDDName(String altDDName) ; /** * 展示名 */ public String getDisplayName(); public void setDisplayName(String displayName); /** * 是否是分布式的 */ public boolean getDistributable(); public void setDistributable(boolean distributable); /** * root目录 */ public String getDocBase(); public void setDocBase(String docBase); /** * URL encoded context path */ public String getEncodedPath(); /** * 登陆配置 ，如loginPage，errorPage等 */ public LoginConfig getLoginConfig(); public void setLoginConfig(LoginConfig config); /** * context path for this web application. */ public String getPath(); public void setPath(String path); /** * reload */ public boolean getReloadable(); public void setReloadable(boolean reloadable); /** * override */ public boolean getOverride(); public void setOverride(boolean override); /** * privileged */ public boolean getPrivileged(); public void setPrivileged(boolean privileged); /** * the Servlet context for which this Context is a facade. */ public ServletContext getServletContext(); /** * the default session timeout (in minutes) */ public int getSessionTimeout(); public void setSessionTimeout(int timeout); public String getWrapperClass(); public void setWrapperClass(String wrapperClass); public Authenticator getAuthenticator(); public InstanceManager getInstanceManager(); public void setInstanceManager(InstanceManager instanceManager); // --------------------------------------------------------- Public Methods public void addApplicationListener(String listener); public void addApplicationParameter(ApplicationParameter parameter); public void addErrorPage(ErrorPage errorPage); public void addFilterDef(FilterDef filterDef); public void addFilterMap(FilterMap filterMap); public void addFilterMapBefore(FilterMap filterMap); public void addLocaleEncodingMappingParameter(String locale, String encoding); public void addMimeMapping(String extension, String mimeType); public void addParameter(String name, String value); public void addRoleMapping(String role, String link); public void addSecurityRole(String role); @Deprecated public void addServletMapping(String pattern, String name); @Deprecated public void addServletMapping(String pattern, String name, boolean jspWildcard); public void addServletMappingDecoded(String pattern, String name); public void addServletMappingDecoded(String pattern, String name, boolean jspWildcard); public void addWelcomeFile(String name); /** * Add the classname of a LifecycleListener */ public void addWrapperLifecycle(String listener); /** * Add the classname of a ContainerListener */ public void addWrapperListener(String listener); public Wrapper createWrapper(); public String[] findApplicationListeners(); public ApplicationParameter[] findApplicationParameters(); public ErrorPage findErrorPage(int errorCode); public ErrorPage findErrorPage(String exceptionType); public ErrorPage[] findErrorPages(); public FilterDef findFilterDef(String filterName); public FilterDef[] findFilterDefs(); public FilterMap[] findFilterMaps(); public String findParameter(String name); public String[] findParameters(); /** * @return the servlet name mapped by the specified pattern (if any); */ public String findServletMapping(String pattern); /** * @return the patterns of all defined servlet mappings for this * Context. */ public String[] findServletMappings(); /** * @return the context-relative URI of the error page for the specified * HTTP status code, if any; otherwise return &lt;code&gt;null&lt;/code&gt;. */ public String findStatusPage(int status); /** * @return the set of HTTP status codes for which error pages have * been specified. If none are specified, a zero-length array * is returned. */ public int[] findStatusPages(); /** * @return the associated ThreadBindingListener. */ public ThreadBindingListener getThreadBindingListener(); public void setThreadBindingListener(ThreadBindingListener threadBindingListener); /** * @return the set of watched resources for this Context */ public String[] findWatchedResources(); public boolean findWelcomeFile(String name); public String[] findWelcomeFiles(); public String[] findWrapperLifecycles(); public String[] findWrapperListeners(); /** * Notify all &#123;@link javax.servlet.ServletRequestListener&#125;s that a request * has started. */ public boolean fireRequestInitEvent(ServletRequest request); /** * Notify all &#123;@link javax.servlet.ServletRequestListener&#125;s that a request * has ended. */ public boolean fireRequestDestroyEvent(ServletRequest request); public void reload(); public void removeApplicationListener(String listener); public void removeApplicationParameter(String name); public void removeErrorPage(ErrorPage errorPage); public void removeFilterDef(FilterDef filterDef); public void removeFilterMap(FilterMap filterMap); public void removeMimeMapping(String extension); public void removeParameter(String name); public void removeServletMapping(String pattern); public void removeWatchedResource(String name); public void removeWelcomeFile(String name); public void removeWrapperLifecycle(String listener); public void removeWrapperListener(String listener); /** * @return the real path for a given virtual path */ public String getRealPath(String path); /** * Is this Context paused whilst it is reloaded? */ public boolean getPaused(); /** * @return the base name to use for WARs, directories or context.xml files * for this context. */ public String getBaseName(); public void setWebappVersion(String webappVersion); public String getWebappVersion(); public void setFireRequestListenersOnForwards(boolean enable); public boolean getFireRequestListenersOnForwards(); public void setSendRedirectBody(boolean enable); public boolean getSendRedirectBody(); public Loader getLoader(); public void setLoader(Loader loader); public WebResourceRoot getResources(); public void setResources(WebResourceRoot resources); public Manager getManager(); public void setManager(Manager manager); public void setCookieProcessor(CookieProcessor cookieProcessor); public CookieProcessor getCookieProcessor(); public void setRequestCharacterEncoding(String encoding); public String getRequestCharacterEncoding(); public void setResponseCharacterEncoding(String encoding); public String getResponseCharacterEncoding(); //......&#125; 5.4.2 StandardContextStandardContext是org.apache.catalina.Context的标准实现，继承自ContainerBase基容器，具备容器的功能，包含Wrapper容器，它的角色是管理在其内部的Wrapper，从Host那里接收请求信息，加载实例化Servlet，然后选择一个合适的Wrapper处理，同一个Context内部中Servlet数据共享，不同Context之间数据隔离。在Context层面上，Filter的作用就出来了，Filter是用来过滤Servlet的组件，Context负责在每个Servlet上面调用Filter过滤，Context与开发Servlet息息相关，跟前面的几个组件相比，Context能够提供更多的信息给Servlet。一个webapp有一个Context，Context负责管理在其内部的组件：Servlet，Cookie，Session等等。 5.4.2.1 主要成员5.4.2.2 构造函数1234567891011public StandardContext() &#123; super(); pipeline.setBasic(new StandardContextValve()); broadcaster = new NotificationBroadcasterSupport(); // Set defaults if (!Globals.STRICT_SERVLET_COMPLIANCE) &#123; // Strict servlet compliance requires all extension mapped servlets // to be checked against welcome files resourceOnlyServlets.add("jsp"); &#125;&#125; 5.4.2.3 startInternalstartInternal方法主要完成以下工作： 广播启动事件 broadcaster.sendNotification(notification); 设置configured为false 配置资源 setResources(new StandardRoot(this)); 配置classLoader setLoader(new WebappLoader(getParentClassLoader())); 设置cookieProcessor cookieProcessor = new Rfc6265CookieProcessor(); 初始化字符串映射集合 getCharsetMapper(); 启动相关组件 启动classLoader ((Lifecycle) loader).start() 启动Realm ((Lifecycle) realm).start(); 启动子容器 child.start(); 启动管道 ((Lifecycle) pipeline).start(); 启动SessionManager ContextManager new StandardManager(); ((Lifecycle) manager).start(); 触发start过failed事件 5.4.2.4 stopInternalstopInternal与startInternal相反 5.4.2.5 destroyInternal销毁子组件 5.4.2.6 backgroundProcess调用子组件的backgroundProcess 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public void backgroundProcess() &#123; if (!getState().isAvailable()) return; Loader loader = getLoader(); if (loader != null) &#123; try &#123; loader.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.loader", loader), e); &#125; &#125; Manager manager = getManager(); if (manager != null) &#123; try &#123; manager.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.manager", manager), e); &#125; &#125; WebResourceRoot resources = getResources(); if (resources != null) &#123; try &#123; resources.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.resources", resources), e); &#125; &#125; InstanceManager instanceManager = getInstanceManager(); if (instanceManager instanceof DefaultInstanceManager) &#123; try &#123; ((DefaultInstanceManager)instanceManager).backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.instanceManager", resources), e); &#125; &#125; super.backgroundProcess();&#125; 5.4.2.7 reloadsynchronized同步的，start–&gt;stop 1234567891011121314151617181920212223242526272829303132333435public synchronized void reload() &#123; // Validate our current component state if (!getState().isAvailable()) throw new IllegalStateException (sm.getString("standardContext.notStarted", getName())); if(log.isInfoEnabled()) log.info(sm.getString("standardContext.reloadingStarted", getName())); // Stop accepting requests temporarily. setPaused(true); try &#123; stop(); &#125; catch (LifecycleException e) &#123; log.error( sm.getString("standardContext.stoppingContext", getName()), e); &#125; try &#123; start(); &#125; catch (LifecycleException e) &#123; log.error( sm.getString("standardContext.startingContext", getName()), e); &#125; setPaused(false); if(log.isInfoEnabled()) log.info(sm.getString("standardContext.reloadingCompleted", getName()));&#125; 5.4.3 WebResourceRoot表示Web应用程序的完整资源集合。 Web应用程序的资源由多个资源集组成，当查找资源时，ResourceSets按以下顺序处理： Pre - 由Web应用程序的context.xml中的元素定义的资源。 资源将按照指定的顺序进行搜索。 Main - Web应用程序的主要资源 - 即WAR或包含展开的WAR的目录 JAR - 由Servlet规范定义的资源JAR。 JAR将按照它们添加到ResourceRoot的顺序进行搜索。 Post - 由Web应用程序的context.xml中的元素定义的资源。 资源将按照指定的顺序进行搜索。 应该注意以下约定： 写操作（包括删除）只会应用于main ResourceSet。 如果其他资源集中某个资源的存在使主ResourceSet上的操作成为NO-OP，则写入操作将失败。 ResourceSet中的文件将隐藏在搜索顺序中靠后的ResourceSet中具有相同名称的目录（以及该目录的所有内容）。 只有主ResourceSet可以定义一个META-INF / context.xml文件，因为该文件定义了Pre和Post资源。 根据Servlet规范，资源JAR中的任何META-INF或WEB-INF目录都将被忽略。 Pre -和Post- 可以定义WEB-INF / lib和WEB-INF /classes，以便为Web应用程序提供额外的库和或类。 WebResourceRoot实现了Lifecycle接口，作为管理资源的组件 5.4.3.1 WebResource代表web资源，目录或文件，参考File类 内部实现封装了File，略 5.4.3.2 StandardRootWebResourceRoot实现类 成员变量： 12345678private Context context;private final List&lt;WebResourceSet&gt; preResources = new ArrayList&lt;&gt;();private WebResourceSet main;private final List&lt;WebResourceSet&gt; mainResources = new ArrayList&lt;&gt;();private final List&lt;WebResourceSet&gt; classResources = new ArrayList&lt;&gt;();private final List&lt;WebResourceSet&gt; jarResources = new ArrayList&lt;&gt;();private final List&lt;WebResourceSet&gt; postResources = new ArrayList&lt;&gt;();private final Cache cache= new Cache(this); Cache负责缓存，用CurrencyHashMap缓存， 内部通过backgroundProcess() 来刷新缓存 12345678910111213141516171819202122232425262728293031323334353637383940public class StandardRoot @Override public void backgroundProcess() &#123; cache.backgroundProcess(); gc(); &#125;&#125;public class Cache &#123; protected void backgroundProcess() &#123; TreeSet&lt;CachedResource&gt; orderedResources = new TreeSet&lt;&gt;(new EvictionOrder()); orderedResources.addAll(resourceCache.values()); Iterator&lt;CachedResource&gt; iter = orderedResources.iterator(); long targetSize = maxSize * (100 - TARGET_FREE_PERCENT_BACKGROUND) / 100; long newSize = evict(targetSize, iter); &#125; private long evict(long targetSize, Iterator&lt;CachedResource&gt; iter) &#123; long now = System.currentTimeMillis(); long newSize = size.get(); while (newSize &gt; targetSize &amp;&amp; iter.hasNext()) &#123; CachedResource resource = iter.next(); // Don't expire anything that has been checked within the TTL if (resource.getNextCheck() &gt; now) &#123; continue; &#125; // Remove the entry from the cache removeCacheEntry(resource.getWebappPath()); newSize = size.get(); &#125; return newSize; &#125; &#125; gc()方法是调用所有的WebResourceSet的gc()，有的WebResourceSet实现类会在内部做缓存 5.4.4 StandardContextValveStandardContext容器实现的基本Valve。 调用是直接调用子类的pipeLine的，也就是说容器就只是容器，不负责任何调用，由pipeLine完成 Wrapper wrapper = request.getWrapper();选择Wrapper由request完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152final class StandardContextValve extends ValveBase &#123; private static final StringManager sm = StringManager.getManager(StandardContextValve.class); public StandardContextValve() &#123; super(true); &#125; /** * 根据指定的请求URI，选择适当的Wrapper处理此请求。 如果找不到匹配的Wrapper， * 则返回适当的HTTP错误。 */ @Override public final void invoke(Request request, Response response) throws IOException, ServletException &#123; // 禁止直接访问 WEB-INF or META-INF MessageBytes requestPathMB = request.getRequestPathMB(); if ((requestPathMB.startsWithIgnoreCase("/META-INF/", 0)) || (requestPathMB.equalsIgnoreCase("/META-INF")) || (requestPathMB.startsWithIgnoreCase("/WEB-INF/", 0)) || (requestPathMB.equalsIgnoreCase("/WEB-INF"))) &#123; response.sendError(HttpServletResponse.SC_NOT_FOUND); return; &#125; // 选择Wrapper Wrapper wrapper = request.getWrapper(); if (wrapper == null || wrapper.isUnavailable()) &#123; response.sendError(HttpServletResponse.SC_NOT_FOUND); return; &#125; // Acknowledge the request try &#123; response.sendAcknowledgement(); &#125; catch (IOException ioe) &#123; container.getLogger().error(sm.getString( "standardContextValve.acknowledgeException"), ioe); request.setAttribute(RequestDispatcher.ERROR_EXCEPTION, ioe); response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR); return; &#125; if (request.isAsyncSupported()) &#123; request.setAsyncSupported(wrapper.getPipeline().isAsyncSupported()); &#125; wrapper.getPipeline().getFirst().invoke(request, response); &#125;&#125; 5.5 HostHost是一个容器，代表Catalina servlet引擎中的虚拟主机。用于： 您希望使用拦截器来查看由此特定虚拟主机处理的每个请求。 希望使用独立的HTTP连接器运行Catalina，但仍希望支持多个虚拟主机。 一般来说，在部署Catalina连接到Web服务器（如Apache）时，不会使用Host，因为连接器将利用Web服务器的组件来确定应该使用哪个Context（或者甚至是Wrapper）来处理这个请求。 连接到Host的父容器通常是一个Engine，但可能是其他一些实现，或者如果没有必要，可以省略。连接到主机的子容器通常是Context的实现。 5.5.1 StandardHostHost接口的标准实现 startInternal只是用来添加个errorValve，默认是org.apache.catalina.valves.ErrorReportValve 1234567891011121314151617181920212223242526272829@Overrideprotected synchronized void startInternal() throws LifecycleException &#123; // Set error report valve String errorValve = getErrorReportValveClass(); if ((errorValve != null) &amp;&amp; (!errorValve.equals(""))) &#123; try &#123; boolean found = false; Valve[] valves = getPipeline().getValves(); for (Valve valve : valves) &#123; if (errorValve.equals(valve.getClass().getName())) &#123; found = true; break; &#125; &#125; if(!found) &#123; Valve valve = (Valve) Class.forName(errorValve).getConstructor().newInstance(); getPipeline().addValve(valve); &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error(sm.getString( "standardHost.invalidErrorReportValveClass", errorValve), t); &#125; &#125; super.startInternal();&#125; 5.5.2 StandardHostValve1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * 选择合适子Context处理请求 */@Overridepublic final void invoke(Request request, Response response) throws IOException, ServletException &#123; // Select the Context to be used for this Request Context context = request.getContext(); if (context == null) &#123; response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, sm.getString("standardHost.noContext")); return; &#125; if (request.isAsyncSupported()) &#123; request.setAsyncSupported(context.getPipeline().isAsyncSupported()); &#125; boolean asyncAtStart = request.isAsync(); boolean asyncDispatching = request.isAsyncDispatching(); try &#123; context.bind(Globals.IS_SECURITY_ENABLED, MY_CLASSLOADER); if (!asyncAtStart &amp;&amp; !context.fireRequestInitEvent(request.getRequest())) &#123; return; &#125; try &#123; if (!asyncAtStart || asyncDispatching) &#123; context.getPipeline().getFirst().invoke(request, response); &#125; else &#123; if (!response.isErrorReportRequired()) &#123; throw new IllegalStateException(sm.getString("standardHost.asyncStateError")); &#125; &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); container.getLogger().error("Exception Processing " + request.getRequestURI(), t); // If a new error occurred while trying to report a previous // error allow the original error to be reported. if (!response.isErrorReportRequired()) &#123; request.setAttribute(RequestDispatcher.ERROR_EXCEPTION, t); throwable(request, response, t); &#125; &#125; // Now that the request/response pair is back under container // control lift the suspension so that the error handling can // complete and/or the container can flush any remaining data response.setSuspended(false); Throwable t = (Throwable) request.getAttribute(RequestDispatcher.ERROR_EXCEPTION); // Protect against NPEs if the context was destroyed during a // long running request. if (!context.getState().isAvailable()) &#123; return; &#125; // Look for (and render if found) an application level error page if (response.isErrorReportRequired()) &#123; if (t != null) &#123; throwable(request, response, t); &#125; else &#123; status(request, response); &#125; &#125; if (!request.isAsync() &amp;&amp; !asyncAtStart) &#123; context.fireRequestDestroyEvent(request.getRequest()); &#125; &#125; finally &#123; // Access a session (if present) to update last accessed time, based // on a strict interpretation of the specification if (ACCESS_SESSION) &#123; request.getSession(false); &#125; context.unbind(Globals.IS_SECURITY_ENABLED, MY_CLASSLOADER); &#125;&#125; 5.6 Engine引擎是一个代表整个Catalina servlet引擎的容器。 您希望使用拦截器来查看整个引擎处理的每个请求。 希望使用独立的HTTP连接器运行Catalina，但仍希望支持多个虚拟主机。 一般来说，当部署连接到Web服务器（如Apache）的Catalina时，不会使用引擎，因为连接器将利用Web服务器来确定应该使用哪个上下文（或者甚至是哪个包装器）来处理请求。 连接到Engine的子容器通常是Host的实现（表示一个虚拟主机）或Context（表示一个单独的Servlet上下文），这取决于Engine的实现。 如果使用，Engine始终是Catalina层次结构中的顶级Container。因此，实现的setParent（）方法应抛出IllegalArgumentException。 Service包含了连接器组件，暂时不考虑 5.6.1 StandardEngineEngine的标准实现 5.6.2 StandardEngineValve1234567891011121314151617181920public final void invoke(Request request, Response response) throws IOException, ServletException &#123; // Select the Host to be used for this Request Host host = request.getHost(); if (host == null) &#123; response.sendError (HttpServletResponse.SC_BAD_REQUEST, sm.getString("standardEngine.noHost", request.getServerName())); return; &#125; if (request.isAsyncSupported()) &#123; request.setAsyncSupported(host.getPipeline().isAsyncSupported()); &#125; // Ask this Host to process this request host.getPipeline().getFirst().invoke(request, response);&#125;]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常见内置模块]]></title>
    <url>%2F2018%2F03%2F16%2Fpython%E5%9F%BA%E7%A1%80%2Fpython%E5%B8%B8%E8%A7%81%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[常见内置模块：datetimefrom datetime import datetime datetime是模块，datetime模块还包含一个datetime类 1234567891011121314now = datetime.now()dt = datetime(2015, 4, 19, 12, 20)# datetime转换为timestamp# timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00&gt;&gt;&gt; print(datetime.fromtimestamp(t)) # 本地时间2015-04-19 12:20:00&gt;&gt;&gt; print(datetime.utcfromtimestamp(t)) # UTC时间2015-04-19 04:20:00now.strftime('%a, %b %d %H:%M')cday = datetime.strptime('2015-6-1 18:19:59', '%Y-%m-%d %H:%M:%S')# 注意转换后的datetime是没有时区信息的。 Python的timestamp是一个浮点数。如果有小数位，小数位表示毫秒数。 12now + timedelta(hours=10)now + timedelta(days=2, hours=12) 一个datetime类型有一个时区属性tzinfo，但是默认为None，所以无法区分这个datetime到底是哪个时区，除非强行给datetime设置一个时区： 12345678&gt;&gt;&gt; from datetime import datetime, timedelta, timezone&gt;&gt;&gt; tz_utc_8 = timezone(timedelta(hours=8)) # 创建时区UTC+8:00&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012)&gt;&gt;&gt; dt = now.replace(tzinfo=tz_utc_8) # 强制设置为UTC+8:00&gt;&gt;&gt; dtdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012, tzinfo=datetime.timezone(datetime.timedelta(0, 28800))) collectionsnamedtuplenamedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数0 1234567&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x1&gt;&gt;&gt; p.y2 dequedeque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈： 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])&gt;&gt;&gt; q.append(&apos;x&apos;)&gt;&gt;&gt; q.appendleft(&apos;y&apos;)&gt;&gt;&gt; qdeque([&apos;y&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;x&apos;]) defaultdict1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: &apos;N/A&apos;)&gt;&gt;&gt; dd[&apos;key1&apos;] = &apos;abc&apos;&gt;&gt;&gt; dd[&apos;key1&apos;] # key1存在&apos;abc&apos;&gt;&gt;&gt; dd[&apos;key2&apos;] # key2不存在，返回默认值&apos;N/A&apos; OrderedDict如果要保持Key的顺序，可以用OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;&apos;a&apos;: 1, &apos;c&apos;: 3, &apos;b&apos;: 2&#125;&gt;&gt;&gt; od = OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)]) OrderedDict的Key会按照插入的顺序排列，不是Key本身排序 CounterCounter是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in &apos;programming&apos;:... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;&apos;g&apos;: 2, &apos;m&apos;: 2, &apos;r&apos;: 2, &apos;a&apos;: 1, &apos;i&apos;: 1, &apos;o&apos;: 1, &apos;n&apos;: 1, &apos;p&apos;: 1&#125;) Base64Base64是一种用64个字符来表示任意二进制数据的方法。 用记事本打开exe、jpg、pdf这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。 12345&gt;&gt;&gt; import base64&gt;&gt;&gt; base64.b64encode(b&apos;binary\x00string&apos;)b&apos;YmluYXJ5AHN0cmluZw==&apos;&gt;&gt;&gt; base64.b64decode(b&apos;YmluYXJ5AHN0cmluZw==&apos;)b&apos;binary\x00string&apos; structstruct的pack函数把任意数据类型变成bytes： 123&gt;&gt;&gt; import struct&gt;&gt;&gt; struct.pack(&apos;&gt;I&apos;, 10240099)b&apos;\x00\x9c@c&apos; hashlibPython的hashlib提供了常见的摘要算法，如MD5，SHA1等等 12345import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in python hashlib?'.encode('utf-8'))print(md5.hexdigest()) 如果数据量很大，可以分块多次调用update()，最后计算的结果是一样的： 123456import hashlibmd5 = hashlib.md5()md5.update(&apos;how to use md5 in &apos;.encode(&apos;utf-8&apos;))md5.update(&apos;python hashlib?&apos;.encode(&apos;utf-8&apos;))print(md5.hexdigest()) SHA1类似 hmac通过一个标准算法，在计算哈希的过程中，把key混入计算过程中。 1234567&gt;&gt;&gt; import hmac&gt;&gt;&gt; message = b&apos;Hello, world!&apos;&gt;&gt;&gt; key = b&apos;secret&apos;&gt;&gt;&gt; h = hmac.new(key, message, digestmod=&apos;MD5&apos;)&gt;&gt;&gt; # 如果消息很长，可以多次调用h.update(msg)&gt;&gt;&gt; h.hexdigest()&apos;fa4ee7d173f2d97ee79022d1a7355bcf&apos; itertoolsPython的内建模块itertools提供了非常有用的用于操作迭代对象的函数。 “无限”计数迭代器：natuals = itertools.count(1) 序列无限重复: cs = itertools.cycle(&#39;ABC&#39;) # 注意字符串也是序列的一种 repeat()负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数：ns = itertools.repeat(&#39;A&#39;, 3) chain()可以把一组迭代对象串联起来，形成一个更大的迭代器 groupby()把迭代器中相邻的重复元素挑出来放在一起 contextlib在Python中，读写文件这样的资源要特别注意，必须在使用完毕后正确关闭它们。正确关闭文件资源的一个方法是使用try...finally： 123456try: f = open(&apos;/path/to/file&apos;, &apos;r&apos;) f.read()finally: if f: f.close() 写try...finally非常繁琐。Python的with语句允许我们非常方便地使用资源，而不必担心资源没有关闭，所以上面的代码可以简化为： 12with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f: f.read() 并不是只有open()函数返回的fp对象才能使用with语句。实际上，任何对象，只要正确实现了上下文管理，就可以用于with语句。 实现上下文管理是通过__enter__和__exit__这两个方法实现的。 @contextmanager编写__enter__和__exit__仍然很繁琐，因此Python的标准库contextlib提供了更简单的写法，上面的代码可以改写如下： 123456@contextmanagerdef create_query(name): print(&apos;Begin&apos;) q = Query(name) yield q print(&apos;End&apos;) 用yield语句把with ... as var把变量输出出去，然后，with语句就可以正常地工作了 @closing如果一个对象没有实现上下文，我们就不能把它用于with语句。这个时候，可以用closing()来把该对象变为上下文对象。 123456from contextlib import closingfrom urllib.request import urlopenwith closing(urlopen(&apos;https://www.python.org&apos;)) as page: for line in page: print(line) closing也是一个经过@contextmanager装饰的generator，这个generator编写起来其实非常简单： 123456@contextmanagerdef closing(thing): try: yield thing finally: thing.close() 它的作用就是把任意对象变为上下文对象，并支持with语句。 urlliburllib提供了一系列用于操作URL的功能。 Get12345678from urllib import requestwith request.urlopen(&apos;https://api.douban.com/v2/book/2129650&apos;) as f: data = f.read() print(&apos;Status:&apos;, f.status, f.reason) for k, v in f.getheaders(): print(&apos;%s: %s&apos; % (k, v)) print(&apos;Data:&apos;, data.decode(&apos;utf-8&apos;)) 123456789from urllib import requestreq = request.Request(&apos;http://www.douban.com/&apos;)req.add_header(&apos;User-Agent&apos;, &apos;Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25&apos;)with request.urlopen(req) as f: print(&apos;Status:&apos;, f.status, f.reason) for k, v in f.getheaders(): print(&apos;%s: %s&apos; % (k, v)) print(&apos;Data:&apos;, f.read().decode(&apos;utf-8&apos;)) Post如果要以POST发送一个请求，只需要把参数data以bytes形式传入。 1234567891011121314151617181920login_data = parse.urlencode([ ('username', email), ('password', passwd), ('entry', 'mweibo'), ('client_id', ''), ('savestate', '1'), ('ec', ''), ('pagerefer', 'https://passport.weibo.cn/signin/welcome?entry=mweibo&amp;r=http%3A%2F%2Fm.weibo.cn%2F')])req = request.Request('https://passport.weibo.cn/sso/login')req.add_header('Origin', 'https://passport.weibo.cn')req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')req.add_header('Referer', 'https://passport.weibo.cn/signin/login?entry=mweibo&amp;res=wel&amp;wm=3349&amp;r=http%3A%2F%2Fm.weibo.cn%2F')with request.urlopen(req, data=login_data.encode('utf-8')) as f: print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', f.read().decode('utf-8')) Handler如果还需要更复杂的控制，比如通过一个Proxy去访问网站，我们需要利用ProxyHandler来处理，示例代码如下： 123456proxy_handler = urllib.request.ProxyHandler(&#123;'http': 'http://www.example.com:3128/'&#125;)proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()proxy_auth_handler.add_password('realm', 'host', 'username', 'password')opener = urllib.request.build_opener(proxy_handler, proxy_auth_handler)with opener.open('http://www.example.com/login.html') as f: pass XMLDOM vs SAX操作XML有两种方法：DOM和SAX。DOM会把整个XML读入内存，解析为树，因此占用内存大，解析慢，优点是可以任意遍历树的节点。SAX是流模式，边读边解析，占用内存小，解析快，缺点是我们需要自己处理事件。 在Python中使用SAX解析XML非常简洁，通常我们关心的事件是start_element，end_element和char_data，准备好这3个函数，然后就可以解析xml了。 举个例子，当SAX解析器读到一个节点时： 1&lt;a href=&quot;/&quot;&gt;python&lt;/a&gt; 会产生3个事件： start_element事件，在读取&lt;a href=&quot;/&quot;&gt;时； char_data事件，在读取python时； end_element事件，在读取&lt;/a&gt;时。 12345678910111213141516171819202122232425from xml.parsers.expat import ParserCreateclass DefaultSaxHandler(object): def start_element(self, name, attrs): print('sax:start_element: %s, attrs: %s' % (name, str(attrs))) def end_element(self, name): print('sax:end_element: %s' % name) def char_data(self, text): print('sax:char_data: %s' % text)xml = r'''&lt;?xml version="1.0"?&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="/python"&gt;Python&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;'''handler = DefaultSaxHandler()parser = ParserCreate()parser.StartElementHandler = handler.start_elementparser.EndElementHandler = handler.end_elementparser.CharacterDataHandler = handler.char_dataparser.Parse(xml) HTMLParserHTML本质上是XML的子集，但是HTML的语法没有XML那么严格，所以不能用标准的DOM或SAX来解析HTML。 好在Python提供了HTMLParser来非常方便地解析HTML，只需简单几行代码： 123456789101112131415161718192021222324252627282930313233from html.parser import HTMLParserfrom html.entities import name2codepointclass MyHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): print('&lt;%s&gt;' % tag) def handle_endtag(self, tag): print('&lt;/%s&gt;' % tag) def handle_startendtag(self, tag, attrs): print('&lt;%s/&gt;' % tag) def handle_data(self, data): print(data) def handle_comment(self, data): print('&lt;!--', data, '--&gt;') def handle_entityref(self, name): print('&amp;%s;' % name) def handle_charref(self, name): print('&amp;#%s;' % name)parser = MyHTMLParser()parser.feed('''&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- test html parser --&gt; &lt;p&gt;Some &lt;a href=\"#\"&gt;html&lt;/a&gt; HTML&amp;nbsp;tutorial...&lt;br&gt;END&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;''') 常用的第三方模块PillowPIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。 由于PIL仅支持到Python 2.7，加上年久失修，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，支持最新Python 3.x，又加入了许多新特性，因此，我们可以直接安装使用Pillow。 操作图像来看看最常见的图像缩放操作，只需三四行代码： 123456789101112from PIL import Image# 打开一个jpg图像文件，注意是当前路径:im = Image.open(&apos;test.jpg&apos;)# 获得图像尺寸:w, h = im.sizeprint(&apos;Original image size: %sx%s&apos; % (w, h))# 缩放到50%:im.thumbnail((w//2, h//2))print(&apos;Resize image to: %sx%s&apos; % (w//2, h//2))# 把缩放后的图像用jpeg格式保存:im.save(&apos;thumbnail.jpg&apos;, &apos;jpeg&apos;) 其他功能如切片、旋转、滤镜、输出文字、调色板等一应俱全。 比如，模糊效果也只需几行代码： 123456from PIL import Image, ImageFilter# 打开一个jpg图像文件，注意是当前路径:im = Image.open(&apos;test.jpg&apos;)# 应用模糊滤镜:im2 = im.filter(ImageFilter.BLUR)im2.save(&apos;blur.jpg&apos;, &apos;jpeg&apos;) 效果如下： PIL的ImageDraw提供了一系列绘图方法，让我们可以直接绘图。比如要生成字母验证码图片： 12345678910111213141516171819202122232425262728293031323334from PIL import Image, ImageDraw, ImageFont, ImageFilterimport random# 随机字母:def rndChar(): return chr(random.randint(65, 90))# 随机颜色1:def rndColor(): return (random.randint(64, 255), random.randint(64, 255), random.randint(64, 255))# 随机颜色2:def rndColor2(): return (random.randint(32, 127), random.randint(32, 127), random.randint(32, 127))# 240 x 60:width = 60 * 4height = 60image = Image.new(&apos;RGB&apos;, (width, height), (255, 255, 255))# 创建Font对象:font = ImageFont.truetype(&apos;Arial.ttf&apos;, 36)# 创建Draw对象:draw = ImageDraw.Draw(image)# 填充每个像素:for x in range(width): for y in range(height): draw.point((x, y), fill=rndColor())# 输出文字:for t in range(4): draw.text((60 * t + 10, 10), rndChar(), font=font, fill=rndColor2())# 模糊:image = image.filter(ImageFilter.BLUR)image.save(&apos;code.jpg&apos;, &apos;jpeg&apos;) 我们用随机颜色填充背景，再画上文字，最后对图像进行模糊，得到验证码图片如下： 如果运行的时候报错： 1IOError: cannot open resource 这是因为PIL无法定位到字体文件的位置，可以根据操作系统提供绝对路径，比如： 1&apos;/Library/Fonts/Arial.ttf&apos; 要详细了解PIL的强大功能，请请参考Pillow官方文档： https://pillow.readthedocs.org/ requestsrequests。它是一个Python第三方库，处理URL资源特别方便。 要通过GET访问一个页面，只需要几行代码： 1234567&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get(&apos;https://www.douban.com/&apos;) # 豆瓣首页&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; r.textr.text&apos;&lt;!DOCTYPE HTML&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta name=&quot;description&quot; content=&quot;提供图书、电影、音乐唱片的推荐、评论和...&apos; 对于带参数的URL，传入一个dict作为params参数： 123&gt;&gt;&gt; r = requests.get(&apos;https://www.douban.com/search&apos;, params=&#123;&apos;q&apos;: &apos;python&apos;, &apos;cat&apos;: &apos;1001&apos;&#125;)&gt;&gt;&gt; r.url # 实际请求的URL&apos;https://www.douban.com/search?q=python&amp;cat=1001&apos; requests自动检测编码，可以使用encoding属性查看： 12&gt;&gt;&gt; r.encoding&apos;utf-8&apos; 无论响应是文本还是二进制内容，我们都可以用content属性获得bytes对象： 12&gt;&gt;&gt; r.contentb&apos;&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n...&apos; requests的方便之处还在于，对于特定类型的响应，例如JSON，可以直接获取： 123&gt;&gt;&gt; r = requests.get(&apos;https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20weather.forecast%20where%20woeid%20%3D%202151330&amp;format=json&apos;)&gt;&gt;&gt; r.json()&#123;&apos;query&apos;: &#123;&apos;count&apos;: 1, &apos;created&apos;: &apos;2017-11-17T07:14:12Z&apos;, ... 需要传入HTTP Header时，我们传入一个dict作为headers参数： 123&gt;&gt;&gt; r = requests.get(&apos;https://www.douban.com/&apos;, headers=&#123;&apos;User-Agent&apos;: &apos;Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit&apos;&#125;)&gt;&gt;&gt; r.text&apos;&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=&quot;UTF-8&quot;&gt;\n &lt;title&gt;豆瓣(手机版)&lt;/title&gt;...&apos; 要发送POST请求，只需要把get()方法变成post()，然后传入data参数作为POST请求的数据： 1&gt;&gt;&gt; r = requests.post(&apos;https://accounts.douban.com/login&apos;, data=&#123;&apos;form_email&apos;: &apos;abc@example.com&apos;, &apos;form_password&apos;: &apos;123456&apos;&#125;) requests默认使用application/x-www-form-urlencoded对POST数据编码。如果要传递JSON数据，可以直接传入json参数： 12params = &#123;&apos;key&apos;: &apos;value&apos;&#125;r = requests.post(url, json=params) # 内部自动序列化为JSON 类似的，上传文件需要更复杂的编码格式，但是requests把它简化成files参数： 12&gt;&gt;&gt; upload_files = &#123;&apos;file&apos;: open(&apos;report.xls&apos;, &apos;rb&apos;)&#125;&gt;&gt;&gt; r = requests.post(url, files=upload_files) 在读取文件时，注意务必使用&#39;rb&#39;即二进制模式读取，这样获取的bytes长度才是文件的长度。 把post()方法替换为put()，delete()等，就可以以PUT或DELETE方式请求资源。 除了能轻松获取响应内容外，requests对获取HTTP响应的其他信息也非常简单。例如，获取响应头： 1234&gt;&gt;&gt; r.headers&#123;Content-Type&apos;: &apos;text/html; charset=utf-8&apos;, &apos;Transfer-Encoding&apos;: &apos;chunked&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;, ...&#125;&gt;&gt;&gt; r.headers[&apos;Content-Type&apos;]&apos;text/html; charset=utf-8&apos; requests对Cookie做了特殊处理，使得我们不必解析Cookie就可以轻松获取指定的Cookie： 12&gt;&gt;&gt; r.cookies[&apos;ts&apos;]&apos;example_cookie_12345&apos; 要在请求中传入Cookie，只需准备一个dict传入cookies参数： 12&gt;&gt;&gt; cs = &#123;&apos;token&apos;: &apos;12345&apos;, &apos;status&apos;: &apos;working&apos;)&gt;&gt;&gt; r = requests.get(url, cookies=cs) 最后，要指定超时，传入以秒为单位的timeout参数： 1&gt;&gt;&gt; r = requests.get(url, timeout=2.5) # 2.5秒后超时 chardetchardet第三方库用它来检测编码，简单易用。 当我们拿到一个bytes时，就可以对其检测编码。用chardet检测编码，只需要一行代码： 12&gt;&gt;&gt; chardet.detect(b&apos;Hello, world!&apos;)&#123;&apos;encoding&apos;: &apos;ascii&apos;, &apos;confidence&apos;: 1.0, &apos;language&apos;: &apos;&apos;&#125; 检测出的编码是ascii，注意到还有个confidence字段，表示检测的概率是1.0（即100%）。 我们来试试检测GBK编码的中文： 123&gt;&gt;&gt; data = &apos;离离原上草，一岁一枯荣&apos;.encode(&apos;gbk&apos;)&gt;&gt;&gt; chardet.detect(data)&#123;&apos;encoding&apos;: &apos;GB2312&apos;, &apos;confidence&apos;: 0.7407407407407407, &apos;language&apos;: &apos;Chinese&apos;&#125; psutilpsutil = process and system utilities，它不仅可以通过一两行代码实现系统监控，还可以跨平台使用，支持Linux／UNIX／OSX／Windows等 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt;&gt;&gt; import psutil## cpu&gt;&gt;&gt; psutil.cpu_count() # CPU逻辑数量4&gt;&gt;&gt; psutil.cpu_count(logical=False) # CPU物理核心2&gt;&gt;&gt; psutil.cpu_times()scputimes(user=10963.31, nice=0.0, system=5138.67, idle=356102.45)## mem&gt;&gt;&gt; psutil.virtual_memory()svmem(total=8589934592, available=2866520064, percent=66.6, used=7201386496, free=216178688, active=3342192640, inactive=2650341376, wired=1208852480)&gt;&gt;&gt; psutil.swap_memory()sswap(total=1073741824, used=150732800, free=923009024, percent=14.0, sin=10705981440, sout=40353792)## disk&gt;&gt;&gt; psutil.disk_partitions() # 磁盘分区信息[sdiskpart(device='/dev/disk1', mountpoint='/', fstype='hfs', opts='rw,local,rootfs,dovolfs,journaled,multilabel')]&gt;&gt;&gt; psutil.disk_usage('/') # 磁盘使用情况sdiskusage(total=998982549504, used=390880133120, free=607840272384, percent=39.1)&gt;&gt;&gt; psutil.disk_io_counters() # 磁盘IOsdiskio(read_count=988513, write_count=274457, read_bytes=14856830464, write_bytes=17509420032, read_time=2228966, write_time=1618405)## net&gt;&gt;&gt; psutil.net_io_counters() # 获取网络读写字节／包的个数snetio(bytes_sent=3885744870, bytes_recv=10357676702, packets_sent=10613069, packets_recv=10423357, errin=0, errout=0, dropin=0, dropout=0)&gt;&gt;&gt; psutil.net_if_addrs() # 获取网络接口信息&#123; 'lo0': [snic(family=&lt;AddressFamily.AF_INET: 2&gt;, address='127.0.0.1', netmask='255.0.0.0'), ...], 'en1': [snic(family=&lt;AddressFamily.AF_INET: 2&gt;, address='10.0.1.80', netmask='255.255.255.0'), ...], 'en0': [...], 'en2': [...], 'bridge0': [...]&#125;&gt;&gt;&gt; psutil.net_if_stats() # 获取网络接口状态&#123; 'lo0': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_UNKNOWN: 0&gt;, speed=0, mtu=16384), 'en0': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_UNKNOWN: 0&gt;, speed=0, mtu=1500), 'en1': snicstats(...), 'en2': snicstats(...), 'bridge0': snicstats(...)&#125;&gt;&gt;&gt; psutil.net_connections()# 要获取当前网络连接信息，使用net_connections()： 进程 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; psutil.pids() # 所有进程ID[3865, 3864, 3863, 3856, 3855, 3853, 3776, ..., 45, 44, 1, 0]&gt;&gt;&gt; p = psutil.Process(3776) # 获取指定进程ID=3776，其实就是当前Python交互环境&gt;&gt;&gt; p.name() # 进程名称'python3.6'&gt;&gt;&gt; p.exe() # 进程exe路径'/Users/michael/anaconda3/bin/python3.6'&gt;&gt;&gt; p.cwd() # 进程工作目录'/Users/michael'&gt;&gt;&gt; p.cmdline() # 进程启动的命令行['python3']&gt;&gt;&gt; p.ppid() # 父进程ID3765&gt;&gt;&gt; p.parent() # 父进程&lt;psutil.Process(pid=3765, name='bash') at 4503144040&gt;&gt;&gt;&gt; p.children() # 子进程列表[]&gt;&gt;&gt; p.status() # 进程状态'running'&gt;&gt;&gt; p.username() # 进程用户名'michael'&gt;&gt;&gt; p.create_time() # 进程创建时间1511052731.120333&gt;&gt;&gt; p.terminal() # 进程终端'/dev/ttys002'&gt;&gt;&gt; p.cpu_times() # 进程使用的CPU时间pcputimes(user=0.081150144, system=0.053269812, children_user=0.0, children_system=0.0)&gt;&gt;&gt; p.memory_info() # 进程使用的内存pmem(rss=8310784, vms=2481725440, pfaults=3207, pageins=18)&gt;&gt;&gt; p.open_files() # 进程打开的文件[]&gt;&gt;&gt; p.connections() # 进程相关网络连接[]&gt;&gt;&gt; p.num_threads() # 进程的线程数量1&gt;&gt;&gt; p.threads() # 所有线程信息[pthread(id=1, user_time=0.090318, system_time=0.062736)]&gt;&gt;&gt; p.environ() # 进程环境变量&#123;'SHELL': '/bin/bash', 'PATH': '/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:...', 'PWD': '/Users/michael', 'LANG': 'zh_CN.UTF-8', ...&#125;&gt;&gt;&gt; p.terminate() # 结束进程Terminated: 15 &lt;-- 自己把自己结束了]]></content>
      <categories>
        <category>python</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络编程]]></title>
    <url>%2F2018%2F03%2F16%2Fpython%E5%9F%BA%E7%A1%80%2Fpython%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[网络编程TCP客户端123456789# 导入socket库:import socket# 创建一个socket:s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 建立连接:s.connect(('www.sina.com.cn', 80))# 发送数据:s.send(b'GET / HTTP/1.1\r\nHost: www.sina.com.cn\r\nConnection: close\r\n\r\n') 创建Socket时，AF_INET指定使用IPv4协议，如果要用更先进的IPv6，就指定为AF_INET6 TCP连接创建的是双向通道，双方都可以同时给对方发数据。 12345678910# 接收数据:buffer = []while True: # 每次最多接收1k字节: d = s.recv(1024) if d: buffer.append(d) else: breakdata = b''.join(buffer) 12# 关闭连接:s.close() 12345header, html = data.split(b&apos;\r\n\r\n&apos;, 1)print(header.decode(&apos;utf-8&apos;))# 把接收的数据写入文件:with open(&apos;sina.html&apos;, &apos;wb&apos;) as f: f.write(html) 服务器12345678910111213141516171819202122232425s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 监听端口:s.bind(('127.0.0.1', 9999))s.listen(5) # 参数指定等待连接的最大数量：print('Waiting for connection...')while True: # 接受一个新连接: sock, addr = s.accept() # 创建新线程来处理TCP连接: t = threading.Thread(target=tcplink, args=(sock, addr)) t.start() def tcplink(sock, addr): print('Accept new connection from %s:%s...' % addr) sock.send(b'Welcome!') while True: data = sock.recv(1024) time.sleep(1) if not data or data.decode('utf-8') == 'exit': break sock.send(('Hello, %s!' % data.decode('utf-8')).encode('utf-8')) sock.close() print('Connection from %s:%s closed.' % addr) UDP123456789s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# 绑定端口:s.bind(('127.0.0.1', 9999))print('Bind UDP on 9999...')while True: # 接收数据: data, addr = s.recvfrom(1024) print('Received from %s:%s.' % addr) s.sendto(b'Hello, %s!' % data, addr) recvfrom()方法返回数据和客户端的地址与端口，这样，服务器收到数据后，直接调用sendto()就可以把数据用UDP发给客户端。 客户端使用UDP时，首先仍然创建基于UDP的Socket，然后，不需要调用connect()，直接通过sendto()给服务器发数据： 1234567s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)for data in [b'Michael', b'Tracy', b'Sarah']: # 发送数据: s.sendto(data, ('127.0.0.1', 9999)) # 接收数据: print(s.recv(1024).decode('utf-8'))s.close() 从服务器接收数据仍然调用recv()方法。 异步IO协程子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。 协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。 协程的线程执行类似一个线程会执行多个不同的程序栈，而不是一个线程一个栈，省去了cpu切换的开销 优点： 协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了 传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。 如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高： 123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c) 执行结果： 123456789101112131415[PRODUCER] Producing 1...[CONSUMER] Consuming 1...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 2...[CONSUMER] Consuming 2...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 3...[CONSUMER] Consuming 3...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 4...[CONSUMER] Consuming 4...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 5...[CONSUMER] Consuming 5...[PRODUCER] Consumer return: 200 OK 注意到consumer函数是一个generator，把一个consumer传入produce后： 首先调用c.send(None)启动生成器； 然后，一旦生产了东西，通过c.send(n)切换到consumer执行； consumer通过yield拿到消息，处理，又通过yield把结果传回； produce拿到consumer处理的结果，继续生产下一条消息； produce决定不生产了，通过c.close()关闭consumer，整个过程结束。 整个流程无锁，由一个线程执行，produce和consumer协作完成任务，所以称为“协程”，而非线程的抢占式多任务。 最后套用Donald Knuth的一句话总结协程的特点： “子程序就是协程的一种特例。” asyncioasyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。 1234567891011121314import asyncio@asyncio.coroutinedef hello(): print("Hello world!") # 异步调用asyncio.sleep(1): r = yield from asyncio.sleep(1) print("Hello again!")# 获取EventLoop:loop = asyncio.get_event_loop()# 执行coroutineloop.run_until_complete(hello())loop.close() `@asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop`中执行。 hello()会首先打印出Hello world!，然后，yield from语法可以让我们方便地调用另一个generator。由于asyncio.sleep()也是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。 把asyncio.sleep(1)看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。 我们用asyncio的异步网络连接来获取sina、sohu和163的网站首页： 12345678910111213141516171819202122import asyncio@asyncio.coroutinedef wget(host): print('wget %s...' % host) connect = asyncio.open_connection(host, 80) reader, writer = yield from connect header = 'GET / HTTP/1.0\r\nHost: %s\r\n\r\n' % host writer.write(header.encode('utf-8')) yield from writer.drain() while True: line = yield from reader.readline() if line == b'\r\n': break print('%s header &gt; %s' % (host, line.decode('utf-8').rstrip())) # Ignore the body, close the socket writer.close()loop = asyncio.get_event_loop()tasks = [wget(host) for host in ['www.sina.com.cn', 'www.sohu.com', 'www.163.com']]loop.run_until_complete(asyncio.wait(tasks))loop.close() async/await为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法async和await，可以让coroutine的代码更简洁易读。 async和await是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换： 把`@asyncio.coroutine替换为async`； 把yield from替换为await。 让我们对比一下上一节的代码： 12345@asyncio.coroutinedef hello(): print(&quot;Hello world!&quot;) r = yield from asyncio.sleep(1) print(&quot;Hello again!&quot;) 用新语法重新编写如下： 1234async def hello(): print(&quot;Hello world!&quot;) r = await asyncio.sleep(1) print(&quot;Hello again!&quot;) aioHttpasyncio实现了TCP、UDP、SSL等协议，aiohttp则是基于asyncio实现的HTTP框架。 1pip install aiohttp 然后编写一个HTTP服务器，分别处理以下URL： / - 首页返回b&#39;&lt;h1&gt;Index&lt;/h1&gt;&#39;； /hello/{name} - 根据URL参数返回文本hello, %s!。 代码如下： 123456789101112131415161718192021222324import asynciofrom aiohttp import webasync def index(request): await asyncio.sleep(0.5) return web.Response(body=b'&lt;h1&gt;Index&lt;/h1&gt;')async def hello(request): await asyncio.sleep(0.5) text = '&lt;h1&gt;hello, %s!&lt;/h1&gt;' % request.match_info['name'] return web.Response(body=text.encode('utf-8'))async def init(loop): app = web.Application(loop=loop) app.router.add_route('GET', '/', index) app.router.add_route('GET', '/hello/&#123;name&#125;', hello) srv = await loop.create_server(app.make_handler(), '127.0.0.1', 8000) print('Server started at http://127.0.0.1:8000...') return srvloop = asyncio.get_event_loop()loop.run_until_complete(init(loop))loop.run_forever() 注意aiohttp的初始化函数init()也是一个coroutine，loop.create_server()则利用asyncio创建TCP服务。]]></content>
      <categories>
        <category>python</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘调度算法-银行家算法]]></title>
    <url>%2F2018%2F03%2F16%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E9%93%B6%E8%A1%8C%E5%AE%B6%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[磁盘调度算法先来先服务算法（FCFS）最短寻道时间优先算法（SSTF）选择调度处理的磁道是与当前磁头所在磁道距离最近的磁道，以使每次的寻找时间最短。当然，总是选择最小寻找时间并不能保证平均寻找时间最小，但是能提供比FCFS算法更好的性能。 扫描算法（SCAN）电梯算法SCAN算法在磁头当前移动方向上选择与当前磁头所在磁道距离最近的请求作为下一次服务的对象。 循环扫描算法（CSCAN） 在扫描算法的基础上规定磁头单向移动来提供服务，回返时直接快速移动至起始端而不服务任何请求。由于SCAN算法偏向于处理那些接近最里或最外的磁道的访问请求，所以使用改进型的C-SCAN算法来避免这个问题。 SCAN中扫完开始端又扫开始端，会浪费 银行家算法当一个新的进程进入系统时，进程必须声明所需每个资源实例最大的数量和类型。显然，资源数量不不能超过系统最大的资源数。与此同时，进程获得的资源必须在有限的时间内释放。 对于银行家算法的实现，需要知道三件事： 每个进程所能获取的每种资源数量是多少[MAX] 每个进程当前所分配到的每种资源的数量是多少[ALLOCATED] 系统当前可分配的每种的资源数量是多少[AVAILABLE] 只有当资源满足以下条件，资源才会被分配： request &lt;= max, 也可设置错误条件，当进程所请求的资源超过最大的要求 request &lt;= available, 或者进 程一直等直到资源可分配]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode经典题目]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%AE%97%E6%B3%95%2Fleetcode%E7%BB%8F%E5%85%B8%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[数学数组Leetcode 370. Range Addition给定一个全为0的array的长度，以及一系列操作，每个操作会指明要操作的开始索引和结束索引，以及要加上的值，求出所给操作执行完之后的array情况，具体样例如下： 12345678length = 5,updates = [[1, 3, 2],[2, 4, 3],[0, 2, -2]]return [-2, 0, 3, 5, 3] 思路：只需要记录start和end即可，对每个operation中的start位置的值加上val，end位置值减去val；第二：最后求解整个list的值的时候，是从前到后逐个相加得到的 1234567891011121314class Solution: def getModifiedArray(self, length, updates): # Write your code here result = [0 for idx in range(length)] for item in updates: result[item[0]] += item[2] if item[1]+1 &lt; length: result[item[1]+1] -= item[2] tmp = 0 for idx in range(length): tmp += result[idx] result[idx] = tmp return result 扩展联想： 重点在于复用前一个值，如何复用？ 其他题目： 求数组中任意一个连续范围的数字和 二维如何计算， 方法一：扩展1D到2D，可以设置4个角的数据，然后自顶向下，自左向右累加即可，注意计算行时先计算本行，再加上一行 4*4矩阵，updates=[[1,1],[3,3],1], [[2,2],[3,3],1]如下： 0 0 0 0 1 0 0 -1 1 0 0 -1 1 1 1 00 0 0 0 0 0 0 0 0 2 0 -2 1 3 3 00 0 0 0 ==&gt; 0 0 0 0 ==&gt; 0 0 0 0 ==&gt; 1 3 3 00 0 0 0 -1 0 0 1 -1 -2 0 3 0 0 0 0 1234567891011121314151617181920def getModifiedArray(self, m, n, updates): # Write your code here result = [[0 for idx in range(m)] for idx in range(n)] for item in updates: result[item[0][0]][item[0][1]] += item[2] if item[1][0] + 1 &lt; m: result[item[1][0] + 1][item[0][1]] -= item[2] if item[1][1] + 1 &lt; n: result[item[0][0]][item[1][1] + 1] -= item[2] if item[1][0] + 1 &lt; m and item[1][1] + 1 &lt; n: result[item[1][0] + 1][item[1][1] + 1] += item[2] for i in range(m): tmp = 0 for j in range(n): tmp += result[i][j] result[i][j] = tmp if i &gt; 0: for j in range(n): result[i][j] += result[i - 1][j] return result 方法二： 一行一行处理，简单方便 12345678910111213def getModifiedArray2(self, m, n, updates): # Write your code here result = [] for i in range(m): up2 = [] for item in updates: if i &gt;= item[0][0] and i &lt;= item[1][0]: up2.append([item[0][1], item[1][1], item[2]]) if len(up2)&gt;0: result.append(self.getModifiedArray(n, up2)) else: result.append([0]*n) return result ​ Leetcode 370.]]></content>
      <categories>
        <category>算法和数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java常见锁]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%B9%B6%E5%8F%91%2F%E5%B8%B8%E8%A7%81%E9%94%81%2F</url>
    <content type="text"><![CDATA[闭锁（Latch） 闭锁（Latch）：一种同步方法，可以延迟线程的进度直到线程到达某个终点状态。也就是说闭锁的状态是一次性的，它确保在闭锁打开之前所有特定的活动都需要在闭锁打开之后才能完成。 CountDownLatch是 JDK 5+里面闭锁的一个实现，允许一个或者多个线程等待某个事件的发生。CountDownLatch有一个正数计数器，countDown 方法对计数器做减操作，await 方法等待计数器达到0。所有await 的线程都会阻塞直到计数器为0或者等待线程中断或者超时。 APIpublic void await() throws InterruptedException public boolean await(long timeout, TimeUnit unit) throws InterruptedException public void countDown() public long getCount() 示例public long timecost(final int times, final Runnable task) throws InterruptedException { if (times &lt;= 0) throw new IllegalArgumentException(); final CountDownLatch startLatch = new CountDownLatch(1);//开始的锁 final CountDownLatch overLatch = new CountDownLatch(times);//结束任务的锁 for (int i = 0; i &lt; times; i++) { new Thread(new Runnable() { public void run() { try { startLatch.await();//阻塞所有线程 task.run(); } catch (InterruptedException ex) { Thread.currentThread().interrupt(); } finally { overLatch.countDown();//任务数减一 } } }).start(); } long start = System.nanoTime(); startLatch.countDown(); //开始任务，因为count为1， overLatch.await(); //阻塞至所有线程都执行完 return System.nanoTime() - start; } 第一个闭锁确保在所有线程开始执行任务前，所有准备工作都已经完成，一旦准备工作完成了就调用startLatch.countDown()打开闭锁，所有线程开始执行。第二个闭锁在于确保所有任务执行完成后主线程才能继续进行，这样保证了主线程等待所有任务线程执行完成后才能得到需要的结果。 await()内部直接调用了AQS的acquireSharedInterruptibly(1)。 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); } 共享锁是说所有共享锁的线程共享同一个资源，一旦任意一个线程拿到共享资源，那么所有线程就都拥有的同一份资源。也就是通常情况下共享锁只是一个标志，所有线程都等待这个标识是否满足，一旦满足所有线程都被激活（相当于所有线程都拿到锁一样）。这里的闭锁 CountDownLatch就是基于共享锁的实现 public int tryAcquireShared(int acquires) { return getState() == 0? 1 : -1; } 闭锁而言第一次await 时tryAcquireShared 应该总是-1 private void doAcquireSharedInterruptibly(int arg)throws InterruptedException { final Node node = addWaiter(Node.SHARED); try { for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;parkAndCheckInterrupt()) break; } } catch (RuntimeException ex) { cancelAcquire(node); throw ex; } // Arrive here only if interrupted cancelAcquire(node); throw new InterruptedException(); } 将当前线程节点以共享模式加入 AQS的 CLH队列中（相关概念参考这里和这里）。进行2。 检查当前节点的前任节点，如果是头结点并且当前闭锁计数为0就将当前节点设置为头结点，唤醒继任节点，返回（结束线程阻塞）。否则进行3。 检查线程是否该阻塞，如果应该就阻塞(park)，直到被唤醒（unpark）。重复2。 如果2、3有异常就抛出异常（结束线程阻塞）。 private void setHeadAndPropagate(Node node, int propagate) { setHead(node); if (propagate &gt; 0 &amp;&amp; node.waitStatus != 0) { Node s = node.next; if (s == null || s.isShared()) unparkSuccessor(node); } } countDown 应该就是在条件满足（计数为0）时唤醒头结点（时间最长的一个节点），然后头结点就会根据FIFO 队列唤醒整个节点列表（如果有的话）。 countDown()直接调用的是 AQS的releaseShared(1) public boolean tryReleaseShared(int releases) { for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; } } CyclicBarrier举例public class CyclicBarrierDemo { final CyclicBarrier barrier; final int MAX_TASK; public CyclicBarrierDemo(int cnt) { barrier = new CyclicBarrier(cnt + 1); MAX_TASK = cnt; } public void doWork(final Runnable work) { new Thread() { public void run() { work.run(); try { int index = barrier.await(); doWithIndex(index); } catch (InterruptedException e) { return; } catch (BrokenBarrierException e) { return; } } }.start(); } private void doWithIndex(int index) { if (index == MAX_TASK / 3) { System.out.println(&quot;Left 30%.&quot;); } else if (index == MAX_TASK / 2) { System.out.println(&quot;Left 50%&quot;); } else if (index == 0) { System.out.println(&quot;run over&quot;); } } public void waitForNext() { try { doWithIndex(barrier.await()); } catch (InterruptedException e) { return; } catch (BrokenBarrierException e) { return; } } public static void main(String[] args) { final int count = 10; CyclicBarrierDemo demo = new CyclicBarrierDemo(count); for (int i = 0; i &lt; 100; i++) { demo.doWork(new Runnable() { public void run() { //do something try { Thread.sleep(1000L); } catch (Exception e) { return; } } }); if ((i + 1) % count == 0) { demo.waitForNext(); } } } } 清单1描述的是一个周期性处理任务的例子，在这个例子中有一对的任务（100个），希望每10个为一组进行处理，当前仅当上一组任务处理完成后才能进行下一组，另外在每一组任务中，当任务剩下50%，30%以及所有任务执行完成时向观察者发出通知。 await()方法将挂起线程，直到同组的其它线程执行完毕才能继续await()方法返回线程执行完毕的索引，注意，索引时从任务数-1开始的，也就是第一个执行完成的任务索引为parties-1,最后一个为0，这个parties 为总任务数，清单中是cnt+1CyclicBarrier 是可循环的 APIpublic CyclicBarrier(int parties) 创建一个新的CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，但它不会在启动barrier 时执行预定义的操作。 public CyclicBarrier(int parties, Runnable barrierAction)创建一个新的yclicBarrier，它将在 给定数量的参与者（线程）处于等待状态时启动，并在启动barrier 时执行给定的屏障操作， 该操作由最后一个进入barrier 的线程执行。 public int await() throws InterruptedException, BrokenBarrierException 在所有参与者都已经在此barrier 上调用await 方法之前，将一直等待。 public int await(long timeout,TimeUnit unit) throws InterruptedException,BrokenBarrierException,TimeoutException 在所有参与者都已经在此屏障上调用await 方法,之前将一直等待,或者超出了指定的等待时间。 public int getNumberWaiting()返回当前在屏障处等待的参与者数目。此方法主要用于调试和断言。 public int getParties()返回要求启动此barrier 的参与者数目。 public boolean isBroken()查询此屏障是否处于损坏状态。 public void reset()将屏障重置为其初始状态。 CyclicBarrier.await*()private int dowait(boolean timed, long nanos)throws InterruptedException, BrokenBarrierException,TimeoutException { final ReentrantLock lock = this.lock; lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } int index = --count; if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); }catch (InterruptedException ie) { if (g == generation &amp;&amp; ! g.broken) { breakBarrier(); throw ie; } else { Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed &amp;&amp; nanos &lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); } } CyclicBarrier 的特点是要么大家都正常执行完毕，要么大家都异常被中断，不会其中有一个被中断而其它正常执行完毕的现象存在。这种特点叫all-or-none 由于有竞争资源的存在（broken/index），所以毫无疑问需要一把锁lock。 检查是否存在中断位(broken)，如果存在就立即以BrokenBarrierException 异常返回。 检查当前线程是否被中断，如果是那么就设置中断位（使其它将要进入等待的线程知道），另外唤醒已经等待的线程，同时以InterruptedException 异常返回，表示线程要处理中断 将剩余任务数减1，如果此时剩下的任务数为0，也就是达到了公共屏障点，那么就执行屏障点任务（如果有的话），同时创建新的Generation（在这个过程中会唤醒其它所有线程，因此当前线程是屏障点线程，那么其它线程就都应该在等待状态）。 下面的for 循环中就是要park 线程。这里park 线程采用的是Condition.await()方法。也就是trip.await()。为什么需要Condition？因为所有的await()其实等待的都是一个条件，一旦条件满足就应该都被唤醒，所以Condition 整好满足这个特点. 生成下一个循环周期并唤醒其它线程private void nextGeneration() { trip.signalAll(); count = parties; generation = new Generation(); } 在这个过程中当然需要使用Condition.signalAll()唤醒所有已经执行完成并且正在等待的线程。另外这里count 描述的是还有多少线程需要执行，是为了线程执行完毕索引计数。 #SemaphoreSemaphore 是一个计数信号量。从概念上讲，信号量维护了一个许可集。如有必要，在许可可用前会阻塞每一个acquire()，然后再获取该许可。每个release() 添加一个许可，从而可能释放一个正在阻塞的获取者 Semaphore 是一个计数器，在计数器不为0的时候对线程就放行，一旦达到0，那么所有请求资源的新线程都会被阻塞，包括增加请求到许可的线程，也就是说Semaphore 不是可重入的。每一次请求一个许可都会导致计数器减少1，同样每次释放一个许可都会导致计数器增加1，一旦达到了0，新的许可请求线程将被挂起。 对象池public class ObjectCache&lt;T&gt; { public interface ObjectFactory&lt;T&gt; { T makeObject(); } class Node { T obj; Node next; } final int capacity; final ObjectFactory&lt;T&gt; factory; final Lock lock = new ReentrantLock(); final Semaphore semaphore; private Node head; private Node tail; public ObjectCache(int capacity, ObjectFactory&lt;T&gt; factory) { this.capacity = capacity; this.factory = factory; this.semaphore = new Semaphore(this.capacity); this.head = null; this.tail = null; } public T getObject() throws InterruptedException { semaphore.acquire(); return getNextObject(); } private T getNextObject() { lock.lock(); try { if (head == null) { return factory.makeObject(); } else { Node ret = head; head = head.next; if (head == null) tail = null; ret.next = null;// help GC return ret.obj; } } finally { lock.unlock(); } } private void returnObjectToPool(T t) { lock.lock(); try { Node node = new Node(); node.obj = t; if (tail == null) { head = tail = node; } else { tail.next = node; tail = node; } } finally { lock.unlock(); } } public void returnObject(T t) { returnObjectToPool(t); semaphore.release(); } } 此对象池最多支持capacity 个对象，这在构造函数中传入。对象池有一个基于FIFO 的队列，每次从对象池的头结点开始取对象，如果头结点为空就直接构造一个新的对象返回。否则将头结点对象取出，并且头结点往后移动。特别要说明的如果对象的个数用完了，那么新的线程将被阻塞，直到有对象被返回回来。返还对象时将对象加入FIFO 的尾节点并且释放一个空闲的信号量，表示对象池中增加一个可用对象。 信号获取方法公平信号获取方法protected int tryAcquireShared(int acquires) { Thread current = Thread.currentThread(); for (;;) { Thread first = getFirstQueuedThread(); if (first != null &amp;&amp; first != current) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; } } 非公平信号获取方法protected int tryAcquireShared(int acquires) { return nonfairTryAcquireShared(acquires); } final int nonfairTryAcquireShared(int acquires) { for (;;) { int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; } } 公平信号和非公平信号在于第一次尝试能否获取信号时，公平信号量总是将当前线程进入AQS 的CLH 队列进行排队（因为第一次尝试时队列的头结点线程很有可能不是当前线程，当然不排除同一个线程第二次进入信号量），从而根据AQS的CLH 队列的顺序FIFO 依次获取信号量；而对于非公平信号量，第一次立即尝试能否拿到信号量，一旦信号量的剩余数available 大于请求数（acquires 通常为1），那么线程就立即得到了释放，而不需要进行AQS 队列进行排队。 所以非公平信号量的吞吐量总是要比公平信号量的吞吐量要大，但是需要强调的是非公平信号量和非公平锁一样存在“饥渴死”的现象，也就是说活跃线程可能总是拿到信号量，而非活跃线程可能难以拿到信号量。而对于公平信号量由于总是靠请求的线程的顺序来获取信号量，所以不存在此问题。 读写锁(ReadWriteLock)ReadWriteLock 描述的是：一个资源能够被多个读线程访问，或者被一个写线程访问，但是不能同时存在读写线程。也就是说读写锁使用的场合是一个共享资源被大量读取操作，而只有少量的写操作（修改数据）。清单1描述了ReadWriteLock 的API。 ReadWriteLock 接口public interface ReadWriteLock { Lock readLock(); Lock writeLock(); } ReadWriteLock 并不是Lock 的子接口，只不过ReadWriteLock 借助Lock 来实现读写两个视角。在ReadWriteLock 中每次读取共享数据就需要读取锁，当需要修改共享数据时就需要写入锁。看起来好像是两个锁，但其实不尽然 ReentrantReadWriteLock的特性公平性 o 非公平锁（默认） 这个和独占锁的非公平性一样，由于读线程之间没有锁竞争，所以读操作没有公平性和非公平性，写操作时，由于写操作可能立即获取到锁，所以会推迟一个或多个读操作或者写操作。因此非公平锁的吞吐量要高于公平锁。 o 公平锁利用AQS 的CLH 队列，释放当前保持的锁（读锁或者写锁）时，优先为等待时间最长的那个写线程分配写入锁，当前前提是写线程的等待时间要比所有读线程的等待时间要长。同样一个线程持有写入锁或者有一个写线程已经在等待了，那么试图获取公平锁的（非重入）所有线程（包括读写线程）都将被阻塞，直到最先的写线程释放锁。如果读线程的等待时间比写线程的等待时间还有长，那么一旦上一个写线程释放锁，这一组读线程将获取锁。 重入性 o 读写锁允许读线程和写线程按照请求锁的顺序重新获取读取锁或者写入锁。当然了只有写线程释放了锁，读线程才能获取重入锁。 o 写线程获取写入锁后可以再次获取读取锁，但是读线程获取读取锁后却不能获取写入锁。 o 另外读写锁最多支持65535个递归写入锁和65535个递归读取锁。 锁降级 o 写线程获取写入锁后可以获取读取锁，然后释放写入锁，这样就从写入锁变成了读取锁，从而实现锁降级的特性。 锁升级 o 读取锁是不能直接升级为写入锁的。因为获取一个写入锁需要释放所有读取锁，所以如果有两个读取锁视图获取写入锁而都不释放读取锁时就会发生死锁。锁获取中断 o 读取锁和写入锁都支持获取锁期间被中断。这个和独占锁一致。 条件变量 o 写入锁提供了条件变量(Condition)的支持，这个和独占锁一致，但是读取锁却不允许获取条件变量，将得到一个UnsupportedOperationException 异常。 重入数 o 读取锁和写入锁的数量最大分别只能是65535（包括重入数）。 ReentrantReadWriteLock实现ReentrantReadWriteLock 里面有两个类：ReadLock/WriteLock，这两个类都是Lock 的实现。 ReadLock片段public static class ReadLock implements Lock, java.io.Serializable { private final Sync sync; protected ReadLock(ReentrantReadWriteLock lock) { sync = lock.sync; } public void lock() { sync.acquireShared(1); } public void lockInterruptibly() throws InterruptedException { sync.acquireSharedInterruptibly(1); } public boolean tryLock() { return sync.tryReadLock(); } public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout)); } public void unlock() { sync.releaseShared(1); } public Condition newCondition() { throw new UnsupportedOperationException(); } } WriteLock片段public static class WriteLock implements Lock, java.io.Serializable { private final Sync sync; protected WriteLock(ReentrantReadWriteLock lock) { sync = lock.sync; } public void lock() { sync.acquire(1); } public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } public boolean tryLock( ) { return sync.tryWriteLock(); } public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1, unit.toNanos(timeout)); } public void unlock() { sync.release(1); } public Condition newCondition() { return sync.newCondition(); } public boolean isHeldByCurrentThread() { return sync.isHeldExclusively(); } public int getHoldCount() { return sync.getWriteHoldCount(); } } 显然WriteLock 就是一个独占锁，这和ReentrantLock 里面的实现几乎相同，都是使用了AQS 的acquire/release 操作。当然了在内部处理方式上与ReentrantLock 还是有一点不同的。对比可以看到，ReadLock 获取的是共享锁，WriteLock 获取的是独占锁。 在AQS 章节中介绍到AQS 中有一个state 字段（int 类型，32位）用来描述有多少线程获持有锁。在独占锁的时代这个值通常是0或者1（如果是重入的就是重入的次数），在共享锁的时代就是持有锁的数量。在上一节中谈到，ReadWriteLock 的读、写锁是相关但是又不一致的，所以需要两个数来描述读锁（共享锁）和写锁（独占锁）的数量。显然现在一个state就不够用了。于是在ReentrantReadWrilteLock 里面将这个字段一分为二，高位16位表示共享锁的数量，低位16位表示独占锁的数量（或者重入数量）。2^16-1=65536，这就是上节中提到的为什么共享锁和独占锁的数量最大只能是65535的原因了。 写入锁获取protected final boolean tryAcquire(int acquires) { Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) { if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); } if ((w == 0 &amp;&amp; writerShouldBlock(current)) ||!compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true; } //公平读写锁写线程是否阻塞 final boolean writerShouldBlock(Thread current) { return !isFirst(current); } 非公平读写锁写线程是否阻塞 final boolean writerShouldBlock(Thread current) { return false; } 持有锁线程数非0（c=getState()不为0），如果写线程数（w）为0（那么读线程数就不为0）或者独占锁线程（持有锁的线程）不是当前线程就返回失败，或者写入锁的数量（其实是重入数）大于65535就抛出一个Error 异常。否则进行2。 如果当且写线程数位0（那么读线程也应该为0，因为步骤1已经处理c!=0的情况），并且当前线程需要阻塞那么就返回失败；如果增加写线程数失败也返回失败。否则进行3。 设置独占线程（写线程）为当前线程，返回true。 写入锁释放protected final boolean tryRelease(int releases) { int nextc = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); if (exclusiveCount(nextc) == 0) { setExclusiveOwnerThread(null); setState(nextc); return true; } else { setState(nextc); return false; } } 读取锁获取protected final int tryAcquireShared(int unused) { Thread current = Thread.currentThread(); int c = getState(); if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (!readerShouldBlock(current) &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != current.getId()) cachedHoldCounter = rh = readHolds.get(); rh.count++; return 1; } return fullTryAcquireShared(current); } final int fullTryAcquireShared(Thread current) { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != current.getId()) rh = readHolds.get(); for (;;) { int c = getState(); int w = exclusiveCount(c); if ((w != 0 &amp;&amp; getExclusiveOwnerThread() != current) ||((rh.count | w) == 0 &amp;&amp; readerShouldBlock(current))) return -1; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) { cachedHoldCounter = rh; // cache for release rh.count++; return 1; } } } 如果写线程持有锁（也就是独占锁数量不为0），并且独占线程不是当前线程，那么就返回失败。因为允许写入线程获取锁的同时获取读取锁。否则进行2。 如果读线程请求锁数量达到了65535（包括重入锁），那么就跑出一个错误Error，否则进行3。 如果读线程不用等待（实际上是是否需要公平锁），并且增加读取锁状态数成功，那么就返回成功，否则进行4。 步骤3失败的原因是CAS 操作修改状态数失败，那么就需要循环不断尝试去修改状态直到成功或者锁被写入线程占有。实际上是过程3的不断尝试直到CAS 计数成功或者被写入线程占有锁。 读取锁释放过程protected final boolean tryReleaseShared(int unused) { HoldCounter rh = cachedHoldCounter; Thread current = Thread.currentThread(); if (rh == null || rh.tid != current.getId()) rh = readHolds.get(); if (rh.tryDecrement() &lt;= 0) throw new IllegalMonitorStateException(); for (;;) { int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) return nextc == 0; } } HoldCounter 作用其实就是当前线程持有共享锁（读取锁）的数量，包括重入的数量。那么这个数量就必须和线程绑定在一起。HoldCounter是绑定到线程上的一个计数器。 static final class HoldCounter { int count; final long tid = Thread.currentThread().getId(); int tryDecrement() { int c = count; if (c &gt; 0) count = c - 1; return c; } } static final class ThreadLocalHoldCounter extends ThreadLocal&lt;HoldCounter&gt; { public HoldCounter initialValue() { return new HoldCounter(); } } 读取锁的tryLock()很像tryAcquireShared(int unused) final boolean tryReadLock() { Thread current = Thread.currentThread(); for (;;) { int c = getState(); if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return false; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != current.getId()) cachedHoldCounter = rh = readHolds.get(); rh.count++; return true; } } } 写入锁的tryLock()很像tryAcquire(int acquires),这里的acquires==1 final boolean tryWriteLock() { Thread current = Thread.currentThread(); int c = getState(); if (c != 0) { int w = exclusiveCount(c); if (w == 0 ||current != getExclusiveOwnerThread()) return false; if (w == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); } if (!compareAndSetState(c, c + 1)) return false; setExclusiveOwnerThread(current); return true; } 总结所有锁（包括内置锁和高级锁）都是有性能消耗的，也就是说在高并发的情况下，由于锁机制带来的上下文切换、资源同步等消耗是非常可观的。在某些极端情况下，线程在锁上的消耗可能比线程本身的消耗还要多。所以如果可能的话，在任何情况下都尽量少用锁，如果不可避免那么采用非阻塞算法是一个不错的解决方案，但是却也不是绝对的。 内部锁Java 语言通过synchronized 关键字来保证原子性。这是因为每一个Object 都有一个隐含的锁，这个也称作监视器对象。在进入synchronized 之前自动获取此内部锁，而一旦离开此方式都会自动释放锁。显然这是一个独占锁。相对于前面介绍的众多高级锁（Lock/ReadWriteLock 等），synchronized 的代价都比后者要高。但是synchronized 的语法比较简单，而且也比较容易使用和理解，不容易写法上的错误。而我们知道Lock 一旦调用了lock()方法获取到锁而未正确释放的话很有可能就死锁了。所以Lock 的释放操作总是跟在finally 代码块里面，这在代码结构上也是一次调整和冗余。 性能由于锁总是带了性能影响，所以是否使用锁和使用锁的场合就变得尤为重要。如果在一个高并发的Web 请求中使用了强制的独占锁，那么就可以发现Web 的吞吐量将急剧下降。 为了利用并发来提高性能，出发点就是：更有效的利用现有的资源，同时让程序尽可能的开拓更多可用的资源 线程阻塞当锁竞争的时候，失败的线程必然会发生阻塞。JVM 既能自旋等待（不断尝试，直到成功， 很多CAS 就是这样实现的），也能够在操作系统中挂起阻塞的线程，直到超时或者被唤醒。通常情况下这取决于上下文切换的开销以及与获取锁需要等待的时间二者之间的关系。自旋等待适合于比较短的等待，而挂起线程比较适合那些比较耗时的等待。 锁竞争 减少锁持有的时间 减少锁请求的频率 采用共享锁取代独占锁死锁如果一个线程永远不释放另外一个线程需要的资源那么就会导致死锁。活锁是线程总是尝试某项操作却总是失败的情况。这种情况下尽管线程没有被阻塞，但是人物却总是不能被执行。 两个人过独木桥，如果在半路相撞，双方礼貌退出去然后再试一次。如果总是失败，那么这两个任务将一直无法得到执行。]]></content>
      <categories>
        <category>java基础</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[背包问题]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%AE%97%E6%B3%95%2F%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[01背包题目有N件物品和一个容量为V 的背包。放入第i件物品耗费的空间是Ci，得到的价值是Wi。求解将哪些物品装入背包可使价值总和最大。 基本思路特点：每种物品仅有一件，可以选择放或不放。 用子问题定义状态：即F[i,v]表示前i件物品恰放入一个容量为v的背包可以获得的最大价值。则其状态转移方程便是： F[i,v] = max{F[i-1,v] , F[i-1,v-Ci]+Wi} 如果不放第i件物品，那么问题就转化为“前i-1件物品放入容量为v的背包中”，价值为F[i-1,v]；如果放第i件物品，那么问题就转化为“前i-1件物品放入剩下的容量为v-Ci的背包中”，此时能获得的最大价值就是F[i-1; v-Ci]再加上通过放入第i件物品获得的价值Wi。 伪代码如下： F[0,0..V]=0 for i=1..N for v=c[i]..V F[i,v]=max{f[i-1,v],f[i-1,v-c[i]]+w[i]}; 优化空间复杂度F[0,0..V]=0 for i=1..N for v=V..c[i] f[v]=max{f[v],f[v-c[i]]+w[i]}; f[i][v]是由f[i-1][v]和f[i-1][v-c[i]]两个子问题递推而来，在每次主循环中我们以v=V..0的顺序推f[v]，能保证推f[v]时f[v-c[i]]保存的是状态f[i-1][v-c[i]]的值。 当v=v’时, 此时的f中 &gt;v’ 保留的是计算了i物品后的最大价值, &lt;=v’ 保留的是计算了i-1物品后的最大价值 求解路径 非优化空间的 for(i=N,j=V;i&gt;0,j&gt;0;i–) if(F[i][j]=F[i-1][j-C[i]]+W[i]){ Print W[i] j←j-C[i] } 优化后 没想到好办法, 类似与非优化的, 也是O(NV) 初始化细节 恰好装满背包在初始化时除了f[0]为0其它f[1..V]均设为-∞，这样就可以保证最终得到的f[N]是一种恰好装满背包的最优解。 只希望价格尽量大初始化时应该将f[0..V]全部设为0。 初始化的f数组事实上就是在没有任何物品可以放入背包时的合法状态。如果要求背包恰好装满，那么此时只有剩余容量为0的背包可能被价值为0的nothing“恰好装满”，其它容量的背包均没有合法的解，属于未定义的状态，它们的值就都应该是-∞了。如果背包并非必须被装满，那么任何容量的背包都有一个合法解“什么都不装”，这个解的价值为0 必须满情况下装不满, 则该值为-∞ 完全背包问题有N种物品和一个容量为V 的背包，每种物品都有无限件可用。放入第i种物品的耗费的空间是Ci，得到的价值是Wi。求解：将哪些物品装入背包，可使这些物品的耗费的空间总和不超过背包容量，且价值总和最大。 基本思路这个问题非常类似于01背包问题，所不同的是每种物品有无限件。也就是从每种物品的角度考虑，与它相关的策略已并非取或不取两种，而是有取0件、取1件、取2件……直至取⌊V /Ci⌋件等很多种。 简单改进01背包的状态转移方程:f[i][v]=max{f[i-1][v-kc[i]]+kw[i]} 但是我们用: for i=1..N for v=c[i]..V f[v]=max{f[v],f[v-c[i]]+w[i]}; 这个和01背包很像,只是v的顺序倒置了,思考:如果是V..0, 则第i件物品只用了一次,因为&lt;=v’保留的是计算了i-1物品后的最大价值;如果是0..V,&lt;=v’的最大容量中第i个物品可能多次利用,即第二重循环为多次加入第i个物品后的最优解 多重背包问题问题有N种物品和一个容量为V的背包。第i种物品最多有n[i]件可用，每件费用是c[i]，价值是w[i]。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。 基本思路 被淘汰的方案： 按k*w[i]处理, 再按01背包处理 12345&gt; for i=1..N&gt; for v=V..c[i]&gt; for k=1..i/c[i]&gt; f[v]=max&#123;f[v],f[v-k*c[i]]+k*w[i]&#125;;&gt; 这里第三层循环内的元素是互斥的 和完全背包类似: f[i][v]=max{f[i-1][v-kc[i]]+kw[i] | 0&lt;=k&lt;=n[i]}复杂度是O(V*Σn[i])。 转化为01背包问题我们考虑把第i种物品换成若干件物品，使得原问题中第i种物品可取的每种策略取0..n[i]件均能等价于取若干件代换以后的物品。 方法是：将第i种物品分成若干件物品，其中每件物品有一个系数，这件物品的费用和价值均是原来的费用和价值乘以这个系数。使这些系数分别为1,2,4,…,2^(k-1),n[i]-2^k+1，且k是满足n[i]-2^k+1&gt;0的最大整数。例如，如果n[i]为13，就将这种物品分成系数分别为1,2,4,8的四件物品。 这样就将第i种物品分成了O(log n[i])种物品，将原问题转化为了复杂度为O(V*Σlog n[i])的01背包问题 伪代码: //表示处理一件01背包中的物品 procedure ZeroOnePack(cost,weight) for v=V..cost f[v]=max{f[v],f[v-cost]+weight} //处理一件完全背包类物品 procedure CompletePack(cost,weight) for v=cost..V f[v]=max{f[v],f[v-c[i]]+w[i]} //多重背包 procedure MultiplePack(cost,weight,amount) if cost*amount&gt;=V CompletePack(cost,weight) //完全背包 return integer k=1 while k&lt;num ZeroOnePack(k*cost,k*weight) //01背包 amount=amount-k k=k*2 ZeroOnePack(amount*cost,amount*weight) //01背包 混合三种背包问题有的物品只可以取一次（01背包），有的物品可以取无限次（完全背包），有的物品可以取的次数有一个上限（多重背包） for i=1..N if 第i件物品是01背包 ZeroOnePack(c[i],w[i]) else if 第i件物品是完全背包 CompletePack(c[i],w[i]) else if 第i件物品是多重背包 MultiplePack(c[i],w[i],n[i]) 二维费用的背包问题问题二维费用的背包问题是指：对于每件物品，具有两种不同的空间耗费，选择这件物品必须同时付出这两种代价。对于每种代价都有一个可付出的最大值（背包容量）。问怎样选择物品可以得到最大的价值。设这两种代价分别为代价1和代价2，第i件物品所需的两种代价分别为a[i]和b[i]。两种代价可付出的最大值（两种背包容量）分别为V和U。物品的价值为w[i]。 分析费用加了一维，只需状态也加一维即可。设f[i][v][u]表示前i件物品付出两种代价分别为v和u时可获得的最大价值。状态转移方程就是： f[i][v][u]=max{f[i-1][v][u],f[i-1][v-a[i]][u-b[i]]+w[i]} 可以转化为二维的, 把i取掉, 类似一维 分组的背包问题问题有N件物品和一个容量为V的背包。第i件物品的费用是Ci，价值是Wi。这些物品被划分为K组，每组中的物品互相冲突，最多选一件。求解将哪些物品装入背包可使这些物品的费用总和不超过背包容量，且价值总和最大。 分析这个问题变成了每组物品有若干种策略：是选择本组的某一件，还是一件都不选。也就是说设f[k][v]表示前k组物品花费费用v能取得的最大权值，则有： f[k][v]=max{f[k-1][v],f[k-1][v-c[i]]+w[i]|物品i属于第k组} 代码: for 所有的组k for v=V..0 for 所有的i属于组k f[v]=max{f[v],f[v-c[i]]+w[i]} 注意这里的三层循环的顺序，“for v=V..0”这一层循环必须在“for 所有的i属于组k”之外。这样才能保证每一组内的物品最多只有一个会被添加到背包中。 当v=v’时, 第三重循环会按最大值进行替换k中的i, 因为是替换, 所以k中只有一个元素添加进来 如果二三重循环互换, 和01背包没区别 有依赖的背包问题问题这种背包问题的物品间存在某种“依赖”的关系。也就是说，物品i依赖于物品j，表示若选物品i，则必须选物品j。为了简化起见，我们先设没有某个物品既依赖于别的物品，又被别的物品所依赖；另外，没有某件物品同时依赖多件物品。]]></content>
      <categories>
        <category>算法和数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[介绍红黑树，一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black。 通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其他路径长出俩倍，因而是接近平衡的。 红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。 性质 每个结点要么是红的要么是黑的。 根结点是黑的。 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。 如果一个结点是红的，那么它的两个儿子都是黑的。 对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。 旋转带互换颜色的旋转不改变红黑树性质 左旋 LeftRoate(T, x) y ← x.right //定义y：y是x的右孩子 x.right ← y.left //y的左孩子成为x的右孩子 if y.left ≠ T.nil y.left.p ← x y.p ← x.p //x的父结点成为y的父结点 if x.p = T.nil then T.root ← y else if x = x.p.left then x.p.left ← y else x.p.right ← y y.left ← x //x作为y的左孩子 x.p ← y 右旋 RIGHT-ROTATE(T, x) 1 y = x.left 2 x.left = y.right 3 if y.right != T.nil 4 y.right.p = x 5 y.p = x.p 6 if x.p == T.nil 7 T.root = y 8 elseif x == x.p.left 9 x.p.left = y 10 else 11 x.p.right = right 12 y.right = x 13 x.p = y 插入 插入节点是红色,如果父节点黑色,则完成,否则调整,有三种情况: 父红叔红, 则叔叔和父亲和祖父换色, 递归调整祖父 父左红叔黑我右,则父左旋;父右红叔黑我左,则父右旋; 父左红叔黑我左,则祖父右旋;父右红叔黑我右,则祖父与父换色,祖父左旋; 综合简述 父红叔红, 则叔叔和父亲和祖父换色, 递归调整祖父 叔黑父我不同向, 父向父方向旋转 叔黑父我同向, 祖父和父换色,祖父向父反向旋转 插入函数 RB-INSERT(T, z) y ← nil x ← T.root while x ≠ T.nil do y ← x if z.key &lt; x.key then x ← x.left else x ← x.right z.p ← y if y == nil[T] then T.root ← z else if z.key &lt; y.key then y.left ← z else y.right ← z z.left ← T.nil z.right ← T.nil z.color ← RED RB-INSERT-FIXUP(T, z) 调整函数 RB-INSERT-FIXUP(T, z) while T.root!=z &amp;&amp; z.p.color == RED if z.p == z.p.p.left then y ← z.p.p.right if y.color == RED then z.p.color ← BLACK ▹ Case 1 y.color ← BLACK ▹ Case 1 z.p.p.color ← RED ▹ Case 1 z ← z.p.p ▹ Case 1 continue else if z == z.p.right then z ← z.p ▹ Case 2 LEFT-ROTATE(T, z) ▹ Case 2 z.p.color ← BLACK ▹ Case 3 z.p.p.color ← RED ▹ Case 3 RIGHT-ROTATE(T, z.p.p) ▹ Case 3 else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) T.root.color ← BLACK 删除二叉搜索树结点删除 没有儿子，即为叶结点。直接把父结点的对应儿子指针设为NULL，删除儿子结点。 只有一个儿子。那么把父结点的相应儿子指针指向儿子的独生子，删除儿子结点。 有两个儿子。你删除节点之后，选择后继节点补充 为什么是后继？注意这里已知必有2个子节点，后继只可能是当前节点的右子树，且保证其左子节点为null，而前驱不行 红黑树类似 只删除节点代码 RB-TRANSPLANT(T,u,v) //该函数将指向u的父节点指向v RB-DBELETE(T,z) y=z; y-original-color = y.color //表示删除的节点的颜色 if z.left == T.nil //左为空,右来替换 x = z.right //x指向被提拔的节点 RB-TRANSPLANT(T,z,z.right) elseif z.right == T.nil //右为空,左来替换 x = z.left RB-TRANSPLANT(T,z,z.left) else y = TREE-MINIMUM(z.right) //左右不空,后继节点替换,类似搜索树,等效于删除了后继节点 y-original-color = y.color x = y.right if y.p == z x.p = y else RB-TRANSPLANT(T, y, y.right) y.right = z.right y.right.p = y RB-TRANSPLANT(T, z, y) //y替换z y.left = z.left //此时的z一定没有左节点(后继节点) y.left.p = y y.color = z.color if y-original-color == BLACK //删除了一个黑节点,要修改树 RB-DELETE-FIXUP(T,x) 调用RB-DELETE-FIXUP(T,x)时x是红黑或双黑的,只是额外的黑色是针对x节点的,而不是color属性 RB-DELETE-FIXUP(T, x) 1 while x != T.root and x.color == BLACK 2 if x == x.p.left 3 w = x.p.right 4 if w.color == RED 5 w.color = BLACK // case 1 6 x.p.color = RED // case 1 7 LEFT-ROTATE(T, x.p) // case 1 8 w = x.p.right // case 1 9 if w.left.color == BLACK and w.right.color == BLACK 10 w.color = RED // case 2 11 x = x.p // case 2 12 else 13 if w.right.color == BLACK 14 w.left.color = BLACK // case 3 15 w.color = RED // case 3 16 RIGHT-ROTATE(T, w) // case 3 17 w = x.p.right // case 3 18 w.color = x.p.color // case 4 19 x.p.color = BLACK // case 4 20 w.right.color = BLACK // case 4 21 LEFT-ROTATE(T, x.p) // case 4 22 x = T.root // case 4 23 else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) 24 x.color = BLACK x 总是指向具有双重黑色的那个非根结点,while 循环的目标是将额外的黑色沿树上移，直到： x 指向一个红黑结点，此时，在第24行，将 x 着为黑色； x 指向根，这是可以简单地消除额外的黑色，或者 做必要的旋转和颜色改变。 情况1 ： x 的兄弟 w 是红色的见 RB-DELETE-FIXUP 第5～8行和上图a。因为 w 必须有红色孩子，我们可以改变 w 和 x.p 的颜色，再对 x.p 做一次左旋，而且红黑性质得以继续保持， x 的新兄弟是旋转之前 w 的某个孩子，其颜色为黑色。这样，情况1就转换成情况2，3或4。 情况2 ： x 的兄弟 w 是黑色的，且 w 的两个孩子都是黑色的见 RB-DELETE-FIXUP 第10~11行和上图b。因为 w 和两个孩子都是黑色的，故从 x 和 w 上各去掉一重黑色，从而 x 只有一重黑色而 w 为红色。为了补偿去掉的黑色，需要在原 x.p 内新增一重额外黑色。然后新结点 x 在最后被着为黑色。 情况3 ： x 的兄弟 w 是黑色的， w 的左孩子是红色的，右孩子是黑色的见 RB-DELETE-FIXUP 第14～17行和上图c。此时可以交换 w 和其左孩子 w.left 的颜色，并对 w 右旋，而红黑性质依然保持，且从情况3转换成了情况1。 情况4 ： x 的兄弟 w 是黑色的，且 w 的右孩子是红色的见 RB-DELETE-FIXUP 第18～22行和上图d。通过做颜色的修改并对 x.p 做一次左旋，可以去掉 x 的额外黑色并把它变成单独黑色。将 x 置为根后， while 会在测试其循环条件时结束。 简单总结: 删除只看兄弟节点 case1: 兄弟红,则对父亲向自己方向旋转(换色),进入case2 case2: 兄弟黑,兄弟两个儿子都黑, 兄变红，如果父黑则对父该递归 case3: 兄弟黑,兄弟与其红子反向,另一子为黑,兄弟向兄弟方向旋转(换色),进入case4 case4: 兄弟黑,兄弟与其红子同向,父亲向自己方向旋转(换色) 例子http://saturnman.blog.163.com/blog/static/557611201097221570/]]></content>
      <categories>
        <category>算法和数据结构</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FB%E6%A0%91%2F</url>
    <content type="text"><![CDATA[参考: http://blog.csdn.net/hguisu/article/details/7786014 http://blog.csdn.net/v_JULY_v/article/details/6530142/ http://blog.csdn.net/swordmanwk/article/details/6549480 算法导论 B树B树是为磁盘或其他直接存取的辅助设备而设计的一种平衡搜索树, 类似于红黑树, 但是在降低IO方面做的更好一些 定义B树是一种平衡的多路查找树，它在文件系统和数据库中很有用。 定义： B树内的每一个节点x都具有以下字段： 当前存储在节点x中的关键字（key）个数n[x]。 存储在x节点中的n[x]个关键字是以非降序的顺序排列的，即：key1[x] ≤key2[x]……≤keyn[x][x]。 一个表示x节点否是叶节点的bool值leaf[x]。 每个内节点拥有指向n[x]+1个指向叶节点的指针，c1[x]，c2[x]……cn[x]+1[x]。叶节点没有ci[x]域 节点x的key值，将子节点的key值范围分开了。如果ki是ci[x]指向的子节点的任意一个key的值，那么有：k1 ≤ key1[x] ≤ k2 ≤ key2[x] ≤ ≤ keyn[x][x] ≤ kn[x]+1. 所有的叶节点具有相同的深度，这个深度也就是树高h。 一个节点可容纳的关键字数目是有上下限的。这个界限可以用包含一个大于2的整数t的表达式来表示，这个数t称为B树的最小度。 除根节点外，每个节点至少要包含t-1个关键字（key），每个内节点（除根外）至少要包含t个关键字。一棵非空的B树，根节点至少要包含一个key。每个节点最多包含2t-1个关键字，因此，内节点最多有2t个子节点。对于一个包含2t-1个关键的节点，我们就称这个节点满了。 以子树c为例,n=2,[43,78]为关键码,其他为指向节点的指针 B树的高度h&lt;logt(n+1/2) Node数据结构typedef int KeyType ; #define m 5 /*B 树的阶，暂设为5*/ typedef struct Node{ int keynum; /* 结点中关键码的个数，即结点的大小*/ struct Node *parent; /*指向双亲结点*/ KeyType key[m+1]; /*关键码向量，0 号单元未用*/ struct Node *ptr[m+1]; /*子树指针向量*/ Record *recptr[m+1]; /*记录指针向量*/ }NodeType; /*B 树结点类型*/ 查找显然, B-树也是一种有序的结构,可以类似二叉查找树查找. 一般先在关键字中找, 没有则到子树中找, B-TREE-SEARCH(x, k) //在x中查找k 1 i = 1 2 while i &lt;= x.n and k &gt;= x.keyi //在关键字中查找k 3 i = i + 1 4 if i &lt;= x.n and k == x.keyi //在key[]中 5 return (x,i) 6 elseif x.leaf //key[]中没有且如果是叶节点 7 return NIL 8 else DISK-READ(x.ci) //子节点中找 9 return B-TREE-SEARCH(x.ci, k) 从查找算法中可以看出， 在B-树中进行查找包含两种基本操作: 在B树关键字中查找； 在结点中查找关键字。 由于B树通常存储在磁盘上， 则前一查找操作是在磁盘上进行的， 而后一查找操作是在内存中进行的，即在磁盘上找到指针p 所指结点后， 先将结点中的信息读入内存， 然后再利用顺序查找或折半查找查询等于K 的关键字。显然， 在磁盘上进行一次查找比在内存中进行一次查找的时间消耗多得多. 创建空树B-TREE-CREATE(T) 1 x = ALLOCATE-NODE() 2 x.leaf = TRUE 3 x.n = 0 4 DISK-WRITE(x) 5 T.root = x 分裂分裂是树长高的唯一途径 x是非满的节点, i指向x.ci的节点,该节点为满,满树有2t-1个关键字, 提最中间的关键字, 生成两个节点 插入单行程向下插入, 根满了就分裂, 保证不满,提中间元素为根, 再调用非满的插入函数 非满插入函数插入最终是在叶子节点上 3~7 插入关键字 9~17 尾递归的找到叶节点, 注意路径中非叶节点满则分裂, 保证尾递归的正确, 也是单行程向下插入 插入例子t=3, 关键字[2,5] ,节点[3,6] 删除与插入情况相对称，除了根结点外（根结点个数不能少于1），B树的关键字数不能少于t-1个。对于简单删除情况，如果我们定位到关键字处在某个结点中，如果这个结点中关键字个数恰好是t-1个，如果直接删除这个关键字，就会违反B树规则。 此时，需要考虑两种处理方案： 把这个结点与其相邻结点合并，合并时需要把父结点的一个关键字加进来，除非相邻的那个结点的关键字数也是t-1个，否则，合并后会超出2t-1的限制，同样违反B树规则。而且，因为从父结点拉下一个关键字，导致父结点的关键字数少1，如果原来父结点关键字数是t-1，那么父结点违反B树规则，这种情况下，必须进行回溯处理。（对于下图（a）初始树，删除结点Z就会出现这种情况） 从相邻结点借一个关键字过来，这种情况要求，相邻结点必须有多于t-1个关键字，借的过程中，需要转经父结点，否则违反B树规则。 为了避免回溯，要求我们在从树根向下搜索关键字的过程中，凡是遇到途经的结点，如果该结点的关键字数是t-1，则我们需要想办法从其他地方搞个关键字过来，使得该结点的关键字数至少为t。 搞，也是从相邻结点搞，如果相邻结点有的话，当然，也要经过父结点进行周转。如果没有，就说明相邻结点的关键字个数也是t-1，这种情况，直接对该结点与其相邻结点进行合并，以满足要求。 代码参考http://blog.csdn.net/swordmanwk/article/details/6549480 删除例子t=3, 关键字[2,5] ,节点[3,6] 思考插入路过的每个节点都非满, 删除路过的每个节点都非t-1, 根节点除外 插入非满保证在叶子节点插入时, 如果叶子节点满了, 则向父节点添加一个关键字, 若路过的每个节点都非满可以保证不用向上回溯 删除同理, 如果要删除的点关键子数为 t-1, 只能向兄弟借, 兄弟借不到就要合并点, 合并后父少个节点, 不满足, 回溯的满足父亲, 不是单向的 B+树与B树区别 有n棵子树的结点中含有n个关键字，每个关键字不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树（根结点）中的最大（或最小）关键字。通常在B+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。 B+树与操作系统的文件索引和数据库索引为什么说B+树比B 树更适合实际应用中操作系统的文件索引和数据库索引？ B+树的磁盘读写代价更低: B+树的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B 树更小。 B+树的查询效率更加稳定: 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 B*树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针；B*树定义了非叶子结点关键字个数至少为(2/3)*M，即块的最低使用率为2/3（代替B+树的1/2）； B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； B*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针； 所以，B*树分配新结点的概率比B+树要低，空间使用率更高；]]></content>
      <categories>
        <category>算法和数据结构</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>B树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java String]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%9F%BA%E7%A1%80%2FString%2F</url>
    <content type="text"><![CDATA[String基础不可变StringString对象是不可变的,每一个修改String值的方法,实际上都是创建了一个新的String对象,原初的String不变 APIString() 初始化一个新创建的 String 对象，使其表示一个空字符序列。 String(byte[] bytes) 通过使用平台的默认字符集解码指定的 byte 数组，构造一个新的 String。 String(byte[] bytes, Charset charset) 通过使用指定的 charset 解码指定的 byte 数组，构造一个新的 String。 String(byte[] ascii, int hibyte) 已过时。 该方法无法将字节正确地转换为字符。从 JDK 1.1 开始，完成该转换的首选方法是使用带有 Charset、字符集名称，或使用平台默认字符集的 String 构造方法。 String(byte[] bytes, int offset, int length) 通过使用平台的默认字符集解码指定的 byte 子数组，构造一个新的 String。 String(byte[] bytes, int offset, int length, Charset charset) 通过使用指定的 charset 解码指定的 byte 子数组，构造一个新的 String。 String(byte[] ascii, int hibyte, int offset, int count) 已过时。 该方法无法将字节正确地转换为字符。从 JDK 1.1 开始，完成该转换的首选方法是使用带有 Charset、字符集名称，或使用平台默认字符集的 String 构造方法。 String(byte[] bytes, int offset, int length, String charsetName) 通过使用指定的字符集解码指定的 byte 子数组，构造一个新的 String。 String(byte[] bytes, String charsetName) 通过使用指定的 charset 解码指定的 byte 数组，构造一个新的 String。 String(char[] value) 分配一个新的 String，使其表示字符数组参数中当前包含的字符序列。 String(char[] value, int offset, int count) 分配一个新的 String，它包含取自字符数组参数一个子数组的字符。 String(int[] codePoints, int offset, int count) 分配一个新的 String，它包含 Unicode 代码点数组参数一个子数组的字符。 String(String original) 初始化一个新创建的 String 对象，使其表示一个与参数相同的字符序列；换句话说，新创建的字符串是该参数字符串的副本。 String(StringBuffer buffer) 分配一个新的字符串，它包含字符串缓冲区参数中当前包含的字符序列。 String(StringBuilder builder) 分配一个新的字符串，它包含字符串生成器参数中当前包含的字符序列。 方法摘要 char charAt(int index) 返回指定索引处的 char 值。 int codePointAt(int index) 返回指定索引处的字符（Unicode 代码点）。 int codePointBefore(int index) 返回指定索引之前的字符（Unicode 代码点）。 int codePointCount(int beginIndex, int endIndex) 返回此 String 的指定文本范围中的 Unicode 代码点数。 int compareTo(String anotherString) 按字典顺序比较两个字符串。 int compareToIgnoreCase(String str) 按字典顺序比较两个字符串，不考虑大小写。 String concat(String str) 将指定字符串连接到此字符串的结尾。 boolean contains(CharSequence s) 当且仅当此字符串包含指定的 char 值序列时，返回 true。 boolean contentEquals(CharSequence cs) 将此字符串与指定的 CharSequence 比较。 boolean contentEquals(StringBuffer sb) 将此字符串与指定的 StringBuffer 比较。 static String copyValueOf(char[] data) 返回指定数组中表示该字符序列的 String。 static String copyValueOf(char[] data, int offset, int count) 返回指定数组中表示该字符序列的 String。 boolean endsWith(String suffix) 测试此字符串是否以指定的后缀结束。 boolean equals(Object anObject) 将此字符串与指定的对象比较。 boolean equalsIgnoreCase(String anotherString) 将此 String 与另一个 String 比较，不考虑大小写。 static String format(Locale l, String format, Object... args) 使用指定的语言环境、格式字符串和参数返回一个格式化字符串。 static String format(String format, Object... args) 使用指定的格式字符串和参数返回一个格式化字符串。 byte[] getBytes() 使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 byte[] getBytes(Charset charset) 使用给定的 charset 将此 String 编码到 byte 序列，并将结果存储到新的 byte 数组。 void getBytes(int srcBegin, int srcEnd, byte[] dst, int dstBegin) 已过时。 该方法无法将字符正确转换为字节。从 JDK 1.1 起，完成该转换的首选方法是通过 getBytes() 方法，该方法使用平台的默认字符集。 byte[] getBytes(String charsetName) 使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin) 将字符从此字符串复制到目标字符数组。 int hashCode() 返回此字符串的哈希码。 int indexOf(int ch) 返回指定字符在此字符串中第一次出现处的索引。 int indexOf(int ch, int fromIndex) 返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索。 int indexOf(String str) 返回指定子字符串在此字符串中第一次出现处的索引。 int indexOf(String str, int fromIndex) 返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始。 String intern() 返回字符串对象的规范化表示形式。 boolean isEmpty() 当且仅当 length() 为 0 时返回 true。 int lastIndexOf(int ch) 返回指定字符在此字符串中最后一次出现处的索引。 int lastIndexOf(int ch, int fromIndex) 返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索。 int lastIndexOf(String str) 返回指定子字符串在此字符串中最右边出现处的索引。 int lastIndexOf(String str, int fromIndex) 返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索。 int length() 返回此字符串的长度。 boolean matches(String regex) 告知此字符串是否匹配给定的正则表达式。 int offsetByCodePoints(int index, int codePointOffset) 返回此 String 中从给定的 index 处偏移 codePointOffset 个代码点的索引。 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 boolean regionMatches(int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 String replace(char oldChar, char newChar) 返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 String replace(CharSequence target, CharSequence replacement) 使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。 String replaceAll(String regex, String replacement) 使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 String replaceFirst(String regex, String replacement) 使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串。 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串。 boolean startsWith(String prefix) 测试此字符串是否以指定的前缀开始。 boolean startsWith(String prefix, int toffset) 测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 CharSequence subSequence(int beginIndex, int endIndex) 返回一个新的字符序列，它是此序列的一个子序列。 String substring(int beginIndex) 返回一个新的字符串，它是此字符串的一个子字符串。 String substring(int beginIndex, int endIndex) 返回一个新字符串，它是此字符串的一个子字符串。 char[] toCharArray() 将此字符串转换为一个新的字符数组。 String toLowerCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为小写。 String toLowerCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为小写。 String toString() 返回此对象本身（它已经是一个字符串！）。 String toUpperCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为大写。 String toUpperCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为大写。 String trim() 返回字符串的副本，忽略前导空白和尾部空白。 static String valueOf(boolean b) 返回 boolean 参数的字符串表示形式。 static String valueOf(char c) 返回 char 参数的字符串表示形式。 static String valueOf(char[] data) 返回 char 数组参数的字符串表示形式。 static String valueOf(char[] data, int offset, int count) 返回 char 数组参数的特定子数组的字符串表示形式。 static String valueOf(double d) 返回 double 参数的字符串表示形式。 static String valueOf(float f) 返回 float 参数的字符串表示形式。 static String valueOf(int i) 返回 int 参数的字符串表示形式。 static String valueOf(long l) 返回 long 参数的字符串表示形式。 static String valueOf(Object obj) 返回 Object 参数的字符串表示形式。 格式化输出类似c/c++ System.out.printf(&quot;Row 1: [%d %f]&quot;,x,y); System.out.format(&quot;Row 1: [%d %f]&quot;,x,y); Formatter%[argument_index$][flags][width][.precision]conversion %[参数索引][标识集][输出宽度][.限制字符]标明如何格式化字符 format(String format, Object... args) // 使用指定格式字符串和参数将一个格式化字符串写入此对象的目标文件中 Formatter f=new Formatter(System.out); f.format(&quot;%15s %5s %10.2f&quot;,name, qut, price ); 转换 参数类别 说明b Boolean值h 调用 Integer.toHexString(arg.hashCode()) 得到的结果。s 调用 arg.toString() 得到的结果。c 字符 结果是一个 Unicode 字符d 十进制整数o 八进制整数x 十六进制整数e 结果被格式化为用计算机科学记数法表示的十进制数f 结果被格式化为十进制数g 根据精度和舍入运算后的值，使用计算机科学记数形式或十进制格式对结果进行格式化。% 字面值 ‘%’ 正则表达式split()将字符串按从正则表达式地方切开 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串。 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串。 String replace(char oldChar, char newChar) 返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 String replace(CharSequence target, CharSequence replacement) 使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。 String replaceAll(String regex, String replacement) 使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 String replaceFirst(String regex, String replacement) 使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。 regex包在regex包中，包括了两个类，Pattern(模式类)和Matcher(匹配器类)。Pattern类是用来表达和陈述所要搜索模式的对象，Matcher类是真正影响搜索的对象。另加一个新的例外类，PatternSyntaxException，当遇到不合法的搜索模式时，会抛出例外。 用法: Pattern p=Pattern.compile(String regex); Matcher m=p.matcher(String args); 基础知识字符 x 字符 x \\ 反斜线字符 \0n 带有八进制值 0 的字符 n (0 &lt;= n &lt;= 7) \xhh 带有十六进制值 0x 的字符 hh \t 制表符 (&apos;\u0009&apos;) \n 新行（换行）符 (&apos;\u000A&apos;) 字符类 [abc] a、b 或 c（简单类） [^abc] 任何字符，除了 a、b 或 c（否定） [a-zA-Z] a 到 z 或 A 到 Z，两头的字母包括在内（范围） [a-d[m-p]] a 到 d 或 m 到 p：[a-dm-p]（并集） [a-z&amp;&amp;[def]] d、e 或 f（交集） [a-z&amp;&amp;[^bc]] a 到 z，除了 b 和 c：[ad-z]（减去） [a-z&amp;&amp;[^m-p]] a 到 z，而非 m 到 p：[a-lq-z]（减去） 预定义字符类 . 任何字符 \d 数字：[0-9] \D 非数字： [^0-9] \s 空白字符：[ \t\n\x0B\f\r] \S 非空白字符：[^\s] \w 单词字符：[a-zA-Z_0-9] \W 非单词字符：[^\w] 边界匹配器 ^ 行的开头 $ 行的结尾 数量词 X? 一次或一次也没有 X* 零次或多次 X+ 一次或多次 X{n} 恰好 n 次 X{n,} 至少 n 次 X{n,m} 至少 n 次，但是不超过 m 次 Logical 运算符 XY X 后跟 Y X|Y X 或 Y (X) X，作为捕获组 举例import java.util.regex.Matcher; import java.util.regex.Pattern; public class Test { public static void main(String[] args) { p(&quot;abc&quot;.matches(&quot;...&quot;)); p(&quot;a2389a&quot;.replaceAll(&quot;\\d&quot;, &quot;*&quot;)); Pattern p = Pattern.compile(&quot;[a-z]{3}&quot;); Matcher m = p.matcher(&quot;abc&quot;); p(m.matches()); //上面的三行代码可以用下面一行代码代替 p(&quot;abc&quot;.matches(&quot;[a-z]{3}&quot;)); } public static void p(Object o){ System.out.println(o); } } /**output * true * a****a * true * true **/]]></content>
      <categories>
        <category>java基础</category>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java 引用类型]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[强引用当我们使用new 这个关键字创建对象时被创建的对象就是强引用，如Object object = new Object() 这个Object()就是一个强引用了，如果一个对象具有强引用。垃圾回收器就不会去回收有强引用的对象。如当jvm内存不足时，具备强引用的对象，虚拟机宁可会报内存空间不足的异常来终止程序，也不会靠垃圾回收器去回收该对象来解决内存。 软引用如果一个对象具备软引用，如果内存空间足够，那么垃圾回收器就不会回收它，如果内存空间不足了，就会回收该对象。当然没有被回收之前，该对象依然可以被程序调用。 弱引用如果一个对象只具有弱引用，只要垃圾回收器在自己的内存空间中线程检测到了，就会立即被回收，对应内存也会被释放掉。相比软引用弱引用的生命周期要比软引用短很多。不过，如果垃圾回收器是一个优先级很低的线程，也不一定会很快就会释放掉软引用的内存。 虚引用如果一个对象只具有虚引用，那么它就和没有任何引用一样，随时会被jvm当作垃圾进行回收。 引用和队列的使用 强引用一般是不会和队列一起使用的，这个过滤掉。 软引用可以和一个引用队列来联合使用，一般软引用可以用来实现内存敏感的高速缓存，如果软引用的所引用的对象被垃圾回收，java虚拟机就会把引用入到与之关联的引用队列中去。 弱引用和队列使用在一起，如果弱引用被所引用的对象回收了，java虚拟机就会把这个弱引用加入到关联的队列中去； 虚引用，在java虚拟机回收虚引用时，会把这个虚引用放到与之关联的引用队列中去。程序可以通过判断引用队列中是否已经引用了虚引用，来了解引用对象是否要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么可以在所引用的对象内存前，采取一些逻辑处理。 jdk中的引用实现类代表软引用的类：java.lang.ref.SoftReference 代表弱引用的类：java.lang.ref.WeakReference 代表虚引用的类：java.lang.ref.PhantomReference 他们同时继承了：java.lang.ref.Reference 引用队列：java.lang.ref.ReferenceQueue,这个引用队列是可以三种引用类型联合使用的，以便跟踪java虚拟机回收所引用对象的活动。 引用实现强引用Object object = new Object() 软引用/** * 实现一个软引用 */ public static void testSoftReference(){ //创建一个强引用Test String str = new String(&quot;Test&quot;); //创建一个引用队列 ReferenceQueue&lt;string&gt; rq = new ReferenceQueue&lt;string&gt;(); //实现一个软引用，将强引用类型str和是实例化的rq放到软引用实现里面 SoftReference&lt;string&gt; srf = new SoftReference&lt;string&gt;(str,rq); //通过软引用get方法获取强引用中创建的内存空间Test值 System.out.println(srf.get()); //程序执行下gc现在jvm的内存空间还有很多所以gc不会回收str的对象 System.gc(); //所以这里执行get还是会打印Test的 System.out.println(srf.get()); } 弱引用/** * 实现一个弱引用 */ public static void testWeakReference(){ String str = new String(&quot;Test&quot;); ReferenceQueue&lt;string&gt; rq = new ReferenceQueue&lt;string&gt;(); //实现一个弱引用，将强引用类型str和是实例化的rq放到弱引用实现里面 WeakReference&lt;string&gt; wrf = new WeakReference&lt;string&gt;(str,rq); //给str给值为null。 然后再通过WeakReference弱引用的get()方法获得Test对象的引用 str = null; //程序多执行gc，提高gc执行线程频率来回收对象。 System.gc(); //假如它还没有被垃圾回收，那么接下来在第执行wrf.get()方法会返回Test对象的引用，并且使得这个对象被str1强引用。 //再接下来在行执行rq.poll()方法会返回null，因为此时引用队列中没有任何引用。 String str1 = wrf.get(); System.out.println(str1); //ReferenceQueue的poll()方法用于返回队列中的引用，如果没有则返回null。 Reference&lt;string&gt; ref= (Reference&lt;string&gt;) rq.poll(); System.out.println(&quot;弱引用已消失&quot;+ref); if(ref!=null){ System.out.println(ref.get()); } }]]></content>
      <categories>
        <category>java基础</category>
        <category>java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java nio 简介]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2FjavaNIO%2F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[简述I/O与CPU时间的比较影响应用程序执行效率的限定性因素，往往并非处理速率，而是 I/O 提高cpu速度和io速度对吞吐量的影响 CPU不再是束缚Java 应用程序并非真的受着 I/O 的束缚。操作系统与 Java 基于流的 I/O模型有些不匹配。操作系统要移动的是大块数据（缓冲区），JVM 的 I/O 类喜欢操作小块数据——单个字节、几行文本。结果，操作系统送来整缓冲区的数据，java.io 的流数据类再花大量时间把它们拆成小块，往往拷贝一个小块就要往返于几层对象。 #IO概念# ##缓冲区操作 ##所谓“输入／输出”讲的无非就是把数据移进或移出缓冲区。 进程执行 I/O 操作，归结起来，也就是向操作系统发出请求，让它要么把缓冲区里的数据排干（写），要么用数据把缓冲区填满（读）。进程使用这一机制处理所有数据进出操作. 注意图中用户空间和内核空间的概念。用户空间是常规进程所在区域。JVM 就是常规进程，驻守于用户空间。用户空间是非特权区域：比如，在该区域执行的代码就不能直接访问硬件设备。内核空间是操作系统所在区域。内核代码有特别的权力：它能与设备控制器通讯，控制着用户区域进程的运行状态，等等。最重要的是，所有 I/O 都直接或间接通过内核空间。 当进程请求 I/O 操作的时候，它执行一个系统调用（有时称为陷阱）将控制权移交给内核。当内核以这种方式被调用，它随即找到进程所需数据，并把数据传送到用户空间内的指定缓冲区。内核试图对数据进行高速缓存或预读取，因此进程所需数据可能已经在内核空间里了。如果是这样，该数据只需简单地拷贝出来即可。如果数据不在内核空间，则进程被挂起，内核着手把数据读进内存。 为什么不直接让磁盘控制器把数据送到用户空间的缓冲区呢? 硬件通常不能直接访问用户空间 其次，像磁盘这样基于块存储的硬件设备操作的是固定大小的数据块，而用户进程请求的可能是任意大小的或非对齐的数据块。在数据往来于用户空间与存储设备的过程中，内核负责数据的分解、再组合工作，因此充当着中间人的角色。 发散/汇聚操作系统能把组装／分解过程进行得更加高效。根据发散／汇聚的概念，进程只需一个系统调用，就能把一连串缓冲区地址传递给操作系统。然后，内核就可以顺序填充或排干多个缓冲区，读的时候就把数据发散到多个用户空间缓冲区，写的时候再从多个缓冲区把数据汇聚起来 虚拟内存虚拟内存意为使用虚假（或虚拟）地址取代物理（硬件RAM）内存地址, 这样做好处颇多，总结起来可分为两大类： 一个以上的虚拟地址可指向同一个物理内存地址。 虚拟内存空间可大于实际可用的硬件内存。 设备控制器不能通过 DMA 直接存储到用户空间，但通过利用上面提到的第一项，则可以达到相同效果。把内核空间地址与用户空间的虚拟地址映射到同一个物理地址，这样，DMA 硬件（只能访问物理内存地址）就可以填充对内核与用户空间进程同时可见的缓冲区 内存页面调度结构 地址翻译 结合高速缓存和虚存 文件IO所有 I/O 都是通过请求页面调度完成的。页面调度是非常底层的操作，仅发生于磁盘扇区与内存页之间的直接传输。而文件 I/O 则可以任意大小、任意定位。文件系统把一连串大小一致的数据块组织到一起。有些块存储元信息，如空闲块、目录、索引等的映射，有些包含文件数据。单个文件的元信息描述了哪些块包含文件数据、数据在哪里结束、最后一次更新是什么时候，等等。 采用分页技术的操作系统执行 I/O 的全过程可总结为以下几步： 确定请求的数据分布在文件系统的哪些页（磁盘扇区组）。 在内核空间分配足够数量的内存页，以容纳得到确定的文件系统页。 在内存页与磁盘上的文件系统页之间建立映射。 为每一个内存页产生页错误。 虚拟内存系统俘获页错误，安排页面调入，从磁盘上读取页内容，使页有效。 一旦页面调入操作完成，文件系统即对原始数据进行解析，取得所需文件内容或属性 信息。 内存映射文件 内存映射 I/O 使用文件系统建立从用户空间直到可用文件系统页的虚拟内存映射。这样做有几个好处： 用户进程把文件数据当作内存，所以无需发布 read( )或 write( )系统调用。 当用户进程碰触到映射内存空间，页错误会自动产生，从而将文件数据从磁盘读进内存。如果用户修改了映射内存空间，相关页会自动标记为脏，随后刷新到磁盘，文件得到更新。 操作系统的虚拟内存子系统会对页进行智能高速缓存，自动根据系统负载进行内存管 数据总是按页对齐的，无需执行缓冲区拷贝。 大型文件使用映射，无需耗费大量内存，即可进行数据拷贝。 文件锁定文件锁定机制允许一个进程阻止其他进程存取某文件，或限制其存取方式。通常的用途是控制共享信息的更新方式，或用于事务隔离。 流I/O并非所有 I/O 都像前几节讲的是面向块的，也有流 I/O，其原理模仿了通道。I/O 字节流必须顺序存取，常见的例子有 TTY（控制台）设备、打印机端口和网络连接。]]></content>
      <categories>
        <category>java基础</category>
        <category>javaNIO</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java nio 通道]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2FjavaNIO%2F%E9%80%9A%E9%81%93%2F</url>
    <content type="text"><![CDATA[通道Channel用于在字节缓冲区和位于通道另一侧的实体（通常是一个文件或套接字）之间有效地传输数据。 通道是一种途径，借助该途径，可以用最小的总开销来访问操作系统本身的I/O服务。缓冲区则是通道内部用来发送和接收数据的端点。 channel类层次结构 通道基础public interface Channel { public boolean isOpen( ); public void close( ) throws IOException; } 通道实现经常使用操作系统的本地代码。通道接口允许您以一种受控且可移植的方式来访问底层的I/O服务。 InterruptibleChannel是一个标记接口，当被通道使用时可以标示该通道是可以中断的（Interruptible）。如果连接可中断通道的线程被中断，那么该通道会以特别的方式工作，关于这一点我们会在3.1.3节中进行讨论。大多数但非全部的通道都是可以中断的。 打开通道通道可以以多种方式创建。Socket通道有可以直接创建新socket通道的工厂方法。但是一个FileChannel对象却只能通过在一个打开的RandomAccessFile、FileInputStream或FileOutputStream对象上调用getChannel( )方法来获取。您不能直接创建一个FileChannel对象。 SocketChannel sc = SocketChannel.open( ); sc.connect (new InetSocketAddress (&quot;somehost&quot;, someport)); ServerSocketChannel ssc = ServerSocketChannel.open( ); ssc.socket( ).bind (new InetSocketAddress (somelocalport)); DatagramChannel dc = DatagramChannel.open( ); RandomAccessFile raf = new RandomAccessFile (&quot;somefile&quot;, &quot;r&quot;); FileChannel fc = raf.getChannel( ); 使用通道通道将数据传输给ByteBuffer对象或者从ByteBuffer对象获取数据进行传输。 ByteChannel接口引申出了ReadableByteChannel 和WritableByteChannel两个接口。ByteChannel接口本身并不定义新的API方法，它是一种用来聚集它自己以一个新名称继承的多个接口的便捷接口。从FileInputStream对象的getChannel( )方法获取的FileChannel对象是只读的 ByteChannel的read( ) 和write( )方法使用ByteBuffer对象作为参数。两种方法均返回已传输的字节数，可能比缓冲区的字节数少甚至可能为零。缓冲区的位置也会发生与已传输字节相同数量的前移。如果只进行了部分传输，缓冲区可以被重新提交给通道并从上次中断的地方继续传输。该过程重复进行直到缓冲区的hasRemaining( )方法返回false值 private static void channelCopy1 (ReadableByteChannel src, WritableByteChannel dest) throws IOException { ByteBuffer buffer = ByteBuffer.allocateDirect (16 * 1024); while (src.read (buffer) != -1) { buffer.flip( ); // Write to the channel; may block dest.write (buffer); buffer.compact( ); }// EOF will leave buffer in fill state buffer.flip( ); // Make sure that the buffer is fully drained while (buffer.hasRemaining( )) { dest.write (buffer); } } 通道可以以阻塞（blocking）或非阻塞（nonblocking）模式运行。只有面向流的（stream-oriented）的通道，如sockets和pipes才能使用非阻塞模式。 关闭通道与缓冲区不同，通道不能被重复使用。一个打开的通道即代表与一个特定I/O服务的特定连接并封装该连接的状态。当通道关闭时，那个连接会丢失，然后通道将不再连接任何东西。 调用通道的close( )方法时，可能会导致在通道关闭底层I/O服务的过程中线程暂时阻塞7，哪怕该通道处于非阻塞模式。通道关闭时的阻塞行为（如果有的话）是高度取决于操作系统或者文件系统的。 通道引入了一些与关闭和中断有关的新行为。如果一个通道实现InterruptibleChannel接口，它的行为以下述语义为准：如果一个线程在一个通道上被阻塞并且同时被中断，那么该通道将被关闭，该被阻塞线程也会产生一个ClosedByInterruptException异常。 Scatter/Gather1.Scatter 从一个Channel读取的信息分散到N个缓冲区中(Buufer).2.Gather 将N个Buffer里面内容按照顺序发送到一个Channel. 例子: ByteBuffer header = ByteBuffer.allocateDirect (10); ByteBuffer body = ByteBuffer.allocateDirect (80); ByteBuffer [] buffers = { header, body }; int bytesRead = channel.read (buffers); 我们可以用一个gather操作将多个缓冲区的数据组合并发送出去。使用相同的缓冲区，我们可以像下面这样汇总数据并在一个socket通道上发送包 body.clear( ); body.put(&quot;FOO&quot;.getBytes()).flip( ); // &quot;FOO&quot; as bytes header.clear( ); header.putShort (TYPE_FILE).putLong (body.limit()).flip( ); long bytesWritten = channel.write (buffers); 文件通道文件通道总是阻塞式的，因此不能被置于非阻塞模式。面向流的I/O的非阻塞范例对于面向文件的操作并无多大意义，这是由文件I/O本质上的不同性质造成的。对于文件I/O，最强大之处在于异步I/O（asynchronous I/O），它允许一个进程可以从操作系统请求一个或多个I/O操作而不必等待这些操作的完成。发起请求的进程之后会收到它请求的I/O操作已完成的通知。 FileChannel对象不能直接创建。一个FileChannel实例只能通过在一个打开的file对象（RandomAccessFile、FileInputStream或FileOutputStream）上调用getChannel( )方法获取 APIpackage java.nio.channels; public abstract class FileChannel extends AbstractChannel implements ByteChannel, GatheringByteChannel, ScatteringByteChannel { // This is a partial API listing // All methods listed here can throw java.io.IOException public abstract int read(ByteBuffer dst, long position); public abstract int write(ByteBuffer src, long position); public abstract long size(); // 返回当前文件的 position 值。返回值是一个长整型（long），表示文件中的当前字节位置。 public abstract long position(); // 将通道的 position 设置为指定值。负值，将异常伺候；值可以超过文件尾，这会导致文件空洞。 public abstract void position(long newPosition); public abstract void truncate(long size); public abstract void force(boolean metaData); public final FileLock lock(); public abstract FileLock lock(long position, long size, boolean shared); public final FileLock tryLock(); public abstract FileLock tryLock(long position, long size, boolean shared); public abstract MappedByteBuffer map(MapMode mode, long position, long size); public abstract long transferTo(long position, long count, WritableByteChannel target); public abstract long transferFrom(ReadableByteChannel src, long position, long count); public static class MapMode { public static final MapMode READ_ONLY; public static final MapMode READ_WRITE; public static final MapMode PRIVATE; } } FileChannel都会尝试使用本地I/O服务。FileChannel类本身是抽象的，您从getChannel( )方法获取的实际对象是一个具体子类（subclass）的一个实例（instance），该子类可能使用本地代码来实现以上API方法中的一些或全部。 访问文件public abstract long position( ) public abstract void position (long newPosition) public abstract int read (ByteBuffer dst) public abstract int read (ByteBuffer dst, long position) public abstract int write (ByteBuffer src) public abstract int write (ByteBuffer src, long position) public abstract long size( ) public abstract void truncate (long size) public abstract void force (boolean metaData) 当磁盘上一个文件的分配空间小于它的文件大小时会出现“文件空洞” , 当需要减少一个文件的size时，truncate( )方法会砍掉您所指定的新size值之外的所有数据。 文件锁定锁（lock）可以是共享的（shared）或独占的（exclusive）。本节中描述的文件锁定特性在很大程度上依赖本地的操作系统实现。锁最终是由操作系统或文件系统来判优的并且几乎总是在进程级而非线程级上判优。锁都是与一个文件关联的，而不是与单个的文件句柄或通道关联。 锁是在文件内部区域上获得的。调用带参数的Lock( )方法会指定文件内部锁定区域的开始position以及锁定区域的size。第三个参数shared表示您想获取的锁是共享的（参数值为true）还是独占的（参数值为false）。要获得一个共享锁，您必须先以只读权限打开文件，而请求独占锁时则需要写权限。 lock( )方法是一种在整个文件上请求独占锁 tryLock( )的方法，它们是lock( )方法的非阻塞变体。这两个tryLock( )和lock( )方法起相同的作用，不过如果请求的锁不能立即获取到则会返回一个null。 public abstract class FileLock { public final FileChannel channel( ) public final long position( ) public final long size( ) public final boolean isShared( ) //是否共享 public final boolean overlaps (long position, long size) public abstract boolean isValid( ); //是否有效 public abstract void release( ) throws IOException; } FileLock类封装一个锁定的文件区域。FileLock对象由FileChannel创建并且总是关联到那个特定的通道实例 一个FileLock对象创建之后即有效，直到它的release( )方法被调用或它所关联的通道被关闭或Java虚拟机关闭时才会失效。 FileLock lock = fileChannel.lock( ) try { &lt;perform read/write/whatever on channel&gt; } catch (IOException) { &lt;handle unexpected exception&gt; } finally { lock.release( ) } 内存映射文件新的FileChannel类提供了一个名为map( )的方法，该方法可以在一个打开的文件和一个特殊类型的ByteBuffer之间建立一个虚拟内存映射 public abstract MappedByteBuffer map (MapMode mode, long position,long size) 映射文件的范围不应超过文件的实际大小。如果您请求一个超出文件大小的映射，文件会被增大以匹配映射的大小。 Channel-to-Channel传输public abstract long transferTo (long position, long count, WritableByteChannel target) public abstract long transferFrom (ReadableByteChannel src,long position, long count) transferTo( )和transferFrom( )方法允许将一个通道交叉连接到另一个通道，而不需要通过一个中间缓冲区来传递数据。只有FileChannel类有这两个方法，因此channel-to-channel传输中通道之一必须是FileChannel。 Socket通道看选择器 全部socket通道类（DatagramChannel、SocketChannel和ServerSocketChannel）都是由位于java.nio.channels.spi包中的AbstractSelectableChannel引申而来。这意味着我们可以用一个Selector对象来执行socket通道的有条件的选择（readiness selection）。 DatagramChannel和SocketChannel实现定义读和写功能的接口而ServerSocketChannel不实现。ServerSocketChannel负责监听传入的连接和创建新的SocketChannel对象，它本身从不传输数据。 非阻塞模式要把一个socket通道置于非阻塞模式，我们要依靠所有socket通道类的公有超级类：SelectableChannel。 管道管道就是一个用来在两个实体之间单向传输数据的导管 public abstract class Pipe { public static Pipe open( ) throws IOException public abstract SourceChannel source( ); public abstract SinkChannel sink( ); public static abstract class SourceChannel extends AbstractSelectableChannel implements ReadableByteChannel, ScatteringByteChannel public static abstract class SinkChannel extends AbstractSelectableChannel implements WritableByteChannel, GatheringByteChannel } Pipe实例是通过调用不带参数的Pipe.open( )工厂方法来创建的。Pipe类定义了两个嵌套的通道类来实现管路。这两个类是Pipe.SourceChannel（管道负责读的一端）和Pipe.SinkChannel（管道负责写的一端）。这两个通道实例是在Pipe对象创建的同时被创建的，可以通过在Pipe对象上分别调用source( )和sink( )方法来取回。]]></content>
      <categories>
        <category>java基础</category>
        <category>javaNIO</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java io]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2Fjava%E5%9F%BA%E7%A1%80%2FIO%2F</url>
    <content type="text"><![CDATA[IO流所有的输入类都继承自InputStream,输入类都继承自OutputStream InputStreamInputStream的作用是标志那些从不同数据起源产生输入的类。这些数据起源包括（每个都有一个相关的InputStream子类）： 字节数组 String对象 文件 “管道”，它的工作原理与现实生活中的管道类似：将一些东西一端置入，它们在另一端输出。 一个由其他种类的流组成的序列，以便我们将其统一收集合并到一个流内。 其他数据集，如Internet连接等。 outputStream的类型这一类别包括的类决定了我们的输入往何处去：一个字节数组（但没有String；假定我们可用字节数组创建一个）；一个文件；或者一个“管道”。 过滤器FilterInputStream 和FilterOutputStream （这两个名字不十分直观）提供了相应的装饰器接口，用于控制一个特定的输入流（InputStream）或者输出流（OutputStream）。 FilterInputStream类型 FilterOutputStream 的类型 File类代表文件或者目录 目录列表器列出File 对象，调用list()，会获得File 对象包含的一个完整列表。然而，若想对这个列表进行某些限制，就需要使用一个“目录过滤器” public interface FilenameFilter { boolean accept(文件目录, 字串名); } 把accept()方法提供给list()方法，使list()能够“回调”accept() ，从而判断应将哪些文件名包括到列表中。 举例：匿名内部类 //: DirList3.java // Building the anonymous inner class &quot;in-place&quot; import java.io.*; public class DirList3 { public static void main(final String[] args) { try { File path = new File(&quot;.&quot;); String[] list; if(args.length == 0) list = path.list(); else list = path.list( new FilenameFilter() { public boolean accept(File dir, String n) { String f = new File(n).getName(); return f.indexOf(args[0]) != -1; } }); for(int i = 0; i &lt; list.length; i++) System.out.println(list[i]); } catch(Exception e) { e.printStackTrace(); } } } ///:~ 顺序目录列表将list提为成员变量，再进行排序list 检查与创建目录File 类并不仅仅是对现有目录路径、文件或者文件组的一个表示。亦可用一个File 对象新建一个目录，甚至创建一个完整的目录路径——假如它尚不存在的话。亦可用它了解文件的属性（长度、上一次修改日期、读/写属性等），检查一个File 对象到底代表一个文件还是一个目录，以及删除一个文件等等。 具体查看jdk文档 输入流1. 从文件输入为打开一个文件以便输入，需要使用一个FileInputStream，同时将一个String 或File 对象作为文件名使用。为提高速度，最好先对文件进行缓冲处理，从而获得用于一个BufferedInputStream 的构建器的结果句柄。为了以格式化的形式读取输入数据，我们将那个结果句柄赋给用于一个DataInputStream 的构建器。DataInputStream 是我们的最终（final）对象，并是我们进行读取操作的接口。 2. 从内存输入这一部分采用已经包含了完整文件内容的String s2，并用它创建一个StringBufferInputStream（字串缓冲输入流）——作为构建器的参数，要求使用一个String，而非一个StringBuffer）。随后，我们用read()依次读取每个字符，并将其发送至控制台 3. 格式化内存输入StringBufferInputStream 的接口是有限的，所以通常需要将其封装到一个DataInputStream 内，从而增强它的能力。。然而，若选择用readByte()每次读出一个字符，那么所有值都是有效的，所以不可再用返回值来侦测何时结束输入。相反，可用available()方法判断有多少字符可用。 import java.io.*; public class TestEOF { public static void main(String[] args) { try { DataInputStream in = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;TestEof.java&quot;))); while(in.available() != 0) System.out.print((char)in.readByte()); } catch (IOException e) { System.err.println(&quot;IOException&quot;); } } } ///:~ 4. 行的编号与文件输出标志DataInputStream 何时结束的一个方法是readLine()。一旦没有更多的字串可以读取，它就会返回null。每个行都会伴随自己的行号打印到文件里。 输出流两类主要的输出流是按它们写入数据的方式划分的：一种按人的习惯写入，另一种为了以后由一个DataInputStream 而写入。RandomAccessFile 是独立的，尽管它的数据格式兼容于DataInputStream 和DataOutputStream。 保存与恢复数据PrintStream 能格式化数据，使其能按我们的习惯阅读。但为了输出数据，以便由另一个数据流恢复，则需用一个DataOutputStream 写入数据，并用一个DataInputStream 恢复（获取）数据。 为了保证任何读方法能够正常工作，必须知道数据项在流中的准确位置，因为既有可能将保存的double 数据作为一个简单的字节序列读入，也有可能作为char 或其他格式读入。所以必须要么为文件中的数据采用固定的格式，要么将额外的信息保存到文件中，以便正确判断数据的存放位置。 从标准输入中读取数据import java.io.*; public class Echo { public static void main(String[] args) { DataInputStream in = new DataInputStream( new BufferedInputStream(System.in)); String s; try { while((s = in.readLine()).length() != 0) System.out.println(s); // An empty line terminates the program } catch(IOException e) { e.printStackTrace(); } } } ///:~ 重导向标准I OJava 1.1 在System 类中添加了特殊的方法，允许我们重新定向标准输入、输出以及错误IO 流。此时要用到下述简单的静态方法调用： setIn(InputStream) setOut(PrintStream) setErr(PrintStream) 例子 import java.io.*; class Redirecting { public static void main(String[] args) { try { BufferedInputStream in = new BufferedInputStream(new FileInputStream(&quot;Redirecting.java&quot;)); // Produces deprecation message: PrintStream out =new PrintStream(new BufferedOutputStream(new FileOutputStream(&quot;test.out&quot;))); System.setIn(in); System.setOut(out); System.setErr(out); BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); String s; while((s = br.readLine()) != null) System.out.println(s); out.close(); // Remember this! } catch(IOException e) { e.printStackTrace(); } } } 压缩序列化Serialization（序列化）是一种将对象以一连串的字节描述的过程；反序列化deserialization是一种将这些字节重建成一个对象的过程。 什么情况下需要序列化 当你想把的内存中的对象保存到一个文件中或者数据库中时候； 当你想用套接字在网络上传送对象的时候； 当你想通过RMI传输对象的时候； 如果我们想要序列化一个对象，首先要创建某些OutputStream(如FileOutputStream、ByteArrayOutputStream等)，然后将这些OutputStream封装在一个ObjectOutputStream中。这时候，只需要调用writeObject()方法就可以将对象序列化，并将其发送给OutputStream（记住：对象的序列化是基于字节的，不能使用Reader和Writer等基于字符的层次结构）。而反序列的过程（即将一个序列还原成为一个对象），需要将一个InputStream(如FileInputstream、ByteArrayInputStream等)封装在ObjectInputStream内，然后调用readObject()即可。 public class MyTest implements Serializable { private static final long serialVersionUID = 1L; private String name=&quot;SheepMu&quot;; private int age=24; public static void main(String[] args) {//以下代码实现序列化 try { ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;my.out&quot;));//输出流保存的文件名为 my.out ；ObjectOutputStream能把Object输出成Byte流 MyTest myTest=new MyTest(); oos.writeObject(myTest); oos.flush(); //缓冲流 oos.close(); //关闭流 } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } fan();//调用下面的 反序列化 代码 } public static void fan()//反序列的过程 { ObjectInputStream oin = null;//局部变量必须要初始化 try { oin = new ObjectInputStream(new FileInputStream(&quot;my.out&quot;)); } catch (FileNotFoundException e1) { e1.printStackTrace(); } catch (IOException e1) { e1.printStackTrace(); } MyTest mts = null; try { mts = (MyTest ) oin.readObject();//由Object对象向下转型为MyTest对象 } catch (ClassNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } System.out.println(&quot;name=&quot;+mts.name); System.out.println(&quot;age=&quot;+mts.age); } } 深复制 public Object deepCopy() throws IOException, ClassNotFoundException{ //字节数组输出流，暂存到内存中 ByteArrayOutputStream bos = new ByteArrayOutputStream(); //序列化 ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); //反序列化 return ois.readObject(); }]]></content>
      <categories>
        <category>java基础</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java nio 缓冲区]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2FjavaNIO%2F%E7%BC%93%E5%86%B2%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[缓冲区一个Buffer对象是固定数量的数据的容器。其作用是一个存储器，或者分段运输区，在这里数据可被存储并在之后用于检索。 缓冲区的工作与通道紧密联系。通道是I/O传输发生时通过的入口，而缓冲区是这些数据传输的来源或目标。 类层次图 基础缓冲区是包在一个对象内的基本数据元素数组。Buffer类相比一个简单数组的优点是它将关于数据的数据内容和信息包含在一个单一的对象中。 属性 容量（Capacity） 缓冲区能够容纳的数据元素的最大数量。 上界（Limit） 缓冲区的第一个不能被读或写的元素。 位置（Position） 下一个要被读或写的元素的索引。 标记（Mark） 一个备忘位置。调用mark( )来设定mark = postion 0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity 。容量是固定的，但另外的三个属性可以在使用缓冲区时改变。 缓冲区APIpackage java.nio; public abstract class Buffer { public final int capacity( ) public final int position( ) public final Buffer position (int newPosition) public final int limit( ) public final Buffer limit (int newLimit) public final Buffer mark( ) public final Buffer reset( ) public final Buffer clear( ) public final Buffer flip( ) public final Buffer rewind( ) public final int remaining( ) public final boolean hasRemaining( ) public abstract boolean isReadOnly( ); } 像clear()这类函数，您通常应当返回void，而不是Buffer引用。这些函数将引用返回到它们在(this)上被引用的对象。这是一个允许级联调用的类设计方法。 buffer.mark().position(5).reset( ); 存取在调用put()时指出了下一个数据元素应该被插入的位置，或者当get()被调用时指出下一个元素应从何处检索 public abstract class ByteBuffer extends Buffer implements Comparable { // This is a partial API listing public abstract byte get( ); public abstract byte get (int index); public abstract ByteBuffer put (byte b); public abstract ByteBuffer put (int index, byte b); } 填充buffer.put((byte)’H’).put((byte)’e’).put((byte)’l’).put((byte)’l’).put((byte)’o’); buffer.put(0,(byte)’M’).put((byte)’w’); 翻转Buffer.flip(); Flip()函数将一个能够继续添加数据元素的填充状态的缓冲区翻转成一个准备读出元素的释放状态。 Rewind()函数与flip()相似，但不影响上界属性。它只是将位置值设回0。您可以使用rewind()后退，重读已经被翻转的缓冲区中的数据。 两次flip()后缓冲区的大小为0 释放如果我们现在将上图的缓冲区传入通道，它将取出我们存放在那里的数据，从位置开始直到上界结束。 布尔函数hasRemaining()会在释放缓冲区时告诉您是否已经达到缓冲区的上界。以下是一种将数据元素从缓冲区释放到一个数组的方法 for (int i = 0; buffer.hasRemaining( ), i++) { myByteArray [i] = buffer.get( ); } remaining()函数将告知您从当前位置到上界还剩余的元素数目 int count = buffer.remaining( ); for (int i = 0; i &lt; count, i++) { myByteArray [i] = buffer.get( ); } 例子: package com.ronsoft.books.nio.buffers; import java.nio.CharBuffer; /** * Buffer fill/drain example. This code uses the simplest * means of filling and draining a buffer: one element at * a time. * @author Ron Hitchens (ron@ronsoft.com) */ public class BufferFillDrain { public static void main (String [] argv) throws Exception { CharBuffer buffer = CharBuffer.allocate (100); while (fillBuffer (buffer)) { buffer.flip( ); drainBuffer (buffer); buffer.clear( ); } } private static void drainBuffer (CharBuffer buffer) { while (buffer.hasRemaining( )) { System.out.print (buffer.get( )); } System.out.println (&quot;&quot;); } private static boolean fillBuffer (CharBuffer buffer) { if (index &gt;= strings.length) { return (false); } String string = strings [index++]; for (int i = 0; i &lt; string.length( ); i++) { buffer.put (string.charAt (i)); } return (true); } private static int index = 0; private static String [] strings = { &quot;A random string value&quot;, &quot;The product ofan infinite number of monkeys&quot;, &quot;Hey hey we&apos;re the Monkees&quot;, &quot;Opening actfor the Monkees: Jimi Hendrix&quot;, &quot;&apos;Scuse me while I kiss this fly&quot;, // Sorry Jimi ;-) &quot;Help Me! Help Me!&quot; }; } 压缩buffer.compact() 调用compact()的作用是丢弃已经释放的数据，保留未释放的数据，并使缓冲区对重新填充容量准备就绪。 标记缓冲区的标记在mark( )函数被调用之前是未定义的，调用时标记被设为当前位置的值。reset( )函数将位置设为当前的标记值。一些缓冲区函数会抛弃已经设定的标记（rewind( )，clear( )，以及flip( )总是抛弃标记） 比较equals( ) //如果每个缓冲区中剩余的内容相同，那么equals( )函数将返回true compareTo( ) 批量移动public abstract class CharBuffer extends Buffer implements CharSequence, Comparable { // This is a partial API listing public CharBuffer get (char [] dst) public CharBuffer get (char [] dst, int offset, int length) public final CharBuffer put (char[] src) public CharBuffer put (char [] src, int offset, int length) public CharBuffer put (CharBuffer src) public final CharBuffer put (String src) public CharBuffer put (String src, int start, int end) } 创建缓冲区public abstract class CharBuffer extends Buffer implements CharSequence, Comparable { // This is a partial API listing public static CharBuffer allocate (int capacity) public static CharBuffer wrap (char [] array) public static CharBuffer wrap (char [] array, int offset, int length) public final boolean hasArray( ) public final char [] array( ) public final int arrayOffset( ) } 新的缓冲区是由分配或包装操作创建的。分配操作创建一个缓冲区对象并分配一个私有的空间来储存容量大小的数据元素。包装操作创建一个缓冲区对象但是不分配任何空间来储存数据元素。它使用您所提供的数组作为存储空间来储存缓冲区中的数据元素。 CharBuffer charBuffer = CharBuffer.allocate (100); char [] myArray = new char [100]; CharBuffer charbuffer = CharBuffer.wrap (myArray); 复制缓冲区public abstract class CharBuffer extends Buffer implements CharSequence, Comparable { // This is a partial API listing public abstract CharBuffer duplicate( ); public abstract CharBuffer asReadOnlyBuffer( ); public abstract CharBuffer slice( ); } Duplicate()函数创建了一个与原始缓冲区相似的新缓冲区。两个缓冲区共享数据元素，拥有同样的容量，但每个缓冲区拥有各自的位置，上界和标记属性。对一个缓冲区内的数据元素所做的改变会反映在另外一个缓冲区上。 asReadOnlyBuffer()函数来生成一个只读的缓冲区视图 分割缓冲区与复制相似，但slice()创建一个从原始缓冲区的当前位置开始的新缓冲区，并且其容量是原始缓冲区的剩余元素数量（limit-position）。这个新缓冲区与原始缓冲区共享一段数据元素子序列。分割出来的缓冲区也会继承只读和直接属性 字节缓冲区APIpublic abstract class ByteBuffer extends Buffer implements Comparable { public static ByteBuffer allocate (int capacity) public static ByteBuffer allocateDirect (int capacity) public abstract boolean isDirect( ); //是否为直接缓冲区。 public static ByteBuffer wrap (byte[] array, int offset, int length) public static ByteBuffer wrap (byte[] array) public abstract ByteBuffer duplicate( ); public abstract ByteBuffer asReadOnlyBuffer( ); public abstract ByteBuffer slice( ); public final boolean hasArray( ) public final byte [] array( ) public final int arrayOffset( ) public abstract byte get( ); public abstract byte get (int index); public ByteBuffer get (byte[] dst, int offset, int length) public ByteBuffer get (byte[] dst, int offset, int length) public abstract ByteBuffer put (byte b); public abstract ByteBuffer put (int index, byte b); public ByteBuffer put (ByteBuffer src) public ByteBuffer put (byte[] src, int offset, int length) public final ByteBuffer put (byte[] src) public final ByteOrder order( ) public final ByteBuffer order (ByteOrder bo) public abstract CharBuffer asCharBuffer( ); public abstract ShortBuffer asShortBuffer( ); public abstract IntBuffer asIntBuffer( ); public abstract LongBuffer asLongBuffer( ); public abstract FloatBuffer asFloatBuffer( ); public abstract DoubleBuffer asDoubleBuffer( ); public abstract char getChar( ); public abstract char getChar (int index); public abstract ByteBuffer putChar (char value); public abstract ByteBuffer putChar (int index, char value); public abstract short getShort( ); public abstract short getShort (int index); public abstract ByteBuffer putShort (short value); public abstract ByteBuffer putShort (int index, short value); public abstract int getInt( ); public abstract int getInt (int index); public abstract ByteBuffer putInt (int value); public abstract ByteBuffer putInt (int index, int value); public abstract long getLong( ); public abstract long getLong (int index); public abstract ByteBuffer putLong (long value); public abstract ByteBuffer putLong (int index, long value); public abstract float getFloat( ); public abstract float getFloat (int index); public abstract ByteBuffer putFloat (float value); public abstract ByteBuffer putFloat (int index, float value); public abstract double getDouble( ); public abstract double getDouble (int index); public abstract ByteBuffer putDouble (double value); public abstract ByteBuffer putDouble (int index, double value); public abstract ByteBuffer compact( ); public boolean equals (Object ob) public int compareTo (Object ob) public String toString( ) public int hashCode( ) } 直接缓冲区字节缓冲区跟其他缓冲区类型最明显的不同在于，它们可以成为通道所执行的I/O的源头和/或目标。 直接缓冲区时I/O的最佳选择，但可能比创建非直接缓冲区要花费更高的成本。直接缓冲区使用的内存是通过调用本地操作系统方面的代码分配的，绕过了标准JVM堆栈。建立和销毁直接缓冲区会明显比具有堆栈的缓冲区更加破费，这取决于主操作系统以及JVM实现。直接缓冲区的内存区域不受无用存储单元收集支配，因为它们位于标准JVM堆栈之外。 视图缓冲区as...Buffer(); 视图缓冲区通过已存在的缓冲区对象实例的工厂方法来创建。这种视图对象维护它自己的属性，容量，位置，上界和标记，但是和原来的缓冲区共享数据元素。 下面的代码创建了一个ByteBuffer缓冲区的CharBuffer视图 CharBuffer charBuffer = byteBuffer.asCharBuffer( ); 一旦您得到了视图缓冲区，您可以用duplicate()，slice()和asReadOnlyBuffer()函数创建进一步的子视图 数据元素视图ByteBuffer类为每一种原始数据类型提供了存取的和转化的方法： public abstract char getChar( ); //index=position public abstract char getChar (int index); ... public abstract ByteBuffer putInt (int value); //index=position public abstract ByteBuffer putInt (int index, int value); ... 如果是getInt()则一次读取4个字节, 其他类似 内存映射缓冲区映射缓冲区是带有存储在文件，通过内存映射来存取数据元素的字节缓冲区。映射缓冲区通常是直接存取内存的，只能通过FileChannel类创建]]></content>
      <categories>
        <category>java基础</category>
        <category>javaNIO</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java nio字符编码]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2FjavaNIO%2F%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[*# 字符集 # 基础Character set（字符集） 字母“A”是一个字符。“%”也是一个字符 Coded character set（编码字符集） 把代码赋值给字符，这样它们就可以用特定的字符编码集表达数字的结果。例如USASCII，ISO 8859-1,Unicode等 Character-encoding scheme（字符编码方案） 编码方案定义了如何把字符编码的序列表达为字节序列。如utf-8 字符集 字符集类charset APIpublic abstract class Charset implements Comparable { //确定在JVM运行中当前指定的字符集是否可用。 public static boolean isSupported (String charsetName) //通过调用静态工厂方法forName()获得具体实例 public static Charset forName (String charsetName) //返回在JVM中当前有效的所有字符集的java.util.SortedMap public static SortedMap availableCharsets( ) //返回字符集的规范名称 public final String name( ) //给出包含别名的Set public final Set aliases( ) //返回规范字符集名称 public String displayName( ) public String displayName (Locale locale) //是否在lana注册 public final boolean isRegistered( ) //是否允许编码。几乎所有的字符集都支持编码 public boolean canEncode( ) //返回CharsetEncoder对象，可以使用和字符集相关的编码方案把字符序列转化为字节序列 public abstract CharsetEncoder newEncoder(); 默认值针对和字符集相关的编码器实现编码 public final ByteBuffer encode (CharBuffer cb) public final ByteBuffer encode (String str) public abstract CharsetDecoder newDecoder(); public final CharBuffer decode (ByteBuffer bb) //是否包含 public abstract boolean contains (Charset cs); //比较规范名称 public final boolean equals (Object ob) public final int compareTo (Object ob) public final int hashCode( ) public final String toString( ) } Charset类封装特定字符集的永恒信息。Charset是抽取。通过调用静态工厂方法forName()获得具体实例，导入所需字符集的名称。所有的Charset方法都是线程安全的；单一实例可以在多个线程中共享。 CharsetEncoder APIpublic abstract class CharsetEncoder { public final Charset charset( ) public final float averageBytesPerChar( ) public final float maxBytesPerChar( ) public final CharsetEncoder reset( ) public final ByteBuffer encode (CharBuffer in) throws CharacterCodingException public final CoderResult encode (CharBuffer in, ByteBuffer out, boolean endOfInput) public final CoderResult flush (ByteBuffer out) public boolean canEncode (char c) public boolean canEncode (CharSequence cs) /** 处理编码错误 */ public CodingErrorAction malformedInputAction( ) public final CharsetEncoder onMalformedInput (CodingErrorAction newAction) public CodingErrorAction unmappableCharacterAction( ) public final CharsetEncoder onUnmappableCharacter ( CodingErrorAction newAction) public final byte [] replacement( ) public boolean isLegalReplacement (byte[] repl) public final CharsetEncoder replaceWith (byte[] newReplacement) } CharsetEncoder对象是一个状态转换引擎：字符进去，字节出来。每个编码器和一个Charset对象相关联，而charset()方法返回一个备份参考。 CharsetEncoder类是一个状态编码引擎。实际上，编码器有状态意味着它们不是线程安全的：CharsetEncoder对象不应该在线程中共享。编码可以在一个简单的步骤中完成，如上面提到的encode()的首个形式，或者重复调用encode()的第二个形式。编码过程如下： 通过调用reset()方法复位编码器的状态。 不调用或多次调用encode()为编码器提供字符，endOfnput参数false表示后面可能有更多的字符。给定的CharBuffer将消耗字符，而编码字节序列将被添加到提供的ByteBuffer上。 最后一次调用encode()，针对endOfInput参数导入true。 调用flush()方法来完成未完成的编码并输出所有剩下的字节。如果在输出ByteBuffer中没有足够的空间，需要多次调用该方法。 当消耗了所有的输入时，当输出ByteBuffer为满时，或者当探测到编码错误时，encode()方法返回。无论如何，将会返回CoderResult对象，来表示发生的情况。结果对象可表示下列结果条件之一： Underflow（下溢）正常情况，表示需要更多的输入。或者是输入CharBuffer内容不足；或者在没有额外的输入的情况下，余下的字符无法进行处理。更新CharBuffer的位置解决被编码器消耗的字符的问题。 Overflow（上溢）表示编码器充满了输出ByteBuffer并且需要产生更多的编码输出。您应该消耗ByteBuffer但是不应该扰乱CharBuffer，CharBuffer将更新它的位置，之后再次调用encode()。重复进行直到得到下溢结果。 Malformed input（有缺陷的输入）编码时，不是有效的Unicode字符。对于解码来说，这意味着解码器遭遇了不识别的字节序列。 Unmappable character（无映射字符）表示编码器不能映射字符或字符的序列到字节上 错误处理CoderResult对象可以从encode()中返回，表示编码字符序列的问题。有两个已定义的代码错误条件：malformed和unmappable。在每一个错误条件上都可以配置编码器实例来采取不同的操作。当这些条件之一发生时，CodingErrorAction类封装可能采取的操作。 CodingErrorAction定义了三个公共域： REPORT(报告)创建CharsetEncoder时的默认行为。这个行为表示编码错误应该通过返回CoderResult对象报告 IGNORE（忽略）表示应忽略编码错误并且如果位置不对的话任何错误的输入都应中止。 REPLACE（替换）当前的替换字节序列处理编码错误。 malformedInputAction()方法返回针对有缺陷的输入生效的行为。调用onMalformedInput()设置在那之后要使用的CodingErrorAction值。无法映射字符的一对类似方法设置错误行为，并返回CharsetEncoder对象句柄。通过返回CharsetEncoder，这些方法允许调用链接。 CharsetEncoder encoder = charset.newEncoder( ) .onMalformedInput (CodingErrorAction.REPLACE) .onUnmappableCharacter (CodingErrorAction.IGNORE); 当前的替换字节序列可以通过调用replacement()方法找回。如果您未设置自己的替换序列，将返回默认值。 isLegalReplacement()方法，用您想要使用的字节阵列检测替换序列的合法性。 replaceWith()设置新的替换序列并导入字节阵列。 CoderResult类public class CoderResult { public static final CoderResult OVERFLOW public static final CoderResult UNDERFLOW public boolean isUnderflow( ) public boolean isOverflow( ) public boolean isError( ) public boolean isMalformed( ) public boolean isUnmappable( ) public int length( ) public static CoderResult malformedForLength (int length) public static CoderResult unmappableForLength (int length) public void throwException( ) throws CharacterCodingException }]]></content>
      <categories>
        <category>java基础</category>
        <category>javaNIO</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java nio 选择器]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E5%9F%BA%E7%A1%80%2FjavaNIO%2F%E9%80%89%E6%8B%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[选择器选择器提供选择执行已经就绪的任务的能力，这使得多元 I/O 成为可能。 就绪选择和多元执行使得单线程能够有效率地同时管理多个 I/O 通道(channels)。 选择器基础将之前创建的一个或多个可选择的通道注册到选择器对象中。一个表示通道和选择器的键将会被返回。当您调用一个选择器对象的select( )方法时，相关的键建会被更新，用来检查所有被注册到该选择器的通道。您可以获取一个键的集合，从而找到当时已经就绪的通道。通过遍历这些键，您可以选择出每个从上次您调用select( )开始直到现在，已经就绪的通道。 就绪选择的真正价值在于潜在的大量的通道可以同时进行就绪状态的检查。调用者可以轻松地决定多个通道中的哪一个准备好要运行。有两种方式可以选择：被激发的线程可以处于休眠状态，直到一个或者多个注册到选择器的通道就绪，或者它也可以周期性地轮询选择器，看看从上次检查之后，是否有通道处于就绪状态。 选择器，可选择通道和选择键类选择器类管理着一个被注册的通道集合的信息和它们的就绪状态。通道是和选择器一起被注册的，并且使用选择器来更新通道的就绪状态。当这么做的时候，可以选择将被激发的线程挂起，直到有就绪的的通道。 SelectableChannel可以被注册到Selector对象上，同时可以指定对那个选择器而言，那种操作是感兴趣的。一个通道可以被注册到多个选择器上，但对每个选择器而言只能被注册一次。 选择键封装了特定的通道与特定的选择器的注册关系。 public abstract class SelectableChannel extends AbstractChannel implements Channel { // This is a partial API listing public abstract SelectionKey register (Selector sel, int ops) throws ClosedChannelException; public abstract SelectionKey register (Selector sel, int ops, Object att) throws ClosedChannelException; public abstract boolean isRegistered( ); public abstract SelectionKey keyFor(Selector sel); public abstract int validOps(); public abstract void configureBlocking(boolean block) throws IOException; public abstract boolean isBlocking(); public abstract Object blockingLock(); } 选择器维护了一个需要监控的通道的集合。一个给定的通道可以被注册到多于一个的选择器上，而且不需要知道它被注册了那个Selector对象上。 public abstract class SelectionKey { public static final int OP_READ public static final int OP_WRITE public static final int OP_CONNECT public static final int OP_ACCEPT public abstract SelectableChannel channel( ); public abstract Selector selector( ); public abstract void cancel( ); public abstract boolean isValid( ); public abstract int interestOps( ); public abstract void interestOps (int ops); public abstract int readyOps( ); public final boolean isReadable( ) public final boolean isWritable( ) public final boolean isConnectable( ) public final boolean isAcceptable( ) public final Object attach (Object ob) public final Object attachment( ) } 建立选择器Selector selector = Selector.open( ); channel1.register (selector, SelectionKey.OP_READ); channel2.register (selector, SelectionKey.OP_WRITE); channel3.register (selector, SelectionKey.OP_READ | SelectionKey.OP_WRITE); // Wait up to 10 seconds for a channel to become ready readyCount = selector.select (10000); select( )方法在将线程置于睡眠状态，直到这些刚兴趣的事情中的操作中的一个发生或者10秒钟的时间过去。 选择器api public abstract class Selector { // This is a partial API listing public static Selector open( ) throws IOException public abstract boolean isOpen( ); public abstract void close( ) throws IOException; public abstract SelectionProvider provider( ); } Selector对象是通过调用静态工厂方法open( )来实例化的。 使用选择键public abstract class SelectionKey { public static final int OP_READ public static final int OP_WRITE public static final int OP_CONNECT public static final int OP_ACCEPT public abstract SelectableChannel channel( ); public abstract Selector selector( ); public abstract void cancel( ); public abstract boolean isValid( ); public abstract int interestOps( ); public abstract void interestOps (int ops); public abstract int readyOps( ); public final boolean isReadable( ) public final boolean isWritable( ) public final boolean isConnectable( ) public final boolean isAcceptable( ) public final Object attach (Object ob) public final Object attachment( ) } 键对象表示了一种特定的注册关系。当应该终结这种关系的时候，可以调用SelectionKey对象的cancel( )方法。可以通过调用isValid( )方法来检查它是否仍然表示一种有效的关系。当键被取消时，它将被放在相关的选择器的已取消的键的集合里。注册不会立即被取消，但键会立即失效（参见4.3节）。当再次调用select( )方法时（或者一个正在进行的select()调用结束时），已取消的键的集合中的被取消的键将被清理掉，并且相应的注销也将完成。通道会被注销，而新的SelectionKey将被返回。 一个SelectionKey对象包含两个以整数形式进行编码的比特掩码：一个用于指示那些通道/选择器组合体所关心的操作(instrest集合)，另一个表示通道准备好要执行的操作（ready集合）。 可以通过调用键的readyOps( )方法来获取相关的通道的已经就绪的操作。ready集合是interest集合的子集，并且表示了interest集合中从上次调用select( )以来已经就绪的那些操作。 public final Object attach (Object ob) public final Object attachment( ) 这两个方法允许您在键上放置一个“附件”，并在后面获取它 SelectionKey对象是线程安全的，但修改interest集合的操作是通过Selector对象进行同步的是很重要的。这可能会导致interestOps( )方法的调用会阻塞不确定长的一段时间。 使用选择器选择过程每一个Selector对象维护三个键的集合： public abstract class Selector { // This is a partial API listing public abstract Set keys( ); public abstract Set selectedKeys( ); public abstract int select( ) throws IOException; public abstract int select (long timeout) throws IOException; public abstract int selectNow( ) throws IOException; public abstract void wakeup( ); } 已注册的键的集合(Registered key set)这个集合通过keys( )方法返回 已选择的键的集合(Selected key set)这个集合的每个成员都是相关的通道被选择器（在前一个选择操作中）判断为已经准备好的，并且包含于键的interest集合中的操作。这个集合通过selectedKeys( )方法返回（并有可能是空的）。 已取消的键的集合(Cancelled key set)这个集合包含了cancel( )方法被调用过的键（这个键已经被无效化），但它们还没有被注销 基本上来说，选择器是对select( )、poll( )等本地调用(native call)或者类似的操作系统特定的系统调用的一个包装。它对每个选择操作应用了特定的过程。 选择操作 已取消的键的集合将会被检查, 并清空 已注册的键的集合中的键的interest集合将被检查。 对于那些操作系统指示至少已经准备好interest集合中的一种操作的通道，将执行以下两种操作中的一种： 如果通道的键还没有处于已选择的键的集合中，那么键的ready集合将被清空 否则，键的ready集合将被表示操作系统发现的当前已经准备好的操作的比特掩码更新。 2可能会花费很长时间,当步骤2结束时，步骤1将重新执行 select操作返回的值是ready集合在步骤2中被修改的键的数量，而不是已选择的键的集合中的通道的总数。而是从上一个select( )调用之后进入就绪状态的通道的数量。 3种形式 select( ) 在没有通道就绪时将无限阻塞 select (long timeout) 如果在超时时间内没有通道就绪时，它将返回0。如果一个或者多个通道在时间限制终止前就绪，立即返回。 selectNow() 不阻塞。如果当前没有通道就绪，它将立即返回0。 停止选择过程wakeup( )可以使使线程从被阻塞的select( )方法中退出 唤醒在select( )方法 调用wakeup( ) 将使得选择器上的第一个还没有返回的选择操作立即返回。如果当前没有在进行中的选择，那么下一次对select( )方法的一种形式的调用将立即返回。后续的选择操作将正常进行 调用close( ) 任何一个在选择操作中阻塞的线程都将被唤醒,与选择器相关的通道将被注销，而键将被取消。 调用interrupt( ) 管理选择键一旦一个选择器将一个键添加到它的已选择的键的集合中，它就不会移除这个键。并且，一旦一个键处于已选择的键的集合中，这个键的ready集合将只会被设置，而不会被清理。 当通道上的至少一个感兴趣的操作就绪时，键的ready集合就会被清空，并且当前已经就绪的操作将会被添加到ready集合中。该键之后将被添加到已选择的键的集合中。 清理一个SelectKey的ready集合的方式是将这个键从已选择的键的集合中移除。选择键的就绪状态只有在选择器对象在选择操作过程中才会修改。 例子:使用select( )来为多个通道提供服务public class SelectSockets { public static int PORT_NUMBER = 1234; private ByteBuffer buffer = ByteBuffer.allocateDirect(1024); public static void main(String[] argv) throws Exception { new SelectSockets().go(argv); } public void go(String[] argv) throws Exception { int port = PORT_NUMBER; if (argv.length &gt; 0) { // Override default listen port port = Integer.parseInt(argv[0]); } System.out.println(&quot;Listening on port &quot; + port); ServerSocketChannel serverChannel = ServerSocketChannel.open(); ServerSocket serverSocket = serverChannel.socket(); Selector selector = Selector.open(); serverSocket.bind(new InetSocketAddress(port)); serverChannel.configureBlocking(false); serverChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { int n = selector.select(); if (n == 0) { continue; // nothing to do } Iterator it = selector.selectedKeys().iterator(); while (it.hasNext()) { SelectionKey key = (SelectionKey) it.next(); if (key.isAcceptable()) { ServerSocketChannel server = (ServerSocketChannel) key.channel(); SocketChannel channel = server.accept(); registerChannel(selector, channel, SelectionKey.OP_READ); sayHello(channel); } if (key.isReadable()) { readDataFromSocket(key); } it.remove(); } } } protected void registerChannel(Selector selector, SelectableChannel channel, int ops) throws Exception { if (channel == null) { return; // could happen } channel.configureBlocking(false); channel.register(selector, ops); } protected void readDataFromSocket(SelectionKey key) throws Exception { SocketChannel socketChannel = (SocketChannel) key.channel(); int count; buffer.clear(); while ((count = socketChannel.read(buffer)) &gt; 0) { buffer.flip(); // Make buffer readable while (buffer.hasRemaining()) { socketChannel.write(buffer); } buffer.clear(); // Empty buffer } if (count &lt; 0) { // Close channel on EOF, invalidates the key socketChannel.close(); } } private void sayHello(SocketChannel channel) throws Exception { buffer.clear(); buffer.put(&quot;Hi there!\r\n&quot;.getBytes()); buffer.flip(); channel.write(buffer); } } 并发性选择器对象是线程安全的，但它们包含的键集合不是 如果底层的Set被改变了，它们将会抛出java.util.ConcurrentModificationException，因此如果您期望在多个线程间共享选择器和/或键，请对此做好准备。您可以直接修改选择键，但请注意您这么做时可能会彻底破坏另一个线程的Iterator。 如果在多个线程并发地访问一个选择器的键的集合的时候存在任何问题，您可以采取一些步骤来合理地同步访问。在执行选择操作时，选择器在Selector对象上进行同步，然后是已注册的键的集合，最后是已选择的键的集合，按照这样的顺序。已取消的键的集合也在选择过程的的第1步和第3步之间保持同步（当与已取消的键的集合相关的通道被注销时）。 异步关闭能力任何时候都有可能关闭一个通道或者取消一个选择键。除非您采取步骤进行同步，否则键的状态及相关的通道将发生意料之外的改变。一个特定的键的集合中的一个键的存在并不保证键仍然是有效的，或者它相关的通道仍然是打开的。 一个线程在关闭一个处于选择操作中的通道时，被阻塞于无限期的等待。当一个通道关闭时，它相关的键也就都被取消了。这并不会影响正在进行的select( )，但这意味着在您调用select( )之前仍然是有效的键，在返回时可能会变为无效。 选择过程的可扩展性选择器可以简化用单线程同时管理多个可选择通道的实现。使用一个线程来为多个通道提供服务，通过消除管理各个线程的额外开销，可能会降低复杂性并可能大幅提升性能。对单CPU的系统而言这可能是一个好主意，但对于一个多CPU的系统呢？ 那么让不同通道请求不同的服务类的办法如何?这只会形成这个场景的一个更小的版本。 一个更好的策略是对所有的可选择通道使用一个选择器，并将对就绪通道的服务委托给其他线程。您只用一个线程监控通道的就绪状态并使用一个协调好的工作线程池来处理共接收到的数据。根据部署的条件，线程池的大小是可以调整的（或者它自己进行动态的调整）。对可选择通道的管理仍然是简单的，而简单的就是好的。]]></content>
      <categories>
        <category>java基础</category>
        <category>javaNIO</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp/http]]></title>
    <url>%2F2018%2F03%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2Ftcp-http%2F</url>
    <content type="text"><![CDATA[计算机网络tcpTCP报文格式 序号：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。 确认序号：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1。 标志位：共6个，即URG、ACK、PSH、RST、SYN、FIN等，具体含义如下： URG：紧急指针（urgent pointer）有效。 ACK：确认序号有效。 PSH：接收方应该尽快将这个报文交给应用层。 RST：重置连接。 SYN：发起一个新连接。 FIN：释放一个连接。 确认方Ack=发起方Req+1，两端配对。 TCP连接Transmission Control Protocol 传输控制协议 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了 SYN攻击： SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。 TCP释放四次挥手（Four-Way Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 为什么建立连接是三次握手，而关闭连接却是四次挥手呢？ 这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。 UDPUDP 是User Datagram Protocol的简称，提供面向事务的简单不可靠信息传送服务 关于何时，发什么数据应用层控制更为精细：应用程序交付报文给UDP，UDP打包给网络层 无需建立连接：无需3次握手 无连接状态：udp不维护连接状态 分组首部开销小：20byte/8byte udp也可以可靠，但是要构建在应用程序中 udp报文 tcp与udp区别 TCP协议面向连接，UDP协议面向非连接； TCP协议传输速度慢，UDP协议传输速度快 TCP协议可靠数据传输，UDP协议可能丢包； UDP程序结构较简单 流模式和数据包模式 通过TCP连接给另一端发送数据，你只调用了一次write，发送了100个字节，但是对方可以分10次收完，每次10个字节；你也可以调用10次write，每次10个字节，但是对方可以一次就收完. UPD是基于报文的，在接收的时候，每次最多只能读取一个报文，报文和报文是不会合并的，如果缓冲区小于报文长度，则多出的部分会被丢弃。 停等协议和流水线操作停等协议每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。流水线操作 必须增加序号范围 发送方和接收方必须缓存多个分组 处理丢失,损坏,延时, 回退N步(GBN),选择重传(SR) GBN:发送的窗口大小为n，接受方的窗口仍然为1。具体看下面的图，这里假设n=9： 首先发送方一口气发送10个数据帧，前面两个帧正确返回了，数据帧2出现了错误，这时发送方被迫重新发送2-8这7个帧，接受方也必须丢弃之前接受的3-8这几个帧。 后退n协议的好处无疑是提高了效率，但是一旦网络情况糟糕，则会导致大量数据重发 只有一个计时器，只记录每一组发送的时间 SR:后退n协议的另外一个问题是，当有错误帧出现后，总是要重发该帧之后的所有帧，毫无疑问在网络不是很好的情况下会进一步恶化网络状况。 接收端总会缓存所有收到的帧，当某个帧出现错误时，只会要求重传这一个帧，只有当某个序号后的所有帧都正确收到后，才会一起提交给高层应用。重传协议的缺点在于接受端需要更多的缓存。 每个分组都有计时器 SR发送方的事件与动作： 收到ACK，倘若该分组序号在窗口内，则SR发送方将那个被确认的分组标记为已接收。如果该分组的序号等于send_base，则窗口基序号向前移动到具有最小序号的未确认分组处。如果窗口移动了并且有序号落在窗口内的未发送分组，则发送这些分组。 SR接收方的事件与动作： 序号在［rcv_base，rcv_base+N-1]内的分组被正确接收。在此情况下，收到的分组落在接收方的窗口内，一个选择ACK被回送给发送方。并判断是否交付 序号在［rcv_base-N,rcv_base-1]内的分组被正确收到（客户端重传）。在此情况下，必须产生一个ACK，即使该分组是接收方以前已确认过的分组。 窗口大小：发送方=接收方 TCPTCP并不是每一个报文段都会回复ACK的，可能会对两个报文段发送一个ACK，也可能会对多个报文段发送1个ACK【累计ACK】 滑动窗口协议是传输层进行流控的一种措施，接收方通过通告发送方自己的窗口大小，从而控制发送方的发送速度，从而达到防止发送方发送速度过快而导致自己被淹没的目的。 ACK包含两个非常重要的信息： 一是期望接收到的下一字节的序号n，该n代表接收方已经接收到了前n-1字节数据 二是当前的窗口大小m，如此发送方在接收到ACK包含的这两个数据后就可以计算出还可以发送多少字节的数据给对方，假定当前发送方已发送到第x字节，则可以发送的字节数就是y=m-(x-n). ​ 可靠数据传输机制及其用途的总结​ 检验和：用于检测在一个传输分组中的比特错误 ​ 定时器：用于超时／重传一个分组，可能因为该分组（或其ACK）在信道中丢失了。由于当一个分组延时但未丢失（过早超时），或当一个分组已经被接收方收到但从接收方到发送方的ACK丢失时，可能产生超时事件，所以接收方可能会收到一个分组的多个冗余副本。 ​ 序号：用于为从发送方流向接收方的数据分组按顺序编号。所接收分组的序号空间的空隙可使接收方检测出丢失的分组。具有相同序号的分组可使接收方检测出一个分组的冗余副本。 ​ 确认：接收方用于告诉发送方一个分组或一组分组已经被正确滴接收到了。确认报文通常携带着被确认的分组或多个分组的序号。确认可以是逐个的或累积的，着取决于协议。 ​ 否定确认：接收方用于告诉发送方某个分组未被正确地接收。否定确认报文通常携带着未被正确接收到分组的序号。 ​ 窗口、流水线：发送方也许被限制仅发送那些序号落在一个指定范围内的分组。通过允许一次发送多个分组但未被确认，发送方的利用率可以在停等操作模式的基础上得到增加。我们很快会看到，窗口长度可根据接收方接收的缓存报文的能力、网络中的拥塞程度或两者情况来进行设置。 TCP拥塞控制拥塞窗口(cwnd):指某一源端数据流在一个RTT内可以最多发送的数据包数。指导性原则: A lost segment implies congestion, and hence, the TCP sender’s rate should be decreased when a segment is lost.(丢失报文即阻塞) An acknowledged segment indicates that the network is delivering the sender’ssegments to the receiver, and hence, the sender’s rate can be increased when anACK arrives for a previously unacknowledged segment(收到ack即非阻塞) Bandwidth probing.TCP’s strategy for adjusting itstransmission rate is to increase its rate in response to arriving ACKs until a lossevent occurs, at which point, the transmission rate is decreased.(带宽检测,收到ack加速,丢包减速) 拥塞控制算法: 慢起动:cwnd小,然后依据ack快速增长(指数),到达拥塞最大值 拥塞避免:一旦进入此状态,cwnd值大约为之前的一半,慢增长(+1) 快速恢复:快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 为了防止cwnd增长过大引起网络拥塞，还需设置一个慢开始门限ssthresh状态变量。ssthresh的用法如下： 当cwnd&lt;ssthresh时，使用慢开始算法。 当cwnd&gt;ssthresh时，改用拥塞避免算法。 当cwnd=ssthresh时，慢开始与拥塞避免算法任意。 快重传 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半。但是接下去并不执行慢开始算法。 考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。 ACK延迟确认机制接收方在收到数据后，并不会立即回复ACK,而是延迟一定时间。一般ACK延迟发送的时间为200ms，但这个200ms并非收到数据后需要延迟的时间。系统有一个固定的定时器每隔200ms会来检查是否需要发送ACK包。这样做有两个目的。 这样做的目的是ACK是可以合并的，也就是指如果连续收到两个TCP包，并不一定需要ACK两次，只要回复最终的ACK就可以了，可以降低网络流量。 如果接收方有数据要发送，那么就会在发送数据的TCP数据包里，带上ACK信息。这样做，可以避免大量的ACK以一个单独的TCP包发送，减少了网络流量。 http在TCP/IP协议栈中的位置在TCP/IP协议栈中的位置HTTP协议通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。如下图所示： 默认HTTP的端口号为80，HTTPS的端口号为443。 HTTP的请求响应模型HTTP的请求响应模型HTTP协议永远都是客户端发起请求，服务器回送响应。无法实现在客户端没有发起请求的时候，服务器将消息推送给客户端。HTTP协议是一个无状态的协议，协议的状态是指下一次传输可以“记住”这次传输信息的能力。 工作流程一次HTTP操作称为一个事务，其工作过程可分为四步： 客户机与服务器需要建立连接 客户机发送一个请求给服务器 服务器接到请求后，给予相应的响应信息 客户端接收服务器所返回的信息，然后客户机与服务器断开连接。 HTTP/1.0和HTTP/1.1的比较 建立连接方面： HTTP/1.0 每次请求都需要建立新的TCP连接，连接不能复用。HTTP/1.1 新的请求可以在上次请求建立的TCP连接之上发送，连接可以复用。优点是减少重复进行TCP三次握手的开销，提高效率。 注意：在同一个TCP连接中，新的请求需要等上次请求收到响应后，才能发送。 Host域 HTTP1.1在Request消息头里头多了一个Host域, HTTP1.0则没有这个域。 HTTP之请求消息Request请求行（request line）、请求头部（header）、空行和请求数据四个部分组成。 1234567GET /562f25980001b1b106000338.jpg HTTP/1.1Host img.mukewang.comUser-Agent Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36Accept image/webp,image/*,*/*;q=0.8Referer http://www.imooc.com/Accept-Encoding gzip, deflate, sdchAccept-Language zh-CN,zh;q=0.8 12345678POST / HTTP1.1Host:www.wrox.comUser-Agent:Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022)Content-Type:application/x-www-form-urlencodedContent-Length:40Connection: Keep-Alivename=Professional%20Ajax&amp;publisher=Wiley Accept：浏览器可接受的MIME类型。 Accept-Charset：浏览器可接受的字符集。 Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。 Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到。 Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中。 Connection：表示是否需要持久连接。 Content-Length：表示请求消息正文的长度。post才需要，因为其他的方式正文为0 Cookie：这是最重要的请求头信息之一 From：请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它。 Host：初始URL中的主机和端口。 If-Modified-Since：只有当所请求的内容在指定的日期之后又经过修改才返回它，否则返回304”Not Modified”应答。 Pragma：指定”no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝。 Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。 HTTP之响应消息ResponseHTTP响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文。 12345678910HTTP/1.1 200 OKDate: Fri, 22 May 2009 06:07:21 GMTContent-Type: text/html; charset=UTF-8&lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body&gt; &lt;!--body goes here--&gt; &lt;/body&gt;&lt;/html&gt; 状态行，HTTP协议版本号， 状态码， 状态消息 消息报头 空行 响应正文 HTTP请求方法根据HTTP标准，HTTP请求可以使用多种请求方法。HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断 KeepAliveConnection: close/Keep-Alive HTTP 1.0加上Keep-Alive header也可以提供HTTP的持续作用功能。 Keep-Alive: timeout=5, max=100 过期时间5秒 最多一百次请求 HTTP/1.0的默认情况下，是不会使用Keep-Alive的, HTTP/1.1持久连接将是默认的连接方式。 HTTPS传输协议原理以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL http的状态响应码1**(信息类)：表示接收到请求并且继续处理​ 100——客户必须继续发出请求​ 101——客户要求服务器根据请求转换HTTP协议版本 2**(响应成功)：表示动作被成功接收、理解和接受 3**(重定向类)：为了完成指定的动作，必须接受进一步处理 4*(客户端错误类)：请求包含错误语法或不能正确执行 5**(服务端错误类)：服务器不能正确执行一个正确的请求 URI 和 URL 区别统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来。 统一资源定位符URL是URI的子集，URL是以描述位置来唯一确定的。 假设所有的Html文档都有唯一的编号，记作html:xxxxx，xxxxx是一串数字，即Html文档的身份证号码，这个能唯一标识一个Html文档，那么这个号码就是一个URI。而URL则通过描述是哪个主机上哪个路径上的文件来唯一确定一个资源，也就是定位的方式来实现的URI。 解决HTTP无状态的问题通过Cookies保存状态信息cookie在请求和响应的http首部，从服务器到客户端，再从客户端到服务器。服务器用cookie来指示会话ID，登录凭据等。 通过Session保存状态信息Session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。客户端的请求里包含了一个session标识 - 称为 session id, 服务器就按照session id把这个 session检索出来使用 Session的实现方式： 使用Cookie来实现 服务器给每个Session分配一个唯一的JSESSIONID，并通过Cookie发送给客户端。当客户端发起新的请求的时候，将在Cookie头中携带这个JSESSIONID。 使用URL回写来实现 URL回写是指服务器在发送给浏览器页面的所有链接中都携带JSESSIONID的参数。Tomcat对Session的实现，是一开始同时使用Cookie和URL回写机制，如果发现客户端支持Cookie，就继续使用Cookie，停止使用URL回写。如果发现Cookie被禁用，就一直使用URL回写。 Cookie和Session有以下明显的不同点： Cookie将状态保存在客户端，Session将状态保存在服务器端； Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies。Session并没有在HTTP的协议中定义； Session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，通过URL回写机制 服务器端的SESSION机制更安全些。因为它不会任意读取客户存储的信息。 浏览器输入网址发生了什么 浏览器里输入网址 浏览器查找域名的IP地址 导航的第一步是通过访问的域名找出其IP地址。DNS查找过程如下： 浏览器缓存 – 浏览器会缓存DNS记录一段时间。 不同浏览器会储存个自固定的一个时间（2分钟到30分钟不等）。 系统缓存 – 如果在浏览器缓存里没有找到需要的记录，浏览器会做一个系统调用（windows里是gethostbyname）。这样便可获得系统缓存中的记录。 路由器缓存 – 接着，前面的查询请求发向路由器，它一般会有自己的DNS缓存。 ISP DNS 缓存 – 接下来要check的就是ISP缓存DNS的服务器。 递归搜索 – 你的ISP的DNS服务器从跟域名服务器开始进行递归搜索，从.com顶级域名服务器到Facebook的域名服务器。一般DNS服务器的缓存中会有.com域名服务器中的域名，所以到顶级服务器的匹配过程不是那么必要了。 DNS递归查找如下图所示： 浏览器给web服务器发送一个HTTP请求 facebook服务的永久重定向响应 浏览器跟踪重定向地址 服务器“处理”请求 服务器发回一个HTML响应 浏览器开始显示HTML 浏览器发送获取嵌入在HTML中的对象 浏览器发送异步（AJAX）请求]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http]]></title>
    <url>%2F2018%2F03%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2Fhttp%2F</url>
    <content type="text"><![CDATA[浏览器输入网址发生了什么 首先嘛，你得在浏览器里输入网址 浏览器查找域名的IP地址 导航的第一步是通过访问的域名找出其IP地址。DNS查找过程如下： 浏览器缓存 – 浏览器会缓存DNS记录一段时间。 不同浏览器会储存个自固定的一个时间（2分钟到30分钟不等）。 系统缓存 – 如果在浏览器缓存里没有找到需要的记录，浏览器会做一个系统调用（windows里是gethostbyname）。这样便可获得系统缓存中的记录。 路由器缓存 – 接着，前面的查询请求发向路由器，它一般会有自己的DNS缓存。 ISP DNS 缓存 – 接下来要check的就是ISP缓存DNS的服务器。 递归搜索 – 你的ISP的DNS服务器从跟域名服务器开始进行递归搜索，从.com顶级域名服务器到Facebook的域名服务器。一般DNS服务器的缓存中会有.com域名服务器中的域名，所以到顶级服务器的匹配过程不是那么必要了。 DNS递归查找如下图所示： 浏览器给web服务器发送一个HTTP请求 facebook服务的永久重定向响应 浏览器跟踪重定向地址 服务器“处理”请求 服务器发回一个HTML响应 浏览器开始显示HTML 浏览器发送获取嵌入在HTML中的对象 浏览器发送异步（AJAX）请求 http1.0过程 客户端打开一个tcp连接 客户端发送请求 服务器发送响应 服务器关闭连接 http1.1在第1,4步之间 2,3可以反复发送 请求和响应可以分多个块发送，更好的扩展性 基本形式一个首部行，一个包含元数据的HTTP首部，一个空行，一个消息体 http请求 get请求没有消息体，以一个空行结束 Accept：浏览器可接受的MIME类型。 Accept-Charset：浏览器可接受的字符集。 Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。 Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到。 Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中。 Connection：表示是否需要持久连接。 Content-Length：表示请求消息正文的长度。post才需要，因为其他的方式正文为0 Cookie：这是最重要的请求头信息之一 From：请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它。 Host：初始URL中的主机和端口。 If-Modified-Since：只有当所请求的内容在指定的日期之后又经过修改才返回它，否则返回304”Not Modified”应答。 Pragma：指定”no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝。 Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。 http响应 响应的第一行称为状态行。它包含响应所用的 HTTP 版本、状态编码 (200) 和原因短语。示例中包含一个头，其中具有五个字段，接着是一个空行（回车和换行符），然后是响应正文的头两行。 HTTP码应码响应码由三位十进制数字组成，它们出现在由HTTP服务器发送的响应的第一行。 响应码分五种类型，由它们的第一位数字表示： 1xx：信息，请求收到，继续处理 2xx：成功，行为被成功地接受、理解和采纳 3xx：重定向，为了完成请求，必须进一步执行的动作 4xx：客户端错误，请求包含语法错误或者请求无法实现 5xx：服务器错误，服务器不能实现一种明显无效的请求 Keep-Alivehttp 1.0会为每一个请求打开一个新连接。实际上一个web会话中打开和关闭所有连接花费的时间远大于传输数据的时间。对于使用ssl或tls的加密https这个问题更为严重。 Http方法 get： 本质就是发送一个请求来取得服务器上的某一资源。资源通过一组HTTP头和呈现数据（如HTML文本，或者图片或者视频等）返回给客户端。 put: 将资源上传到已知url的服务器 delete： 从指定的url删除一个资源 post： 将资源上传到已知url的服务器，但是没有指定如何处理这个资源 head： 相等于get，不过只返回头部 请求主体 Coolkiecookie在请求和响应的http首部，从服务器到客户端，再从客户端到服务器。服务器用cookie来指示会话ID，登录凭据等。 服务器可以设置不止一个cookie。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper原理]]></title>
    <url>%2F2018%2F03%2F16%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fzookeeper%2F%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Zookeeper的基本概念角色 系统模型 目的 最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 2 . 可靠性：具有简单、健壮、良好的性能，如果消息m被到一台服务器接受，那么它将被所有的服务器接受。 3 .实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 4 .等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。 5.原子性：更新只能成功或者失败，没有中间状态。 6 .顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。 ZooKeeper的工作原理Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。 zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。 每个Server在工作过程中有三种状态： LOOKING：当前Server不知道leader是谁，正在搜寻 LEADING：当前Server即为选举出来的leader FOLLOWING：leader已经选举出来，当前Server与之同步 选主流程basic paxos流程 fast paxosfast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。其流程图如下所示： 同步流程选完leader以后，zk就进入状态同步过程。 完成同步后通知follower 已经成为uptodate状态； Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。 工作流程leaderLeader主要有三个功能： 恢复数据； 维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型； Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。 PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；ACK消息是Follower的对提议的回复，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。 followerFollower主要有四个功能： 向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）； 接收Leader消息并进行处理； 接收Client的请求，如果为写请求，发送给Leader进行投票； 返回Client结果。 Follower的消息循环处理如下几种来自Leader的消息： PING消息： 心跳消息； PROPOSAL消息：Leader发起的提案，要求Follower投票； COMMIT消息：服务器端最新一次提案的信息； UPTODATE消息：表明同步完成； REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息； SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。 在实际实现中，Follower是通过5个线程来实现功能的。]]></content>
      <categories>
        <category>大数据</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法模版]]></title>
    <url>%2F2018%2F03%2F16%2F%E7%AE%97%E6%B3%95%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A8%A1%E7%89%88%2F%E7%AE%97%E6%B3%95%E6%A8%A1%E7%89%88%2F</url>
    <content type="text"><![CDATA[1. 两个有序数组的topK问题数组大小为m，n 解法： 截取两个数组长度 &lt;=k 结果肯定在两个数组的前k个数 比较中间值的大小，中间值小的前部分数组必在结果中，记大小为k1, 则递归求 arr1(k1+1,n)和arr2(0,m)的top(k-k1) 递归出口： 某一数组大小为0 123456789101112int findKth(int[] nums1, int[] nums2, int k) &#123; int m = nums1.length, n = nums2.length; if (m &gt; n) return findKth(nums2, nums1, k); if (m == 0) return nums2[k - 1]; if (k == 1) return Math.min(nums1[0], nums2[0]); int i = Math.min(m, k / 2), j = Math.min(n, k / 2); if (nums1[i - 1] &gt; nums2[j - 1]) &#123; return findKth(nums1, Arrays.copyOfRange(nums2, j, n), k - j); &#125; else &#123; return findKth(Arrays.copyOfRange(nums1, i, m), nums2, k - i); &#125;&#125; 推广： 求两个有序数组中位数 12int m = nums1.length, n = nums2.length, left = (m + n + 1) / 2, right = (m + n + 2) / 2;return (findKth(nums1, nums2, left) + findKth(nums1, nums2, right)) / 2.0; 2. 最大水容器 开始1,7进场，然后抛弃短的7，求1,6，然后抛弃短的2.。。。。如果一样长，左右随便取一个 由于7短，故7与任何一个组容器均小于1,7，故7可以抛弃 123456789101112131415161718public int MaxArea(int[] height)&#123; int left = 0, right = height.Length - 1; int maxArea = 0; while (left &lt; right &amp;&amp; left &gt;= 0 &amp;&amp; right &lt;= height.Length - 1) &#123; maxArea = Math.Max(maxArea, Math.Min(height[left], height[right]) * (right - left)); if (height[left] &gt; height[right]) &#123; right--; &#125; else &#123; left++; &#125; &#125; return maxArea;&#125; 3. 快排1234567891011121314151617181920212223void quick_sort(int s[], int l, int r) &#123; if (l &lt; r) &#123; //Swap(s[l], s[(l + r) / 2]); //将中间的这个数和第一个数交换 参见注1 int i = l, j = r, x = s[l]; while (i &lt; j) &#123; while(i &lt; j &amp;&amp; s[j] &gt;= x) // 从右向左找第一个小于x的数 j--; if(i &lt; j) s[i++] = s[j]; while(i &lt; j &amp;&amp; s[i] &lt; x) // 从左向右找第一个大于等于x的数 i++; if(i &lt; j) s[j--] = s[i]; &#125; s[i] = x; quick_sort(s, l, i - 1); // 递归调用 quick_sort(s, i + 1, r); &#125; &#125;]]></content>
      <categories>
        <category>算法和数据结构</category>
        <category>模版</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper简介]]></title>
    <url>%2F2018%2F03%2F16%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fzookeeper%2F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Zookeeper 分布式服务框架是 Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 zk不能阻止局部故障，但可以提供工具集建立安全处理局部故障的分布式应用 特性简易ZooKeeper的最重要核心就是一个精简文件系统，提供一些简单的操作以及附加的抽象（例如排序和通知）。 易表达ZooKeeper的原型是一个丰富的集合，它们是一些已建好的块，可以用来构建大型的协作数据结构和协议，例如：分布式队列、分布式锁以及一组对等体的选举。 高可用性ZooKeeper运行在一些集群上，被设计成可用性较高的，因此应用程序可以依赖它。ZooKeeper可以帮助你的系统避免单点故障，从而建立一个可靠的应用程序。 松散耦合ZooKeeper的交互支持参与者之间并不了解对方。例如：ZooKeeper可以被当做一种公共的机制，使得进程彼此不知道对方的存在也可以相互发现并且交互，对等方可能甚至不是同步的。 ZooKeeper是一个库ZooKeeper提供了一个开源的、共享的执行存储，以及通用协作的方法，分担了每个程序员写通用协议的负担。随着时间的推移，人们可以增加和改进这个库来满足自己的需求。 组成员制将zk视为一个高可用文件系统。Zookeeper的数据存储采用的是结构化存储，结构化存储是没有文件和目录的概念，里边的目录和文件被抽象成了节点（node），zookeeper里可以称为znode。 创建组Znode有两种类型：短暂的和持久的。短暂的znode在创建的客户端与服务器端断开（无论是明确的断开还是故障断开）连接时，该znode都会被删除；相反，持久的znode则不会。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CreateGroup implements Watcher&#123; private static final int SESSION_TIMEOUT = 1000;//会话延时 private ZooKeeper zk = null; private CountDownLatch countDownLatch = new CountDownLatch(1);//同步计数器 public void process(WatchedEvent event) &#123; if(event.getState() == KeeperState.SyncConnected)&#123; countDownLatch.countDown();//计数器减一 &#125; &#125; /** * 创建zk对象 * 当客户端连接上zookeeper时会执行process(event)里的countDownLatch.countDown()，计数器的值变为0，则countDownLatch.await()方法返回。 * @param hosts * @throws IOException * @throws InterruptedException */ public void connect(String hosts) throws IOException, InterruptedException &#123; zk = new ZooKeeper(hosts, SESSION_TIMEOUT, this); countDownLatch.await();//阻塞程序继续执行 &#125; /** * 创建group * * @param groupName 组名 * @throws KeeperException * @throws InterruptedException */ public void create(String groupName) throws KeeperException, InterruptedException &#123; String path = &quot;/&quot; + groupName; String createPath = zk.create(path, null, Ids.OPEN_ACL_UNSAFE/*允许任何客户端对该znode进行读写*/, CreateMode.PERSISTENT/*持久化的znode*/); System.out.println(&quot;Created &quot; + createPath); &#125; /** * 关闭zk * @throws InterruptedException */ public void close() throws InterruptedException &#123; if(zk != null)&#123; try &#123; zk.close(); &#125; catch (InterruptedException e) &#123; throw e; &#125;finally&#123; zk = null; System.gc(); &#125; &#125; &#125;&#125; 加入组类似创建，路径不一样 列出组成员String path = &quot;/&quot; + groupName; List&lt;String&gt; children = zk.getChildren(path, false); 查看节点的情况 删除ZNODE1234567891011121314151617181920212223/** * 删除分组 * @author leo * */public class DeleteGroup extends ConnectionWatcher &#123; public void delete(String groupName) &#123; String path = &quot;/&quot; + groupName; try &#123; List&lt;String&gt; children = zk.getChildren(path, false); for(String child : children)&#123; zk.delete(path + &quot;/&quot; + child, -1); &#125; zk.delete(path, -1);//版本号为-1， &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; zk.delete(path,version)方法的第二个参数是znode版本号，如果提供的版本号和znode版本号一致才会删除这个znode，这样可以检测出对znode的修改冲突。通过将版本号设置为-1，可以绕过这个版本检测机制，无论znode的版本号是什么，都会直接将其删除。 Zookeeper服务 ACL 控制访问列表 数据模型Zookeeper 维护一个分级节点树 临时性znodeznode分 临时性和永久性，创建时决定，不可改变 临时性在与创建它的客户端会话结束则删除，永久性必须明确删除后才会 临时性znode不该有子节点 序号一个顺序的znode由zeekeeper给定一个序号作为其名称的一部分 Watch在znode有改变时，Watch使客户端了解相应信息 Watch只被触发一次，若要多次提醒，要多次注册 相关操作ZooKeeper中共有9中操作： create：创建一个znode（父节点必须存在） delete：删除一个znode（没有子节点） exists：测试一个znode getACL，setACL：获取/设置一个znode的ACL（权限控制） getChildren：获取一个znode的子节点 getData，setData：获取/设置一个znode所保存的数据 sync：将客户端的znode视图与ZooKeeper同步 这里更新数据是必须要提供znode的版本号（也可以使用-1强制更新，这里可以执行前通过exists方法拿到znode的元数据Stat对象，然后从Stat对象中拿到对应的版本号信息），如果版本号不匹配，则更新会失败。因此一个更新失败的客户端可以尝试是否重试或执行其它操作。 delete，setData必须指明版本号 Watch 触发器zookeeper所有读操作(getData(),getChildren(),exists())具有设置watch的选项。 zookeeper watch的定义如下：watch事件是一次性触发器，当watch监视的数据发生变化时，通知设置了该watch的client，即watcher。 对于watch，zookeeper提供以下保证： watch对于其他事件、watch、异步响应是有序的。zookeeper client library保证有序分发 客户端监视一个节点，总是先获取watch事件，再发现节点的数据变化。 watch事件的顺序对应于zookeeper服务所见的数据更新的顺序。 关于watch要记住的是： watch是一次性触发的，如果获取一个watch事件并希望得到新变化的通知，需要重新设置watch watch是一次性触发的并且在获取watch事件和设置新watch事件之间有延迟，所以不能可靠的观察到节点的每一次变化。要认识到这一点。 watch object只触发一次，比如，一个watch object被注册到同一个节点的getData()和exists()，节点被删除，仅对应于exists()的watch ojbect被调用 若与服务端断开连接，直到重连后才能获取watch事件。 ACL一个znode对应一个ACL，决定谁可以对znode操作 验证模式： digest 用户名和密码 host 主机名 id ip地址 执行zk 运行在以叫做 ensemble 的集群上，通过复制获得高可用性 对znode的每一次修改都复制到 ensemble 的集群上，如果小部分出现故障，则有一台最新，其他的保存副本直至最新。过程为两个阶段，可能无限重复 领导者选举 大部分跟随者同步了领导者后才算完成 原子广播 原子性。领导者获得更新消息后，通知跟随者，大部分跟随者更新后领导者更新 一致性每一个znode更新会给定义一zxid（事务ID），更新是被排序的 特性： 顺序的一致性 原子性 单系统印象 容错性 合时性 会话客户端启动时，尝试与服务器连接，直到成功连上一台。但会话有超时，用ping保持活跃 状态应用场景Zookeeper 从设计模式角度来看，是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式 统一命名服务（Name Service）配置管理（Configuration Management）配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。像这样的配置信息完全可以交给 Zookeeper 来管理，将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。]]></content>
      <categories>
        <category>大数据</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat classLoader]]></title>
    <url>%2F2018%2F03%2F16%2Fjava%E6%A1%86%E6%9E%B6%2Ftomcat%2FClassLoader%2F</url>
    <content type="text"><![CDATA[8 ClassLoadertomcat的classLoader是自定义的classLoader。每个容器要载入所需的servlet类。能够 定义访问规则，如果通过jvm的classLoader，则servlet能够访问所有的classpath下的所有class，而servlet只允许访问WEB-INF/classs目录及子目录下面的类 可以实现自动重载功能 缓存 tomcat的classLoader类接口是org.apache.catalina.Loader 8.1 java的classLoader关于jvm的classLoader参考 classLoader讲解 classLoader加载类时先查缓存，再从父加载器中寻找，最后自己再寻找 8.2 Loader接口servlet及其相关类需要定义一些规制：只允许访问WEB-INF/classs目录及子目录下面的类，不能访问其他classpath的类 tomcat的载入器指的是Web应用程序载入器，而不仅仅是类载入器。载入器必须实现org.apache.catalina.Loader接口。在载入器的实现中，会使用一个自定义载入器，是WebappClassLoaderBase的实例 Loader还定义了对仓库的操作，WEB-INF/classs和WEB-INF/lib是作为仓库添加到载入器中的。 12345678910111213public interface Loader &#123; public void backgroundProcess(); public ClassLoader getClassLoader(); public Context getContext(); public void setContext(Context context); public boolean getDelegate(); public void setDelegate(boolean delegate); public boolean getReloadable(); public void setReloadable(boolean reloadable); public void addPropertyChangeListener(PropertyChangeListener listener); public boolean modified(); public void removePropertyChangeListener(PropertyChangeListener listener);&#125; 解读api： backgroundProcess，执行周期性任务，如加载等方法，被Context调用 getClassLoader对应加载仓库下的类的classLoader， 注意这里的Loader并未和文件路径绑定，而是抽象出仓库的概念 tomcat的载入器通常会和一个Context级别的容器相关联，set/get 支持ReLoad modified() 绑定的仓库是否有变 是否委托给父加载器set/getDelegate(); 一般Context是禁用自动重载功能的，可以通过如下更改 1&lt;Context path="/myApp" docBase="myApp" debug="0" reloadable="true"/&gt; 8.2 WebappLoader类12public class WebappLoader extends LifecycleMBeanBase implements Loader, PropertyChangeListener LifecycleMBeanBase是LifecycleBase子类，实现了JmxEnabled接口，JmxEnabled提供基于jmx的管理 WebappLoader会创建一个WebappClassLoaderBase的实例作为其类载入器 WebappLoader的实现比较易懂 8.2.1 主要属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 加载类的加载器 */private WebappClassLoaderBase classLoader = null;/** * The Context with which this Loader has been associated. */private Context context = null;/** * The "follow standard delegation model" flag that will be used to * configure our ClassLoader. */private boolean delegate = false;/** * The Java class name of the ClassLoader implementation to be used. * This class should extend WebappClassLoaderBase * 根据这个类名反射生成classLoader并赋值 */private String loaderClass = ParallelWebappClassLoader.class.getName();/** * The parent class loader of the class loader we will create. */private ClassLoader parentClassLoader = null;/** * The reloadable flag for this Loader. */private boolean reloadable = false;/** * The string manager for this package. * StringManager用于管理异常码，可以阅读下源码 */protected static final StringManager sm = StringManager.getManager(Constants.Package);/** * Classpath set in the loader. */private String classpath = null;/*** The property change support for this component.*/protected final PropertyChangeSupport support = new PropertyChangeSupport(this); 注意parentClassLoader和classLoader区别，parentClassLoader取自context.getParentClassLoader()，并传给classLoader，classLoader会根据 delegate判断是否让parentClassLoader加载类 8.2.2 get/setter12345678910111213141516171819202122 /** * Return the Java class loader to be used by this Container. */ @Override public void setContext(Context context) &#123;//.... if (this.context != null) &#123; this.context.removePropertyChangeListener(this); &#125; // Process this property change Context oldContext = this.context; this.context = context; support.firePropertyChange("context", oldContext, this.context); // Register with the new Container (if any) if (this.context != null) &#123; setReloadable(this.context.getReloadable()); this.context.addPropertyChangeListener(this); &#125; &#125; 以setContext为例，其他很像。 8.2.3 其他公有方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 // --------------------- Public Methods /** * 更换线程的classLoader，再通过线程进行context.reload() * 这里的线程显然是系统分派的一个子线程，可能是context内部分配的 */ @Override public void backgroundProcess() &#123; if (reloadable &amp;&amp; modified()) &#123; try &#123; Thread.currentThread().setContextClassLoader (WebappLoader.class.getClassLoader()); if (context != null) &#123; context.reload(); &#125; &#125; finally &#123; if (context != null &amp;&amp; context.getLoader() != null) &#123; Thread.currentThread().setContextClassLoader (context.getLoader().getClassLoader()); &#125; &#125; &#125; &#125; public String[] getLoaderRepositories() &#123;//... URL[] urls = classLoader.getURLs(); String[] result = new String[urls.length]; for (int i = 0; i &lt; urls.length; i++) &#123; result[i] = urls[i].toExternalForm(); &#125; return result; &#125; public String getLoaderRepositoriesString() &#123; String repositories[]=getLoaderRepositories(); StringBuilder sb=new StringBuilder(); for( int i=0; i&lt;repositories.length ; i++ ) &#123; sb.append( repositories[i]).append(":"); &#125; return sb.toString(); &#125; @Override public boolean modified() &#123; return classLoader != null ? classLoader.modified() : false ; &#125; 8.2.4 组件周期方法start过程： 创建类加载器 设置仓库 将WEB-INF/classes目录传入类加载器的addRepository中 将WEB-INF/lib目录传入类加载器的setJarPath中 设置类路径 设置访问权限 设置访问目录的权限 启动类加载器 注册组件 设置状态 123456789101112131415161718192021222324252627282930313233343536373839404142@Overrideprotected void startInternal() throws LifecycleException &#123; if (context.getResources() == null) &#123; log.info("No resources for " + context); setState(LifecycleState.STARTING); return; &#125; // Construct a class loader based on our current repositories list try &#123; classLoader = createClassLoader(); classLoader.setResources(context.getResources()); classLoader.setDelegate(this.delegate); // Configure our repositories setClassPath(); setPermissions(); ((Lifecycle) classLoader).start(); String contextName = context.getName(); if (!contextName.startsWith("/")) &#123; contextName = "/" + contextName; &#125; ObjectName cloname = new ObjectName(context.getDomain() + ":type=" + classLoader.getClass().getSimpleName() + ",host=" + context.getParent().getName() + ",context=" + contextName); Registry.getRegistry(null, null) .registerComponent(classLoader, cloname, null); &#125; catch (Throwable t) &#123; t = ExceptionUtils.unwrapInvocationTargetException(t); ExceptionUtils.handleThrowable(t); log.error( "LifecycleException ", t ); throw new LifecycleException("start: ", t); &#125; setState(LifecycleState.STARTING);&#125; stop过程： 设置状态 停止并销毁类加载器 注销组件 12345678910111213141516171819202122232425262728@Overrideprotected void stopInternal() throws LifecycleException &#123; setState(LifecycleState.STOPPING); // Remove context attributes as appropriate ServletContext servletContext = context.getServletContext(); servletContext.removeAttribute(Globals.CLASS_PATH_ATTR); if (classLoader != null) &#123; try &#123; classLoader.stop(); &#125; finally &#123; classLoader.destroy(); &#125; // classLoader must be non-null to have been registered try &#123; //.... Registry.getRegistry(null, null).unregisterComponent(cloname); &#125; catch (Exception e) &#123; log.error("LifecycleException ", e); &#125; &#125; classLoader = null;&#125; 这里有个注册操作：Registry.getRegistry.unregisterComponent和Registry.getRegistry.registerComponent 8.2.5 classpath解析和classLoader创建classLoader创建通过loaderClass（String）反射创建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102 /** * Create associated classLoader. */ private WebappClassLoaderBase createClassLoader() throws Exception &#123; Class&lt;?&gt; clazz = Class.forName(loaderClass); WebappClassLoaderBase classLoader = null; if (parentClassLoader == null) &#123; parentClassLoader = context.getParentClassLoader(); &#125; Class&lt;?&gt;[] argTypes = &#123; ClassLoader.class &#125;; Object[] args = &#123; parentClassLoader &#125;; Constructor&lt;?&gt; constr = clazz.getConstructor(argTypes); classLoader = (WebappClassLoaderBase) constr.newInstance(args); return classLoader; &#125; /** * Set the appropriate context attribute for our class path. This * is required only because Jasper depends on it. */ private void setClassPath() &#123; // Validate our current state information if (context == null) return; ServletContext servletContext = context.getServletContext(); if (servletContext == null) return; StringBuilder classpath = new StringBuilder(); // Assemble the class path information from our class loader chain ClassLoader loader = getClassLoader(); if (delegate &amp;&amp; loader != null) &#123; // Skip the webapp loader for now as delegation is enabled loader = loader.getParent(); &#125; while (loader != null) &#123; if (!buildClassPath(classpath, loader)) &#123; break; &#125; loader = loader.getParent(); &#125; if (delegate) &#123; // Delegation was enabled, go back and add the webapp paths loader = getClassLoader(); if (loader != null) &#123; buildClassPath(classpath, loader); &#125; &#125; this.classpath = classpath.toString(); // Store the assembled class path as a servlet context attribute servletContext.setAttribute(Globals.CLASS_PATH_ATTR, this.classpath); &#125; private boolean buildClassPath(StringBuilder classpath, ClassLoader loader) &#123; if (loader instanceof URLClassLoader) &#123; URL repositories[] = ((URLClassLoader) loader).getURLs(); for (int i = 0; i &lt; repositories.length; i++) &#123; String repository = repositories[i].toString(); if (repository.startsWith("file://")) repository = utf8Decode(repository.substring(7)); else if (repository.startsWith("file:")) repository = utf8Decode(repository.substring(5)); else continue; if (repository == null) continue; if (classpath.length() &gt; 0) classpath.append(File.pathSeparator); classpath.append(repository); &#125; &#125; else if (loader == ClassLoader.getSystemClassLoader())&#123; // Java 9 onwards. The internal class loaders no longer extend // URLCLassLoader String cp = System.getProperty("java.class.path"); if (cp != null &amp;&amp; cp.length() &gt; 0) &#123; if (classpath.length() &gt; 0) &#123; classpath.append(File.pathSeparator); &#125; classpath.append(cp); &#125; return false; &#125; else &#123; log.info( "Unknown loader " + loader + " " + loader.getClass()); return false; &#125; return true; &#125; private static final Log log = LogFactory.getLog(WebappLoader.class);&#125; 8.2.6 PropertyChangeXXX模型1234567891011121314151617181920212223242526272829303132333435363738 protected final PropertyChangeSupport support = new PropertyChangeSupport(this); @Override public void addPropertyChangeListener(PropertyChangeListener listener) &#123; support.addPropertyChangeListener(listener); &#125; @Override public void removePropertyChangeListener(PropertyChangeListener listener) &#123; support.removePropertyChangeListener(listener); &#125;支持PropertyChange的模型基本都是这样，如下：​``` java public class MyBean &#123; private final PropertyChangeSupport pcs = new PropertyChangeSupport(this); public void addPropertyChangeListener(PropertyChangeListener listener) &#123; this.pcs.addPropertyChangeListener(listener); &#125; public void removePropertyChangeListener(PropertyChangeListener listener) &#123; this.pcs.removePropertyChangeListener(listener); &#125; private String value; public String getValue() &#123; return this.value; &#125; public void setValue(String newValue) &#123; String oldValue = this.value; this.value = newValue; this.pcs.firePropertyChange("value", oldValue, newValue); &#125; [...] &#125; 由于该类实现了PropertyChangeListener，主要是监听Context的属性变化123456789101112131415161718@Overridepublic void propertyChange(PropertyChangeEvent event) &#123; // Validate the source of this event if (!(event.getSource() instanceof Context)) return; // Process a relevant property change if (event.getPropertyName().equals("reloadable")) &#123; try &#123; setReloadable ( ((Boolean) event.getNewValue()).booleanValue() ); &#125; catch (NumberFormatException e) &#123; log.error(sm.getString("webappLoader.reloadable", event.getNewValue().toString())); &#125; &#125;&#125; 8.2.7 JmxEnabled接口实现该接口实现的是注册的功能，暂且不展开 可以结合父类LifecycleMBeanBase和WebappLoader简单了解 8.3 类加载器 WebappClassLoaderBase12public abstract class WebappClassLoaderBase extends URLClassLoader implements Lifecycle, InstrumentableClassLoader, WebappProperties, PermissionCheck WebappClassLoaderBase继承自jdk的URLClassLoader 默认情况下，此类加载器遵循规范要求的委派模型。 将先查询系统类加载器，然后查询本地存储库，然后才能委托给父类加载器查询。 这允许Web应用程序覆盖除J2SE以外的任何共享类。 J2SE是系统类加载器加载的，Web应用程序能覆盖父加载器加载的 对从未从webapp存储库加载的JAXP XML解析器接口，JNDI接口和servlet API的类提供特殊处理 delegate属性修改查询本地存储库和委托给父类加载器的顺序。 由于Jasper编译技术的限制，任何包含servlet API类的存储库都将被类加载器忽略。 类加载器生成源URL，其中包含从JAR文件加载类时的完整JAR URL，即使在JAR中包含类时也允许在类级别设置安全权限。 本地存储库按照通过初始构造函数添加的顺序进行搜索。 不进行安全校验除非设置了安全管理器 8.3.1 重写父类的loadClass加载具有指定名称的类，使用以下算法进行搜索，直到找到并返回类。 如果找不到类，返回ClassNotFoundException。 调用findLoadedClass（String）来检查是否已经加载了该类。 如果有，则返回相同的Class对象。 如果委托属性设置为true，请调用父类加载器的loadClass（）方法（如果有）。 调用findClass（）在本地定义的存储库中查找此类。 调用我们的父类加载器的loadClass（）方法（如果有的话）。 如果使用上述步骤找到类，并且resolve标志为true，则该方法将在生成的Class对象上调用resolveClass（Class）。 resolveClass :链接指定的类。 这个（误导性的）方法可以被类加载器用来链接一个类。 如果类c已经被链接，那么这个方法只是返回。 否则，类将按照Java™语言规范的“执行”一章所述进行链接。resolveClass 应该是和findLoadedClass相对应的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143public Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; if (log.isDebugEnabled()) log.debug("loadClass(" + name + ", " + resolve + ")"); Class&lt;?&gt; clazz = null; // Log access to stopped class loader checkStateForClassLoading(name); // (0) Check our previously loaded local class cache clazz = findLoadedClass0(name); if (clazz != null) &#123; if (log.isDebugEnabled()) log.debug(" Returning class from cache"); if (resolve) resolveClass(clazz); return (clazz); &#125; // (0.1) Check our previously loaded class cache clazz = findLoadedClass(name); if (clazz != null) &#123; if (log.isDebugEnabled()) log.debug(" Returning class from cache"); if (resolve) resolveClass(clazz); return (clazz); &#125; // (0.2) Try loading the class with the system class loader, to prevent // the webapp from overriding Java SE classes. This implements // SRV.10.7.2 String resourceName = binaryNameToPath(name, false); ClassLoader javaseLoader = getJavaseClassLoader(); boolean tryLoadingFromJavaseLoader; try &#123; // Use getResource as it won't trigger an expensive // ClassNotFoundException if the resource is not available from // the Java SE class loader. However (see // https://bz.apache.org/bugzilla/show_bug.cgi?id=58125 for // details) when running under a security manager in rare cases // this call may trigger a ClassCircularityError. // See https://bz.apache.org/bugzilla/show_bug.cgi?id=61424 for // details of how this may trigger a StackOverflowError // Given these reported errors, catch Throwable to ensure any // other edge cases are also caught tryLoadingFromJavaseLoader = (javaseLoader.getResource(resourceName) != null); &#125; catch (Throwable t) &#123; // Swallow all exceptions apart from those that must be re-thrown ExceptionUtils.handleThrowable(t); // The getResource() trick won't work for this class. We have to // try loading it directly and accept that we might get a // ClassNotFoundException. tryLoadingFromJavaseLoader = true; &#125; if (tryLoadingFromJavaseLoader) &#123; try &#123; clazz = javaseLoader.loadClass(name); if (clazz != null) &#123; if (resolve) resolveClass(clazz); return (clazz); &#125; &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; &#125; // (0.5) Permission to access this class when using a SecurityManager if (securityManager != null) &#123; int i = name.lastIndexOf('.'); if (i &gt;= 0) &#123; try &#123; securityManager.checkPackageAccess(name.substring(0,i)); &#125; catch (SecurityException se) &#123; String error = "Security Violation, attempt to use " + "Restricted Class: " + name; log.info(error, se); throw new ClassNotFoundException(error, se); &#125; &#125; &#125; boolean delegateLoad = delegate || filter(name, true); // (1) Delegate to our parent if requested if (delegateLoad) &#123; if (log.isDebugEnabled()) log.debug(" Delegating to parent classloader1 " + parent); try &#123; clazz = Class.forName(name, false, parent); if (clazz != null) &#123; if (log.isDebugEnabled()) log.debug(" Loading class from parent"); if (resolve) resolveClass(clazz); return (clazz); &#125; &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; &#125; // (2) Search local repositories if (log.isDebugEnabled()) log.debug(" Searching local repositories"); try &#123; clazz = findClass(name); if (clazz != null) &#123; if (log.isDebugEnabled()) log.debug(" Loading class from local repository"); if (resolve) resolveClass(clazz); return (clazz); &#125; &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; // (3) Delegate to parent unconditionally if (!delegateLoad) &#123; if (log.isDebugEnabled()) log.debug(" Delegating to parent classloader at end: " + parent); try &#123; clazz = Class.forName(name, false, parent); if (clazz != null) &#123; if (log.isDebugEnabled()) log.debug(" Loading class from parent"); if (resolve) resolveClass(clazz); return (clazz); &#125; &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; &#125; &#125; throw new ClassNotFoundException(name);&#125; 同理，getResourceAsStream和getResource也是这种顺序 findClass和loadClass区别 loadClass是public，findClass是private的 loadClass被其他类用来加载类，而findClass一般用来被loadClass调用查找该classLoader解析类的方法 在ClassLoader的抽象类中 1234567891011121314151617181920212223242526272829303132333435363738&gt; protected Class&lt;?&gt; loadClass(String name, boolean resolve)&gt; throws ClassNotFoundException&gt; &#123;&gt; synchronized (getClassLoadingLock(name)) &#123;&gt; // First, check if the class has already been loaded&gt; Class&lt;?&gt; c = findLoadedClass(name);&gt; if (c == null) &#123;&gt; long t0 = System.nanoTime();&gt; try &#123;&gt; if (parent != null) &#123;&gt; c = parent.loadClass(name, false);&gt; &#125; else &#123;&gt; c = findBootstrapClassOrNull(name);&gt; &#125;&gt; &#125; catch (ClassNotFoundException e) &#123;&gt; // ClassNotFoundException thrown if class not found&gt; // from the non-null parent class loader&gt; &#125;&gt;&gt; if (c == null) &#123;&gt; // If still not found, then invoke findClass in order&gt; // to find the class.&gt; long t1 = System.nanoTime();&gt; c = findClass(name);&gt;&gt; // this is the defining class loader; record the stats&gt; sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);&gt; sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);&gt; sun.misc.PerfCounter.getFindClasses().increment();&gt; &#125;&gt; &#125;&gt; if (resolve) &#123;&gt; resolveClass(c);&gt; &#125;&gt; return c;&gt; &#125;&gt; &#125;&gt; Invoke findLoadedClass(String) to check if the class has already been loaded. Invoke the loadClass method on the parent class loader. If the parent is null the class loader built-in to the virtual machine is used, instead. Invoke the findClass(String) method to find the class. 所以我们一般只需要重写 findClass(String) 即可 8.3.2 启动方法start()将/WEB-INF/classes和/WEB-INF/lib作为两个本地厂库 12345678910111213141516171819public void start() throws LifecycleException &#123; state = LifecycleState.STARTING_PREP; WebResource classes = resources.getResource("/WEB-INF/classes"); if (classes.isDirectory() &amp;&amp; classes.canRead()) &#123; localRepositories.add(classes.getURL()); &#125; WebResource[] jars = resources.listResources("/WEB-INF/lib"); for (WebResource jar : jars) &#123; if (jar.getName().endsWith(".jar") &amp;&amp; jar.isFile() &amp;&amp; jar.canRead()) &#123; localRepositories.add(jar.getURL()); jarModificationTimes.put( jar.getName(), Long.valueOf(jar.getLastModified())); &#125; &#125; state = LifecycleState.STARTED;&#125;]]></content>
      <categories>
        <category>java框架</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>classLoader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算的三种服务模式]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E4%BA%91%E8%AE%A1%E7%AE%97%2F%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E4%B8%89%E7%A7%8D%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%BC%8F%EF%BC%9AIaaS%EF%BC%8CPaaS%E5%92%8CSaaS%2F</url>
    <content type="text"><![CDATA[云计算的三种服务模式：IaaS，PaaS和SaaS任何一个在互联网上提供其服务的公司都可以叫做云计算公司。其实云计算分几层的，分别是Infrastructure（基础设施）-as-a-Service，Platform（平台）-as-a-Service，Software（软件）-as-a-Service。基础设施在最下端，平台在中间，软件在顶端。别的一些“软”的层可以在这些层上面添加。 IaaS: Infrastructure-as-a-Service（基础设施即服务） 第一层叫做IaaS，有时候也叫做Hardware-as-a-Service，几年前如果你想在办公室或者公司的网站上运行一些企业应用，你需要去买服务器，或者别的高昂的硬件来控制本地应用，让你的业务运行起来。 但是现在有IaaS，你可以将硬件外包到别的地方去。IaaS公司会提供场外服务器，存储和网络硬件，你可以租用。节省了维护成本和办公场地，公司可以在任何时候利用这些硬件来运行其应用。 一些大的IaaS公司包括Amazon,Microsoft, VMWare, Rackspace和Red Hat.不过这些公司又都有自己的专长，比如Amazon和微软给你提供的不只是IaaS，他们还会将其计算能力出租给你来host你的网站。 PaaS: Platform-as-a-Service（平台即服务） 第二层就是所谓的PaaS，某些时候也叫做中间件。你公司所有的开发都可以在这一层进行，节省了时间和资源。 PaaS公司在网上提供各种开发和分发应用的解决方案，比如虚拟服务器和操作系统。这节省了你在硬件上的费用，也让分散的工作室之间的合作变得更加容易。网页应用管理，应用设计，应用虚拟主机，存储，安全以及应用开发协作工具等。 一些大的PaaS提供者有Google AppEngine,Microsoft Azure，Force.com,Heroku，Engine Yard。最近兴起的公司有AppFog, Mendix 和 Standing Cloud SaaS: Software-as-a-Service（软件即服务） 第三层也就是所谓SaaS。这一层是和你的生活每天接触的一层，大多是通过网页浏览器来接入。任何一个远程服务器上的应用都可以通过网络来运行，就是SaaS了。 你消费的服务完全是从网页如Netflix, MOG, Google Apps, Box.net, Dropbox或者苹果的iCloud那里进入这些分类。尽管这些网页服务是用作商务和娱乐或者两者都有，但这也算是云技术的一部分。 一些用作商务的SaaS应用包括Citrix的GoToMeeting，Cisco的WebEx，Salesforce的CRM，ADP，Workday和SuccessFactors。 Iaas和Paas之间的比较​ PaaS的主要作用是将一个开发和运行平台作为服务提供给用户，而IaaS的主要作用是提供虚拟机或者其他资源作为服务提供给用户。接下来，将在七个方面对PaaS和IaaS进行比较： ​ 1) 开发环境：PaaS基本都会给开发者提供一整套包括IDE在内的开发和测试环境，而IaaS方面用户主要还是沿用之前比较熟悉那套开发环境，但是因为之前那套开发环境在和云的整合方面比较欠缺，所以使用起来不是很方便。​ 2) 支持的应用：因为IaaS主要是提供虚拟机，而且普通的虚拟机能支持多种操作系统，所以IaaS支持的应用的范围是非常广泛的。但如果要让一个应用能跑在某个PaaS平台不是一件轻松的事，因为不仅需要确保这个应用是基于这个平台所支持的语言，而且也要确保这个应用只能调用这个平台所支持的API，如果这个应用调用了平台所不支持的API，那么就需要对这个应用进行修改。 3) 开放标准：虽然很多IaaS平台都存在一定的私有功能，但是由于OVF等协议的存在，使得IaaS在跨平台和避免被供应商锁定这两面是稳步前进的。而PaaS平台的情况则不容乐观，因为不论是Google的App Engine，还是Salesforce的Force.com都存在一定的私有API。​ 4) 可伸缩性：PaaS平台会自动调整资源来帮助运行于其上的应用更好地应对突发流量。而IaaS平台则需要开发人员手动对资源进行调整才能应对。​ 5) 整合率和经济性： PaaS平台整合率是非常高，比如PaaS的代表GoogleApp Engine能在一台服务器上承载成千上万的应用，而普通的IaaS平台的整合率最多也不会超过100，而且普遍在10左右，使得IaaS的经济性不如PaaS。​ 6) 计费和监管：因为PaaS平台在计费和监管这两方面不仅达到了IaaS平台所能企及的操作系统层面，比如，CPU和内存的使用量等，而且还能做到应用层面，比如，应用的反应时间（Response Time）或者应用所消耗的事务多少等，这将提高计费和管理的精确性。​ 7) 学习难度：因为在IaaS上面开发和管理应用和现有的方式比较接近，而PaaS上面开发则有可能需要学一门新的语言或者新的框架，所以IaaS学习难度更低。 PaaS IaaS 开发环境 完善 普通 支持的应用 有限 广 通用性 欠缺 稍好 可伸缩性 自动伸缩 手动伸缩 整合率和经济性 高整合率，更经济 低整合率 计费和监管 精细 简单 学习难度 略难 低 未来的PK​ 在当今云计算环境当中，IaaS是非常主流的，无论是Amazon EC2还是Linode或者Joyent等，都占有一席之地，但是随着Google的App Engine，Salesforce的Force.com还是微软的Windows Azure等PaaS平台的推出，使得PaaS也开始崭露头角。谈到这两者的未来，特别是这两者之间的竞争关系，我个人认为，短期而言，因为IaaS模式在支持的应用和学习难度这两方面的优势，使得IaaS将会在短期之内会成为开发者的首选，但是从长期而言，因为PaaS模式的高整合率所带来经济型使得如果PaaS能解决诸如通用性和支持的应用等方面的挑战，它将会替代IaaS成为开发者的“新宠”。]]></content>
      <categories>
        <category>大数据</category>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce组件]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fmapreduce%2Fmr%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[组件inputformat在MR程序的开发过程中，经常会遇到输入数据不是HDFS或者数据输出目的地不是HDFS的，MapReduce的设计已经考虑到这种情况，它为我们提供了两个组建，只需要我们自定义适合的InputFormat和OutputFormat，就可以完成这个需求 MapReduce中Map阶段的数据输入是由InputFormat决定的，我们可以看到除了实现InputFormat抽象类以外，我们还需要自定义InputSplit和自定义RecordReader类，这两个类的主要作用分别是：split确定数据分片的大小以及数据的位置信息，recordReader具体的读取数据。 1234public abstract class InputFormat&lt;K, V&gt; &#123; public abstract List&lt;InputSplit&gt; getSplits(JobContext context) throws IOException, InterruptedException; // 获取Map阶段的数据分片集合信息 public abstract RecordReader&lt;K,V&gt; createRecordReader(InputSplit split, TaskAttemptContext context） throws IOException, InterruptedException; // 创建具体的数据读取对象&#125; 自定义InputSplit 1234public abstract class InputSplit &#123; public abstract long getLength() throws IOException, InterruptedException; // 获取当前分片的长度大小 public abstract String[] getLocations() throws IOException, InterruptedException; // 获取当前分片的位置信息 &#125; 自定义RecordReader 12345678public abstract class RecordReader&lt;KEYIN, VALUEIN&gt; implements Closeable &#123; public abstract void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException; // 初始化，如果在构造函数中初始化了，那么该方法可以为空 public abstract boolean nextKeyValue() throws IOException, InterruptedException; //是否存在下一个key/value，如果存在返回true。否则返回false。 public abstract KEYIN getCurrentKey() throws IOException, InterruptedException; // 获取当然key public abstract VALUEIN getCurrentValue() throws IOException, InterruptedException; // 获取当然value public abstract float getProgress() throws IOException, InterruptedException; // 获取进度信息 public abstract void close() throws IOException; // 关闭资源&#125; RecordReadermapper每个节点计算mapper后汇总到reduce CombinerCombiner 是 MapReduce 程序中 Mapper 和 Reducer 之外的一种组件，它的作用是在 maptask 之后给 maptask 的结果进行局部汇总，以减轻 reducetask 的计算负载，减少网络传输 如何使用combiner Combiner 和 Reducer 一样，编写一个类，然后继承 Reducer， reduce 方法中写具体的 Combiner 逻辑，然后在 job 中设置 Combiner 类： job.setCombinerClass(FlowSumCombine.class) 使用combiner注意事项 Combiner 和 Reducer 的区别在于运行的位置： Combiner 是在每一个 maptask 所在的节点运行 Reducer 是接收全局所有 Mapper 的输出结果 Combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来 Combiner 的使用要非常谨慎，因为 Combiner 在 MapReduce 过程中可能调用也可能不调用，可能调一次也可能调多次，所以： Combiner 使用的原则是：有或没有都不能影响业务逻辑，都不能影响最终结果(求平均值时，combiner和reduce逻辑不一样) Sort自定义的 bean 来封装信息，并将 bean 作为 map 输出的 key 来传输 MR 程序在处理数据的过程中会对数据排序(map 输出的 kv 对传输到 reduce 之前，会排序)， 排序的依据是 map 输出的 key， 所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到 key 中，让 key 实现接口： WritableComparable， 然后重写 key 的 compareTo 方法 PartitionerMapReduce 中会将 map 输出的 kv 对，按照相同 key 分组，然后分发给不同的 reducetask默认的分发规则为：根据 key 的 hashcode%reducetask 数来分发 如果reduceTask的数量&gt;= getPartition的结果数 ，则会多产生几个空的输出文件part-r-000xx 如果 1&lt;reduceTask的数量&lt;getPartition的结果数 ，则有一部分分区数据无处安放，会Exception！！！ 如果 reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000 GroupingComparatorGroupingComparator是在reduce的 secondary sort阶段分组来使用的，GroupComparator是如何对进入reduce函数中的key Iterable&lt;value&gt; 进行影响。可以在自定义的GroupComparator 中确定哪儿些value组成一组(key不同)，进入一个reduce函数 我们需要理清楚的还有map阶段你的几个自定义： parttioner中的getPartition()这个是map阶段自定义分区， bean中定义CopmareTo()是在溢出和merge时用来来排序的。 reduce OutputFormatMapReduce中Reducer阶段的数据输出是由OutputFormat决定的，决定数据的输出目的地和job的提交对象 12345public abstract class OutputFormat&lt;K, V&gt; &#123; public abstract RecordWriter&lt;K, V&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException; // 获取具体的数据写出对象 public abstract void checkOutputSpecs(JobContext context) throws IOException, InterruptedException; // 检查输出配置信息是否正确 public abstract OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException; // 获取输出job的提交者对象&#125; 序列化Java 的序列化是一个重量级序列化框架（ Serializable），一个对象被序列化后，会附带很多额 外的信息（各种校验信息， header，继承体系等），不便于在网络中高效传输；所以， hadoop 自己开发了一套序列化机制（ Writable），精简，高效Hadoop 中的序列化框架已经对基本类型和 null 提供了序列化的实现了。分别是： 自定义对象实现mapreduce框架的序列化 如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 mapreduce框中的 shuffle 过程一定会对 key 进行排序，此时，自定义的 bean 实现的接口应该是：public class FlowBean implements WritableComparable&lt;FlowBean&gt; 12345678910111213141516public class StudentWritable implements WritableComparable&lt;StudentWritable&gt; &#123; private String name; private int age; public void write(DataOutput out) throws IOException &#123; out.writeUTF(this.name); out.writeInt(this.age); &#125; //注意顺序是一样的 public void readFields(DataInput in) throws IOException &#123; this.name = in.readUTF(); this.age = in.readInt(); &#125; public int compareTo(StudentWritable o) &#123; return 0; &#125;&#125; hadoop序列化的特点 紧凑：高效实用存储空间 快速：读写数据的额外开销小 可扩展：可透明地读取老格式的数据 互操作：支持多语言的交互 Job 运行原理shuffle机制及map的输出如何汇聚到reduce shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个map task和reduce task节点上完成的，整体来看，分为3个操作： 分区partition Sort根据key排序 Combiner进行局部value的合并 详细流程 maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序 reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快 mapreduce程序的运行流程（经典面试题） 一个 mr 程序启动的时候，最先启动的是 MRAppMaster， MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程 maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为： 利用客户指定的 inputformat 来获取 RecordReader 读取数据，形成输入 KV 对 将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存 将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件 （超过缓存内存写到磁盘临时文件，最后都写到该文件，ruduce 获取该文件后，删除 ） MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据分区） Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新归并排序， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运算输出的结果 KV，然后调用客户指定的 outputformat 将结果数据输出到外部存储 maptask并行度决定机制一个job的map阶段并行度由客户端在提交job时决定 而客户端对map阶段并行度的规划的基本逻辑为：将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理 这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成 splitSize=max{minSize,min{maxSize,blockSize}} 图中有错]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce介绍]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fmapreduce%2Fmapreduce%2F</url>
    <content type="text"><![CDATA[1 MapReduce原理Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架；Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上； 1.1 为什么要MAPREDUCE 海量数据在单机上处理因为硬件资源限制，无法胜任 而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理 单机版：内存受限，磁盘受限，运算能力受限 分布式： 文件分布式存储（HDFS） 运算逻辑需要至少分成2个阶段（一个阶段独立并发，一个阶段汇聚） 运算程序如何分发 程序如何分配运算任务（切片） 两阶段的程序如何启动？如何协调？ 整个程序运行过程中的监控？容错？重试？ mapreduce就是这样一个分布式程序的通用框架，其应对以上问题的整体结构如下： `1. MRAppMaster(mapreduceapplication master) MapTask ReduceTask 1.2 MAPREDUCE框架结构及核心运行机制1.2.1 结构一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster：负责整个程序的过程调度及状态协调 mapTask：负责map阶段的整个数据处理流程 ReduceTask：负责reduce阶段的整个数据处理流程 1.2.2 MR程序运行流程1.2.2.1 流程示意图 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程 maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为： 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件 MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区） Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储 1.3 MapTask并行度决定机制maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度那么，mapTask并行实例是否越多越好呢？其并行度又是如何决定呢？ 1.3.1 mapTask并行度的决定机制一个job的map阶段并行度由客户端在提交job时决定 而客户端对map阶段并行度的规划的基本逻辑为： 将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理 这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图： 1.3.2 FileInputFormat切片机制 切片定义在InputFormat类中的getSplit()方法 FileInputFormat中默认的切片机制： a) 简单地按照文件的内容长度进行切片b) 切片大小，默认等于block大小c) 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 FileInputFormat中切片的大小的参数配置 在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 | minsize：默认值：1 配置参数： mapreduce.input.fileinputformat.split.minsize || —————————————- || maxsize：默认值：Long.MAXValue 配置参数：mapreduce.input.fileinputformat.split.maxsize || blocksize | 选择并发数的影响因素： 运算节点的硬件配置 运算任务的类型：CPU密集型还是IO密集型 运算任务的数据量 1.3.3 map并行度的经验之谈如果硬件配置为2*12core + 64G，恰当的map并行度是大约每个节点20-100个map，最好每个map的执行时间至少一分钟 如果job的每个map或者 reduce task的运行时间都只有30-40秒钟，那么就减少该job的map或者reduce数，每一个task(map|reduce)的setup和加入到调度器中进行调度，这个中间的过程可能都要花费几秒钟 如果input的文件非常的大，比如1TB，可以考虑将hdfs上的每个blocksize设大，比如设成256MB或者512MB 1.4 ReduceTask并行度的决定reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： 12//默认值是1，手动设置为4job.setNumReduceTasks(4); 如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜 注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask 2 MapReduce实践2.1 MAPREDUCE示例编写及编程规范2.1.1 编程规范（1）用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） （3）Mapper的输出数据是KV对的形式（KV的类型可自定义） （4）Mapper中的业务逻辑写在map()方法中 （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 （6）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV （7）Reducer的业务逻辑写在reduce()方法中 （8）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 （9）用户自定义的Mapper和Reducer都要继承各自的父类 （10）整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 2.1.2 wordcount示例编写定义一个mapper类 1234567891011121314151617181920//首先要定义四个泛型的类型//keyin: LongWritable valuein: Text//keyout: Text valueout:IntWritablepublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; //map方法的生命周期： 框架每传一行数据就被调用一次 //key : 这一行的起始点在文件中的偏移量 //value: 这一行的内容 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //拿到一行数据转换为string String line = value.toString(); //将这一行切分出各个单词 String[] words = line.split(" "); //遍历数组，输出&lt;单词，1&gt; for(String word:words)&#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 定义一个reducer类 12345678910111213public class WordCountReduce extends Reduce&lt;Text, IntWritable, Text, IntWritable&gt;&#123;//生命周期：框架每传递进来一个kv 组，reduce方法被调用一次 @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //定义一个计数器 int count = 0; //遍历这一组kv的所有v，累加到count中 for(IntWritable value:values)&#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 定义一个主类，用来描述job并提交job 1234567891011121314151617181920212223242526272829public class WordCountRunner &#123; //把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里……）描述成一个job对象 //把这个描述好的job提交给集群去运行 public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job wcjob = Job.getInstance(conf); //指定我这个job所在的jar包// wcjob.setJar("/home/hadoop/wordcount.jar"); wcjob.setJarByClass(WordCountRunner.class); wcjob.setMapperClass(WordCountMapper.class); wcjob.setReducerClass(WordCountReducer.class); //设置我们的业务逻辑Mapper类的输出key和value的数据类型 wcjob.setMapOutputKeyClass(Text.class); wcjob.setMapOutputValueClass(IntWritable.class); //设置我们的业务逻辑Reducer类的输出key和value的数据类型 wcjob.setOutputKeyClass(Text.class); wcjob.setOutputValueClass(IntWritable.class); //指定要处理的数据所在的位置 FileInputFormat.setInputPaths(wcjob, "hdfs://hdp-server01:9000/wordcount/data/big.txt"); //指定处理完成之后的结果所保存的位置 FileOutputFormat.setOutputPath(wcjob, new Path("hdfs://hdp-server01:9000/wordcount/output/")); //向yarn集群提交这个job boolean res = wcjob.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; 2.2 MapReduce程序运行模式2.2.1 本地运行 mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行 而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上 怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数） 本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可 2.2.2 集群运行模式 将mapreduce程序提交给yarn集群resourcemanager，分发到很多的节点上并发执行 处理的数据和输出结果应该位于hdfs文件系统 提交集群的实现步骤： A. 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动 ​ $ hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriverinputpath outputpath B. 直接在linux的eclipse中运行main方法（项目中要带参数：mapreduce.framework.name=yarn以及yarn的两个基本配置） C. 如果要在windows的eclipse中提交job给集群，则要修改YarnRunner类 mapreduce程序在集群中运行时的大体流程： 2.3 MapReduce中的Combiner combiner是MR程序中Mapper和Reducer之外的一种组件 combiner组件的父类就是Reducer combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行 Reducer是接收全局所有Mapper的输出结果； combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量 具体实现步骤： 自定义一个combiner继承Reducer，重写reduce方法 在job中设置： job.setCombinerClass(CustomCombiner.class) (5) combiner能够应用的前提是不能影响最终的业务逻辑而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来 3 shuffle机制3.1 概述 mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle； shuffle: 洗牌. 发牌——（核心机制：数据分区，排序，缓存）； 具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序； 3.2 主要流程 shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个map task和reduce task节点上完成的，整体来看，分为3个操作： 分区partition Sort根据key排序 Combiner进行局部value的合并 3.3 详细流程 maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序 reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快 缓冲区的大小可以通过参数调整, 参数：io.sort.mb 默认100M 4 MAPREDUCE中的序列化4.1 概述Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息header，继承体系。。。。），不便于在网络中高效传输；所以hadoop自己开发了一套序列化机制（Writable），精简，高效 4.2 Jdk序列化和MR序列化之间的比较一个是readObject和writeObject 一个是自定义流的解析 4.3 自定义对象实现MR中的序列化接口如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序,此时，自定义的bean实现的接口应该是： public class FlowBean implements WritableComparable&lt;FlowBean&gt; 需要自己实现的方法是： 1234567891011121314151617181920212223242526/** * 反序列化的方法，反序列化时，从流中读取到的各个字段的顺序应该与序列化时写出去的顺序保持一致 */@Overridepublic void readFields(DataInput in) throws IOException &#123; upflow = in.readLong(); dflow = in.readLong(); sumflow = in.readLong(); &#125;/** * 序列化的方法 */@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeLong(upflow); out.writeLong(dflow); //可以考虑不序列化总流量，因为总流量是可以通过上行流量和下行流量计算出来的 out.writeLong(sumflow);&#125;@Overridepublic int compareTo(FlowBean o) &#123; //实现按照sumflow的大小倒序排序 return sumflow&gt;o.getSumflow()?-1:1;&#125; 5 MapReduce与YARN5.1 YARN概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序 yarn并不清楚用户提交的程序的运行机制 yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源） yarn中的主管角色叫ResourceManager yarn中具体提供运算资源的角色叫NodeManager 这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce. storm程序，spark程序，tez …… 所以，spark. storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可 Yarn就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享 5.2 Yarn中运行运算程序的示例mapreduce程序的调度过程，如下图 6 Mapreduce中的分区PartitionerMapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask 默认的分发规则为：根据key的hashcode%reducetask数来分发 所以：如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner 自定义一个CustomPartitioner继承抽象类：Partitioner然后在job对象中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class)]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIVE介绍]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhive%2F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1 基本概念1.1 简介Hive是一个数据仓库基础工具在Hadoop中用来处理结构化数据。它架构在Hadoop之上，并使得查询和分析方便。并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能 可扩展 Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。 延展性 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 容错 良好的容错性，节点出现问题SQL仍可完成执行。 1.2 Hive架构下面的组件图描绘了Hive的结构： 该组件图包含不同的单元。下表描述每个单元： 单元名称 操作 用户接口/界面 Hive是一个数据仓库基础工具软件，可以创建用户和HDFS之间互动。用户界面，Hive支持是Hive的Web UI，Hive命令行，HiveHD洞察（在Windows服务器）。 元存储 Hive选择各自的数据库服务器，用以储存表，数据库，列模式或元数据表，它们的数据类型和HDFS映射。 HiveQL处理引擎 HiveQL类似于SQL的查询上Metastore模式信息。这是传统的方式进行MapReduce程序的替代品之一。相反，使用Java编写的MapReduce程序，可以编写为MapReduce工作，并处理它的查询。 执行引擎 HiveQL处理引擎和MapReduce的结合部分是由Hive执行引擎。执行引擎处理查询并产生结果和MapReduce的结果一样。它采用MapReduce方法。 HDFS 或 HBASE Hadoop的分布式文件系统或者HBASE数据存储技术是用于将数据存储到文件系统。 用户接口主要由三个：CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。 元数据存储：Hive 将元数据存储在数据库中。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有 MapReduce 调用执行。 1.3 数据存储 Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等） 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。 Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。 db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在hdfs中表现所属db目录下一个文件夹 external table：外部表, 与table类似，不过其数据存放位置可以在任意指定路径 普通表: 删除表后, hdfs上的文件都删了 External外部表删除后, hdfs上的文件没有删除, 只是把文件删除了 partition：在hdfs中表现为table目录下的子目录 bucket：桶, 在hdfs中表现为同一个表目录下根据hash散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中 1.4 安装https://www.cnblogs.com/hmy-blog/p/6506417.html 2 Hive基本操作2.1 DDL操作2.1.1 建表123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 说明： CREATETABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 LIKE 允许用户复制现有的表结构，但是不复制数据。 1234ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过 SerDe 确定表的具体的列的数据。 STORED AS SEQUENCEFILE|TEXTFILE|RCFILE 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 CLUSTERED BY 对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 把表（或者分区）组织成桶（Bucket）有两个理由： 获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。 使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 2.1.2 修改表 增加/删除分区 12345ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ...partition_spec:: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)ALTER TABLE table_name DROP partition_spec, partition_spec,... 重命名表 1ALTER TABLE table_name RENAME TO new_table_name 增加/更新列 123ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 显示命令 123456show tablesshow databasesshow partitionsshow functionsdesc extended t_name;desc formatted table_name; 2.2 DML操作2.2.1 load12LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 filepath： 相对路径，例如：project/data1 绝对路径，例如：/user/hive/project/data1 包含模式的完整 URI，列如：hdfs://namenode:9000/user/hive/project/data1 LOCAL关键字 如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。 如果没有指定 LOCAL 关键字，则根据inpath中的uri 查找文件 OVERWRITE 关键字 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 2.2.2 insert将查询结果插入Hive表 123456789INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statementMultiple inserts:FROM from_statement INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...Dynamic partition inserts:INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement 2.2.3 导出表数据123456INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...multiple inserts:FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... 2.3 select12345678SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference[WHERE where_condition] [GROUP BY col_list [HAVING condition]] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] ] [LIMIT number] order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。 distribute by根据distribute by指定的内容将数据分到同一个reducer。 Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by 2.3 Join1234join_table: table_reference JOIN table_factor [join_condition] | table_reference &#123;LEFT|RIGHT|FULL&#125; [OUTER] JOIN table_reference join_condition | table_reference LEFT SEMI JOIN table_reference join_condition Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。另外，Hive 支持多于 2 个表的连接 写 join 查询时，需要注意几个关键点： 只支持等值join 1234567SELECT a.* FROMa JOIN b ON (a.id = b.id)SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department =b.department)是正确的，然而:SELECT a.* FROM a JOIN b ON (a.id&gt;b.id)是错误的。 ​ 可以 join 多于 2 个表。 1234567891011例如 SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如： SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。 join 时，每次 map/reduce 任务的逻辑 1234567reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如： SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有： SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。 LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况 1234567891011121314151617SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:a.val, NULL所以 a 表中的所有记录都被保留了；“a RIGHT OUTER JOIN b”会保留所有 b 表的记录。Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况： SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds='2009-07-07' AND b.ds='2009-07-07'会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法： SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07')这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。 3 Hive Shell参数3.1 命令行hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-ffilename&gt;|&lt;-e query-string&gt;][-S] 说明： -i 从文件初始化HQL。 -e从命令行执行指定的HQL -f 执行HQL脚本 -v 输出执行的HQL语句到控制台 -p connect to Hive Server onport number -hiveconf x=y Use this to set hive/hadoopconfiguration variables. 3.2 Hive参数配置方式Hive参数大全： https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties 对于一般参数，有以下三种设定方式： 配置文件 命令行参数 参数声明 配置文件：Hive的配置文件包括 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml 默认配置文件：$HIVE_CONF_DIR/hive-default.xml 用户自定义配置会覆盖默认配置。 另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。 配置文件的设定对本机启动的所有Hive进程都有效。 命令行参数：启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：bin/hive -hiveconf hive.root.logger=INFO,console 参数声明：可以在HQL中使用SET关键字设定参数，例如: set mapred.reduce.tasks=100; ​ 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。 4 Hive函数4.1 内置运算符4.2 内置函数4.3 Hive自定义函数和Transformt]]></content>
      <categories>
        <category>大数据</category>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIVE sql]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhive%2F%E6%95%99%E7%A8%8B-%E6%98%93%E4%BD%B0%2F</url>
    <content type="text"><![CDATA[1 数据类型Hive所有数据类型分为四种类型，给出如下： 列类型 文字 Null 值 复杂类型 1.1 列类型列类型被用作Hive的列数据类型。它们如下： 整型| 类型 | 后缀 | 示例 || ——– | —- | —- || TINYINT | Y | 10Y || SMALLINT | S | 10S || INT | - | 10 || BIGINT | L | 10L | 字符串类型 字符串类型的数据类型可以使用单引号(‘’)或双引号(“”)来指定 | 数据类型 | 长度 || ——- | ———- || VARCHAR | 1 to 65355 || CHAR | 255 | 时间戳 它支持传统的UNIX时间戳可选纳秒的精度。它支持的java.sql.Timestamp格式“YYYY-MM-DD HH:MM:SS.fffffffff”和格式“YYYY-MM-DD HH:MM:ss.ffffffffff”。 日期 DATE值在年/月/日的格式形式描述 NaN. 小数点 12DECIMAL(precision, scale)decimal(10,0) 联合类型 联合是异类的数据类型的集合。可以使用联合创建的一个实例。语法和示例如下： 12345678910UNIONTYPE&lt;int, double, array&lt;string&gt;, struct&lt;a:int,b:string&gt;&gt;&#123;0:1&#125; &#123;1:2.0&#125; &#123;2:[&quot;three&quot;,&quot;four&quot;]&#125; &#123;3:&#123;&quot;a&quot;:5,&quot;b&quot;:&quot;five&quot;&#125;&#125; &#123;2:[&quot;six&quot;,&quot;seven&quot;]&#125; &#123;3:&#123;&quot;a&quot;:8,&quot;b&quot;:&quot;eight&quot;&#125;&#125; &#123;0:9&#125; &#123;1:10.0&#125; 1.2 文字 浮点类型 浮点类型是只不过有小数点的数字。通常，这种类型的数据组成DOUBLE数据类型。 十进制类型 十进制数据类型是只不过浮点值范围比DOUBLE数据类型更大。十进制类型的范围大约是$ -10^{-308}到 10^308$ 1.3 Null 值缺少值通过特殊值 - NULL表示。 1.4 复杂类型 数组 在Hive 数组与在Java中使用的方法相同。 1Syntax: ARRAY&lt;data_type&gt; 映射 映射在Hive类似于Java的映射。 1Syntax: MAP&lt;primitive_type, data_type&gt; 结构体 在Hive结构体类似于使用复杂的数据。 1Syntax: STRUCT&lt;col_name : data_type [COMMENT col_comment], ...&gt; 2 数据库操作Hive是一种数据库技术，可以定义数据库和表来分析结构化数据。 123CREATE DATABASE|SCHEMA [IF NOT EXISTS] &lt;database name&gt;SHOW DATABASES; 12DROP DATABASE StatementDROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; jdbc12345678910111213141516171819public class HiveCreateDb &#123; private static String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver"; public static void main(String[] args) throws SQLException &#123; // Register driver and create driver instance Class.forName(driverName); // get connection Connection con = DriverManager.getConnection("jdbc:hive://localhost:10000/default", "", ""); Statement stmt = con.createStatement(); stmt.executeQuery("CREATE DATABASE userdb"); // stmt.executeQuery("DROP DATABASE userdb"); System.out.println(“Database userdb created successfully.”); con.close(); &#125;&#125; 3 表操作3.1 创建123456CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][ROW FORMAT row_format][STORED AS file_format] 1234567hive&gt; CREATE TABLE IF NOT EXISTS employee ( eid int, name String,&gt; salary String, destination String)&gt; COMMENT 'Employee details'&gt; ROW FORMAT DELIMITED&gt; FIELDS TERMINATED BY '\t'&gt; LINES TERMINATED BY '\n'&gt; STORED AS TEXTFILE; 12345678910111213141516171819202122232425262728public class HiveCreateTable &#123; private static String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver"; public static void main(String[] args) throws SQLException &#123; // Register driver and create driver instance Class.forName(driverName); // get connection Connection con = DriverManager.getConnection("jdbc:hive://localhost:10000/userdb", "", ""); // create statement Statement stmt = con.createStatement(); // execute statement stmt.executeQuery("CREATE TABLE IF NOT EXISTS " +" employee ( eid int, name String, " +" salary String, destignation String)" +" COMMENT ‘Employee details’" +" ROW FORMAT DELIMITED" +" FIELDS TERMINATED BY ‘\t’" +" LINES TERMINATED BY ‘\n’" +" STORED AS TEXTFILE;"); System.out.println(“ Table employee created.”); con.close(); &#125;&#125; LOAD DATA语句 作者：诸葛非卿 Java技术QQ群：227270512 / Linux QQ群：479429477 本章将介绍如何创建一个表以及如何将数据插入。创造表的约定在Hive中非常类似于使用SQL创建表。 CREATE TABLE语句Create Table是用于在Hive中创建表的语句。语法和示例如下： 语法123456CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][ROW FORMAT row_format][STORED AS file_format] 示例假设需要使用CREATE TABLE语句创建一个名为employee表。下表列出了employee表中的字段和数据类型： Sr.No 字段名称 数据类型 1 Eid int 2 Name String 3 Salary Float 4 Designation string 下面的数据是一个注释，行格式字段，如字段终止符，行终止符，并保存的文件类型。 1234COMMENT ‘Employee details’FIELDS TERMINATED BY ‘\t’LINES TERMINATED BY ‘\n’STORED IN TEXT FILE 下面的查询创建使用上述数据的表名为 employee。 1234567hive&gt; CREATE TABLE IF NOT EXISTS employee ( eid int, name String,&gt; salary String, destination String)&gt; COMMENT ‘Employee details’&gt; ROW FORMAT DELIMITED&gt; FIELDS TERMINATED BY ‘\t’&gt; LINES TERMINATED BY ‘\n’&gt; STORED AS TEXTFILE; 如果添加选项IF NOT EXISTS，Hive 忽略大小写，万一表已经存在的声明。 成功创建表后，能看到以下回应： 123OKTime taken: 5.905 secondshive&gt; JDBC 程序以下是使用JDBC程序来创建表给出的一个例子。 12345678910111213141516171819202122232425262728293031323334import java.sql.SQLException;import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import java.sql.DriverManager;public class HiveCreateTable &#123; private static String driverName = &quot;org.apache.hadoop.hive.jdbc.HiveDriver&quot;; public static void main(String[] args) throws SQLException &#123; // Register driver and create driver instance Class.forName(driverName); // get connection Connection con = DriverManager.getConnection(&quot;jdbc:hive://localhost:10000/userdb&quot;, &quot;&quot;, &quot;&quot;); // create statement Statement stmt = con.createStatement(); // execute statement stmt.executeQuery(&quot;CREATE TABLE IF NOT EXISTS &quot; +&quot; employee ( eid int, name String, &quot; +&quot; salary String, destignation String)&quot; +&quot; COMMENT ‘Employee details’&quot; +&quot; ROW FORMAT DELIMITED&quot; +&quot; FIELDS TERMINATED BY ‘\t’&quot; +&quot; LINES TERMINATED BY ‘\n’&quot; +&quot; STORED AS TEXTFILE;&quot;); System.out.println(“ Table employee created.”); con.close(); &#125;&#125; 将该程序保存在一个名为HiveCreateDb.java文件。下面的命令用于编译和执行这个程序。 12$ javac HiveCreateDb.java$ java HiveCreateDb 输出1Table employee created. LOAD DATA语句一般来说，在SQL创建表后，我们就可以使用INSERT语句插入数据。但在Hive中，可以使用LOAD DATA语句插入数据。 同时将数据插入到Hive，最好是使用LOAD DATA来存储大量记录。有两种方法用来加载数据：一种是从本地文件系统，第二种是从Hadoop文件系统。 语法加载数据的语法如下： 12LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOCAL是标识符指定本地路径。它是可选的。 OVERWRITE 是可选的，覆盖表中的数据。 PARTITION 这是可选的 1hive&gt; LOAD DATA LOCAL INPATH &apos;/home/user/sample.txt&apos; 3.2 修改12345ALTER TABLE name RENAME TO new_nameALTER TABLE name ADD COLUMNS (col_spec[, col_spec ...])ALTER TABLE name DROP [COLUMN] column_nameALTER TABLE name CHANGE column_name new_name new_typeALTER TABLE name REPLACE COLUMNS (col_spec[, col_spec ...]) 3.3 删除表1DROP TABLE [IF EXISTS] table_name; 4 HIVE分区Hive组织表到分区。它是将一个表到基于分区列，如日期，城市和部门的值相关方式。使用分区，很容易对数据进行部分查询。 表或分区是细分成桶，以提供额外的结构，可以使用更高效的查询的数据。桶的工作是基于表的一些列的散列函数值。 例如，一个名为Tab1表包含雇员数据，如 id, name, dept 和yoj (即加盟年份)。假设需要检索所有在2012年加入，查询搜索整个表所需的信息员工的详细信息。但是，如果用年份分区雇员数据并将其存储在一个单独的文件，它减少了查询处理时间。 4.1 添加分区12345ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec[LOCATION 'location1'] partition_spec [LOCATION 'location2'] ...;partition_spec:: (p_column = p_col_value, p_column = p_col_value, ...) 举例： 123hive&gt; ALTER TABLE employee&gt; ADD PARTITION (year=’2013’)&gt; location '/2012/part2012'; 4.2 重命名分区1ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec; 举例： 12hive&gt; ALTER TABLE employee PARTITION (year=’1203’) &gt; RENAME TO PARTITION (Yoj=’1203’); 4.3 删除分区1ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec, PARTITION partition_spec,...; 举例： 12hive&gt; ALTER TABLE employee DROP [IF EXISTS] &gt; PARTITION (year=’1203’); 5 内置运算法6 内置函数6.1 一般函数Hive支持以下内置函数： 返回类型 签名 描述 BIGINT round(double a) 返回a最近的BIGINT值。 BIGINT floor(double a) 返回最大BIGINT值等于或小于a。 BIGINT ceil(double a) 它返回最小BIGINT值等于或大于a。 double rand(), rand(int seed) 它返回一个随机数 string concat(string A, string B,…) 它返回从A后串联B产生的字符串 string substr(string A, int start) 它返回一个起始，从起始位置的子字符串，直到A.结束 string substr(string A, int start, int length) 返回从给定长度的起始start位置开始的字符串。 string upper(string A) 它返回从转换的所有字符为大写产生的字符串。 string ucase(string A) 和上面的一样 string lower(string A) 它返回转换B的所有字符为小写产生的字符串。 string lcase(string A) 和上面的一样 string trim(string A) 它返回字符串从A.两端修剪空格的结果 string ltrim(string A) 它返回A从一开始修整空格产生的字符串(左手侧) string rtrim(string A) rtrim(string A)，它返回A从结束修整空格产生的字符串(右侧) string regexp_replace(string A, string B, string C) 它返回从替换所有子在B结果配合C.在Java正则表达式语法的字符串 int size(Map&lt;K.V&gt;) 它返回在映射类型的元素的数量。 int size(Array) 它返回在数组类型元素的数量。 value of cast( as ) 它把表达式的结果expr&lt;类型&gt;如cast(‘1’作为BIGINT)代表整体转换为字符串’1’。如果转换不成功，返回的是NULL。 string from_unixtime(int unixtime) 转换的秒数从Unix纪元(1970-01-0100:00:00 UTC)代表那一刻，在当前系统时区的时间戳字符的串格式：”1970-01-01 00:00:00” string to_date(string timestamp) 返回一个字符串时间戳的日期部分：to_date(“1970-01-01 00:00:00”) = “1970-01-01” int year(string date) 返回年份部分的日期或时间戳字符串：year(“1970-01-01 00:00:00”) = 1970, year(“1970-01-01”) = 1970 int month(string date) 返回日期或时间戳记字符串月份部分：month(“1970-11-01 00:00:00”) = 11, month(“1970-11-01”) = 11 int day(string date) 返回日期或时间戳记字符串当天部分：day(“1970-11-01 00:00:00”) = 1, day(“1970-11-01”) = 1 string get_json_object(string json_string, string path) 提取从基于指定的JSON路径的JSON字符串JSON对象，并返回提取的JSON字符串的JSON对象。如果输入的JSON字符串无效，返回NULL。 示例以下查询演示了一些内置函数： round() 函数 1hive&gt; SELECT round(2.6); 成功执行的查询，能看到以下回应： 13 floor() 函数 1hive&gt; SELECT floor(2.6) ; 成功执行的查询，能看到以下回应： 12 6.2 聚合函数Hive支持以下内置聚合函数。这些函数的用法类似于SQL聚合函数。 返回类型 签名 描述 BIGINT count(*), count(expr), count(*) - 返回检索行的总数。 DOUBLE sum(col), sum(DISTINCT col) 返回该组或该组中的列的不同值的分组和所有元素的总和。 DOUBLE avg(col), avg(DISTINCT col) 返回上述组或该组中的列的不同值的元素的平均值。 DOUBLE min(col) 返回该组中的列的最小值。 DOUBLE max(col) 返回该组中的列的最大值。 7 Hive视图和索引7.1 创建一个视图123CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ][COMMENT table_comment]AS SELECT ... 示例 举个例子来看。假设employee表拥有如下字段：Id, Name, Salary, Designation 和 Dept。生成一个查询检索工资超过30000卢比的员工详细信息，我们把结果存储在一个名为视图 emp_30000. 123456789+------+--------------+-------------+-------------------+--------+| ID | Name | Salary | Designation | Dept |+------+--------------+-------------+-------------------+--------+|1201 | Gopal | 45000 | Technical manager | TP ||1202 | Manisha | 45000 | Proofreader | PR ||1203 | Masthanvali | 40000 | Technical writer | TP ||1204 | Krian | 40000 | Hr Admin | HR ||1205 | Kranthi | 30000 | Op Admin | Admin |+------+--------------+-------------+-------------------+--------+ 下面使用上述业务情景查询检索员的工详细信息： 123hive&gt; CREATE VIEW emp_30000 AS &gt; SELECT * FROM employee &gt; WHERE salary&gt;30000; 7.2 删除一个视图使用下面的语法来删除视图： 1DROP VIEW view_name 7.3 索引创建索引 索引也不过是一个表上的一个特定列的指针。创建索引意味着创建一个表上的一个特定列的指针。它的语法如下： 12345678910111213CREATE INDEX index_nameON TABLE base_table_name (col_name, ...)AS &apos;index.handler.class.name&apos;[WITH DEFERRED REBUILD][IDXPROPERTIES (property_name=property_value, ...)][IN TABLE index_table_name][PARTITIONED BY (col_name, ...)][ [ ROW FORMAT ...] STORED AS ... | STORED BY ...][LOCATION hdfs_path][TBLPROPERTIES (...)] 下面的查询创建一个索引： 1234hive&gt; CREATE INDEX inedx_salary ON TABLE employee(salary) &gt; AS &apos;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&apos;; 这是一个指向salary列。如果列被修改，变更使用的索引值存储。 删除索引 1DROP INDEX &lt;index_name&gt; ON &lt;table_name&gt; 8 HiveQL Select Where1234567SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [HAVING having_condition] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]] [LIMIT number];]]></content>
      <categories>
        <category>大数据</category>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库-事务]]></title>
    <url>%2F2018%2F03%2F15%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95-B%E6%A0%91%2F</url>
    <content type="text"><![CDATA[参考: http://blog.csdn.net/hguisu/article/details/7786014 http://blog.csdn.net/v_JULY_v/article/details/6530142/ http://blog.csdn.net/swordmanwk/article/details/6549480 算法导论 B树B树是为磁盘或其他直接存取的辅助设备而设计的一种平衡搜索树, 类似于红黑树, 但是在降低IO方面做的更好一些 定义B树是一种平衡的多路查找树，它在文件系统和数据库中很有用。 定义： B树内的每一个节点x都具有以下字段： 当前存储在节点x中的关键字（key）个数n[x]。 存储在x节点中的n[x]个关键字是以非降序的顺序排列的，即：key1[x] ≤key2[x]……≤keyn[x][x]。 一个表示x节点否是叶节点的bool值leaf[x]。 每个内节点拥有指向n[x]+1个指向叶节点的指针，c1[x]，c2[x]……cn[x]+1[x]。叶节点没有ci[x]域 节点x的key值，将子节点的key值范围分开了。如果ki是ci[x]指向的子节点的任意一个key的值，那么有：k1 ≤ key1[x] ≤ k2 ≤ key2[x] ≤ ≤ keyn[x][x] ≤ kn[x]+1. 所有的叶节点具有相同的深度，这个深度也就是树高h。 一个节点可容纳的关键字数目是有上下限的。这个界限可以用包含一个大于2的整数t的表达式来表示，这个数t称为B树的最小度。 除根节点外，每个节点至少要包含t-1个关键字（key），每个内节点（除根外）至少要包含t个关键字。一棵非空的B树，根节点至少要包含一个key。每个节点最多包含2t-1个关键字，因此，内节点最多有2t个子节点。对于一个包含2t-1个关键的节点，我们就称这个节点满了。 以子树c为例,n=2,[43,78]为关键码,其他为指向节点的指针 B树的高度h&lt;logt(n+1/2) Node数据结构typedef int KeyType ; #define m 5 /*B 树的阶，暂设为5*/ typedef struct Node{ int keynum; /* 结点中关键码的个数，即结点的大小*/ struct Node *parent; /*指向双亲结点*/ KeyType key[m+1]; /*关键码向量，0 号单元未用*/ struct Node *ptr[m+1]; /*子树指针向量*/ Record *recptr[m+1]; /*记录指针向量*/ }NodeType; /*B 树结点类型*/ 查找显然, B-树也是一种有序的结构,可以类似二叉查找树查找. 一般先在关键字中找, 没有则到子树中找, B-TREE-SEARCH(x, k) //在x中查找k 1 i = 1 2 while i &lt;= x.n and k &gt;= x.keyi //在关键字中查找k 3 i = i + 1 4 if i &lt;= x.n and k == x.keyi //在key[]中 5 return (x,i) 6 elseif x.leaf //key[]中没有且如果是叶节点 7 return NIL 8 else DISK-READ(x.ci) //子节点中找 9 return B-TREE-SEARCH(x.ci, k) 从查找算法中可以看出， 在B-树中进行查找包含两种基本操作: 在B树关键字中查找； 在结点中查找关键字。 由于B树通常存储在磁盘上， 则前一查找操作是在磁盘上进行的， 而后一查找操作是在内存中进行的，即在磁盘上找到指针p 所指结点后， 先将结点中的信息读入内存， 然后再利用顺序查找或折半查找查询等于K 的关键字。显然， 在磁盘上进行一次查找比在内存中进行一次查找的时间消耗多得多. 创建空树B-TREE-CREATE(T) 1 x = ALLOCATE-NODE() 2 x.leaf = TRUE 3 x.n = 0 4 DISK-WRITE(x) 5 T.root = x 分裂分裂是树长高的唯一途径 x是非满的节点, i指向x.ci的节点,该节点为满,满树有2t-1个关键字, 提最中间的关键字, 生成两个节点 插入单行程向下插入, 根满了就分裂, 保证不满,提中间元素为根, 再调用非满的插入函数 非满插入函数插入最终是在叶子节点上 3~7 插入关键字 9~17 尾递归的找到叶节点, 注意路径中非叶节点满则分裂, 保证尾递归的正确, 也是单行程向下插入 插入例子t=3, 关键字[2,5] ,节点[3,6] 删除与插入情况相对称，除了根结点外（根结点个数不能少于1），B树的关键字数不能少于t-1个。对于简单删除情况，如果我们定位到关键字处在某个结点中，如果这个结点中关键字个数恰好是t-1个，如果直接删除这个关键字，就会违反B树规则。 此时，需要考虑两种处理方案： 把这个结点与其相邻结点合并，合并时需要把父结点的一个关键字加进来，除非相邻的那个结点的关键字数也是t-1个，否则，合并后会超出2t-1的限制，同样违反B树规则。而且，因为从父结点拉下一个关键字，导致父结点的关键字数少1，如果原来父结点关键字数是t-1，那么父结点违反B树规则，这种情况下，必须进行回溯处理。（对于下图（a）初始树，删除结点Z就会出现这种情况） 从相邻结点借一个关键字过来，这种情况要求，相邻结点必须有多于t-1个关键字，借的过程中，需要转经父结点，否则违反B树规则。 为了避免回溯，要求我们在从树根向下搜索关键字的过程中，凡是遇到途经的结点，如果该结点的关键字数是t-1，则我们需要想办法从其他地方搞个关键字过来，使得该结点的关键字数至少为t。 搞，也是从相邻结点搞，如果相邻结点有的话，当然，也要经过父结点进行周转。如果没有，就说明相邻结点的关键字个数也是t-1，这种情况，直接对该结点与其相邻结点进行合并，以满足要求。 代码参考http://blog.csdn.net/swordmanwk/article/details/6549480 删除例子t=3, 关键字[2,5] ,节点[3,6] 思考插入路过的每个节点都非满, 删除路过的每个节点都非t-1, 根节点除外 插入非满保证在叶子节点插入时, 如果叶子节点满了, 则向父节点添加一个关键字, 若路过的每个节点都非满可以保证不用向上回溯 删除同理, 如果要删除的点关键子数为 t-1, 只能向兄弟借, 兄弟借不到就要合并点, 合并后父少个节点, 不满足, 回溯的满足父亲, 不是单向的 B+树与B树区别 有n棵子树的结点中含有n个关键字，每个关键字不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树（根结点）中的最大（或最小）关键字。通常在B+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。 B+树与操作系统的文件索引和数据库索引为什么说B+树比B 树更适合实际应用中操作系统的文件索引和数据库索引？ B+树的磁盘读写代价更低: B+树的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B 树更小。 B+树的查询效率更加稳定: 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 B*树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针；B*树定义了非叶子结点关键字个数至少为(2/3)*M，即块的最低使用率为2/3（代替B+树的1/2）； B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； B*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针； 所以，B*树分配新结点的概率比B+树要低，空间使用率更高；]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>B树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库-事务]]></title>
    <url>%2F2018%2F03%2F15%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93-%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[[TOC] 1 原则1.1 ACID 原子性（Atomic） 事务必须是原子工作单元；对于其数据修改，要么全都执行，要么全都不执行。 一致性（Consistent）事务在完成时，必须使所有的数据都保持一致状态。在相关数据库中，所有规则都必须应用于事务的修改，以保持所有数据的完整性。事务结束时，所有的内部数据结构（如 B 树索引或双向链表）都必须是正确的。 隔离性（Isolation）由并发事务所作的修改必须与任何其它并发事务所作的修改隔离。事务查看数据时数据所处的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看中间状态的数据。这称为隔离性，因为它能够重新装载起始数据，并且重播一系列事务，以使数据结束时的状态与原始事务执行的状态相同。 持久性（Durability） 在事务完成以后，该事务对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 1.1.1 概念 脏读：事务 A 读取了事务 B 未提交的数据，并在这个基础上又做了其他操作。 不可重复读：事务 A 读取了事务 B 已提交的更改数据。例如B事务在A事务期间完成, A读取B前后提交的数据会不一致 幻读：事务 A 读取了事务 B 已提交的新增数据。 1.1.2 数据库隔离级别 隔离级别 含义 DEFAULT 使用后端数据库默认的隔离级别 READ_UNCOMMITTED 最低的隔离级别，允许读取尚未提交的数据变更 READ_COMMITTED 允许读取并发事务已经提交的数据 REPEATABLE_READ 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改 SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的 在MySQL中，实现了这四种隔离级别，分别有可能产生问题如下所示| 隔离级别 | 脏读 | 不可重复读取 | 幻读 || :————— | —- | —— | —- || READ UNCOMMITTED | 会 | 会 | 会 || READ COMMITTED | 否 | 会 | 会 || REPEATABLE READ | 否 | 否 | 会 || SERIALIZABLE | 否 | 否 | 否 | MySQL默认采用REPEATABLE_READ隔离级别；Oracle默认采用READ_COMMITTED隔离级别 2 Spring中事务2.1 核心接口Spring事务管理涉及的接口的联系如下： 2.1.1 事务管理器Spring并不直接管理事务，而是提供了多种事务管理器，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。 2.1.2 事务属性的定义事务管理器接口PlatformTransactionManager通过getTransaction(TransactionDefinition definition)方法来得到事务, 事务属性包含了5个方面， 123456public interface TransactionDefinition &#123; int getPropagationBehavior(); // 返回事务的传播行为 int getIsolationLevel(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getTimeout(); // 返回事务必须在多少秒内完成 boolean isReadOnly(); // 事务是否只读，事务管理器能够根据这个返回值进行优化，确保事务是只读的&#125; 传播行为 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。 | 传播行为 | 含义 || ————————- | —————————————- || PROPAGATION_REQUIRED | 表示当前方法必须运行在事务中。如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务 || PROPAGATION_SUPPORTS | 表示当前方法不需要事务上下文，但是如果存在当前事务的话，那么该方法会在这个事务中运行 || PROPAGATION_MANDATORY | 表示该方法必须在事务中运行，如果当前事务不存在，则会抛出一个异常 || PROPAGATION_REQUIRED_NEW | 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager || PROPAGATION_NOT_SUPPORTED | 表示该方法不应该运行在事务中。如果存在当前事务，在该方法运行期间，当前事务将被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager || PROPAGATION_NEVER | 表示当前方法不应该运行在事务上下文中。如果当前正有一个事务在运行，则会抛出异常 || PROPAGATION_NESTED | 表示如果当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。如果当前事务不存在，那么其行为与PROPAGATION_REQUIRED一样。注意各厂商对这种传播行为的支持是有所差异的。可以参考资源管理器的文档来确认它们是否支持嵌套事务 | 只读 如果事务只对后端的数据库进行该操作，数据库可以利用事务的只读特性来进行一些特定的优化。通过将事务设置为只读，你就可以给数据库一个机会，让它应用它认为合适的优化措施。 事务超时 为了使应用程序很好地运行，事务不能运行太长的时间。因为事务可能涉及对后端数据库的锁定，所以长时间的事务会不必要的占用数据库资源。事务超时就是事务的一个定时器，在特定时间内事务如果没有执行完毕，那么就会自动回滚，而不是一直等待其结束。 回滚规则 默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚。但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。 2.1.3事务状态调用PlatformTransactionManager接口的getTransaction()的方法得到的是TransactionStatus接口的一个实现： 1234567public interface TransactionStatus&#123; boolean isNewTransaction(); // 是否是新的事物 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成&#125; 可以发现这个接口描述的是一些处理事务提供简单的控制事务执行和查询事务状态的方法，在回滚或提交的时候需要应用对应的事务状态。 2.2 编程式事务2.2.1 编程式和声明式事务的区别Spring提供了对编程式事务和声明式事务的支持，编程式事务允许用户在代码中精确定义事务的边界，而声明式事务（基于AOP）有助于用户将操作与事务规则进行解耦。简单地说，编程式事务侵入到了业务代码里面，但是提供了更加详细的事务管理；而声明式事务由于基于AOP，所以既能起到事务管理的作用，又可以不影响业务代码的具体实现。 2.2.2 使用TransactionTemplate采用TransactionTemplate和采用其他Spring模板，如JdbcTempalte和HibernateTemplate是一样的方法。它使用回调方法，把应用程序从处理取得和释放资源中解脱出来。如同其他模板，TransactionTemplate是线程安全的。代码片段： 12345678TransactionTemplate tt = new TransactionTemplate(); // 新建一个TransactionTemplate Object result = tt.execute( new TransactionCallback()&#123; public Object doTransaction(TransactionStatus status)&#123; updateOperation(); return resultOfUpdateOperation(); &#125; &#125;); // 执行execute方法进行事务管理 使用TransactionCallback()可以返回一个值。如果使用TransactionCallbackWithoutResult则没有返回值。 2.2.3 使用PlatformTransactionManager123456789101112DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); //定义一个某个框架平台的TransactionManager，如JDBC、HibernatedataSourceTransactionManager.setDataSource(this.getJdbcTemplate().getDataSource()); // 设置数据源DefaultTransactionDefinition transDef = new DefaultTransactionDefinition(); // 定义事务属性transDef.setPropagationBehavior(DefaultTransactionDefinition.PROPAGATION_REQUIRED); // 设置传播行为属性TransactionStatus status = dataSourceTransactionManager.getTransaction(transDef); // 获得事务状态try &#123; // 数据库操作 // .... dataSourceTransactionManager.commit(status);// 提交&#125; catch (Exception e) &#123; dataSourceTransactionManager.rollback(status);// 回滚&#125; 2.3 声明式事务2.3.1 配置方式 根据代理机制的不同，总结了五种Spring事务的配置方式，配置文件如下： （1）每个Bean都有一个代理 1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns=...&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id="userDao" class="org.springframework.transaction.interceptor.TransactionProxyFactoryBean"&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name="transactionManager" ref="transactionManager" /&gt; &lt;property name="target" ref="userDaoTarget" /&gt; &lt;property name="proxyInterfaces" value="com.bluesky.spring.dao.GeneratorDao" /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;prop key="*"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; （2）所有Bean共享一个代理基类 1234567891011121314151617&lt;beans&gt; &lt;bean id="transactionBase" class="org.springframework.transaction.interceptor.TransactionProxyFactoryBean" lazy-init="true" abstract="true"&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name="transactionManager" ref="transactionManager" /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;prop key="*"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="userDao" parent="transactionBase" &gt; &lt;/bean&gt;&lt;/beans&gt; （3）使用拦截器 12345678910111213141516171819202122232425&lt;bean&gt; &lt;bean id="transactionInterceptor" class="org.springframework.transaction.interceptor.TransactionInterceptor"&gt; &lt;property name="transactionManager" ref="transactionManager" /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;prop key="*"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean class="org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator"&gt; &lt;property name="beanNames"&gt; &lt;list&gt; &lt;value&gt;*Dao&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="interceptorNames"&gt; &lt;list&gt; &lt;value&gt;transactionInterceptor&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; （4）使用tx标签配置的拦截器 123456789101112131415&lt;beans&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package="com.bluesky" /&gt; &lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="*" propagation="REQUIRED" /&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id="interceptorPointCuts" expression="execution(* com.bluesky.spring.dao.*.*(..))" /&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="interceptorPointCuts" /&gt; &lt;/aop:config&gt; &lt;/beans&gt; （5）全注解 123456789101112&lt;beans&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package="com.bluesky" /&gt; &lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 此时在DAO上需加上@Transactional注解，如下： 12345678@Transactional@Component("userDao")public class UserDaoImpl extends HibernateDaoSupport implements UserDao &#123; public List&lt;User&gt; listUsers() &#123; return this.getSession().createQuery("from User").list(); &#125; &#125; 或者在方法上 123456789101112131415161718192021222324/** * 1.添加事务注解 * 使用propagation 指定事务的传播行为，即当前的事务方法被另外一个事务方法调用时如何使用事务。 * 默认取值为REQUIRED，即使用调用方法的事务 * REQUIRES_NEW：使用自己的事务，调用的事务方法的事务被挂起。 * * 2.使用isolation 指定事务的隔离级别，最常用的取值为READ_COMMITTED * 3.默认情况下 Spring 的声明式事务对所有的运行时异常进行回滚，也可以通过对应的属性进行设置。通常情况下，默认值即可。 * 4.使用readOnly 指定事务是否为只读。 表示这个事务只读取数据但不更新数据，这样可以帮助数据库引擎优化事务。若真的是一个只读取数据库值得方法，应设置readOnly=true * 5.使用timeOut 指定强制回滚之前事务可以占用的时间。 */ @Transactional(propagation=Propagation.REQUIRES_NEW, isolation=Isolation.READ_COMMITTED, noRollbackFor=&#123;UserAccountException.class&#125;, readOnly=true, timeout=3) @Override public void purchase(String username, String isbn) &#123; //1.获取书的单价 int price = bookShopDao.findBookPriceByIsbn(isbn); //2.更新书的库存 bookShopDao.updateBookStock(isbn); //3.更新用户余额 bookShopDao.updateUserAccount(username, price); &#125;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库-mysql]]></title>
    <url>%2F2018%2F03%2F15%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2Fmysql%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[MySQL运行原理与基础架构细说MySQL逻辑架构 1.最上层是一些客户端和连接服务，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcp/ip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 2.第二层架构主要完成大多少的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。 3.存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。 4.数据存储层，主要是将数据存储在运行于裸设备的文件系统之上，并完成与存储引擎的交互。 2.存储引擎介绍：InnoDB引擎： 1.将数据存储在表空间中，表空间由一系列的数据文件组成，由InnoDB管理； 2.支持每个表的数据和索引存放在单独文件中(innodb_file_per_table)； 3.支持事务，采用MVCC来控制并发，并实现标准的4个事务隔离级别，支持外键； 4.索引基于聚簇索引建立，对于主键查询有较高性能； 5.数据文件的平台无关性，支持数据在不同的架构平台移植； 6.能够通过一些工具支持真正的热备。如XtraBackup等； 7.内部进行自身优化如采取可预测性预读，能够自动在内存中创建hash索引等。 MyISAM引擎： 1.MySQL5.1中默认，不支持事务和行级锁； 2.提供大量特性如全文索引、空间函数、压缩、延迟更新等； 3.数据库故障后，安全恢复性差； 4.对于只读数据可以忍受故障恢复，MyISAM依然非常适用； 5.日志服务器的场景也比较适用，只需插入和数据读取操作； 6.不支持单表一个文件，会将所有的数据和索引内容分别存在两个文件中； 7.MyISAM对整张表加锁而不是对行，所以不适用写操作比较多的场景； 8.支持索引缓存不支持数据缓存。 存储引擎选取参考因素1.是否有事务需求 如果需要事务支持最好选择InnoDB或者XtraDB，如果主要是select和insert操作MyISAM比较合适，一般使用日志型的应用。 2.备份操作需求 如果能够关闭服务器进行备份，那么该因素可以忽略，如果需要在线进行热备份，则InnoDB引擎是一个不错的选择。 3.故障恢复需求 在对恢复要求比较好的场景中推荐使用InnoDB，因为MyISAM数据损坏概率比较大而且恢复速度比较慢。 4.性能上的需求 有些业务需求只有某些特定的存储引擎才能够满足，如地理空间索引也只有MyISAM引擎支持。所以在应用架构需求环境中也需要管理员折衷考虑，当然从各方面比较而言，InnoDB引擎还是默认应该被推荐使用的。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql存储引擎</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce深入]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fmapreduce%2Fmapreduce%20%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[1 MapReduce原理mapreduce&amp;yarn的工作机制 mapReduce原理剖析 Shuffle原理 mapreduce运行全流程 mapTask任务分配机制 2 疑问 map的结果输出在哪里，在hdfs的指定目录下面？如果是，那么目录结构又是什么样子的？又是如何指定的呢？ shuffle的过程是将所有的map结果进行归并和划分，哪里取数据，参考问题1，这个归并过程在reduce的进程里面处理，还是在reduce程序启动前已经处理好了。 如何启动多个map任务，分片？那么如何分，代码都是一样的怎么看出来分了片。在map函数外部分？？inputformat？ 自定义Partitioner 12345678@Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; String prefix = key.toString().substring(0,3); Integer partNum = pmap.get(prefix); return (partNum==null?4:partNum); &#125; ​ map的输出会进行分区，分区是如何做的？分区的数量如何判断？分区和reduce的对应关系又是如何？ 自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task 1job.setNumReduceTasks(5); 注意： 如果reduceTask的数量&gt;= getPartition的结果数 ，则会多产生几个空的输出文件part-r-000xx 如果 1&lt;reduceTask的数量&lt;getPartition的结果数 ，则有一部分分区数据无处安放，会Exception！！！ 如果 reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000 ​]]></content>
      <categories>
        <category>大数据</category>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm介绍]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fstorm%2F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1. 简介Apache Storm是一个分布式实时大数据处理系统。Storm设计用于在容错和水平可扩展方法中处理大量数据。它是一个流数据框架 什么是Apache Storm？Apache Storm是一个分布式实时大数据处理系统。Storm设计用于在容错和水平可扩展方法中处理大量数据。它是一个流数据框架，具有最高的摄取率。虽然Storm是无状态的，它通过Apache ZooKeeper管理分布式环境和集群状态。它很简单，您可以并行地对实时数据执行各种操作。 Apache Storm继续成为实时数据分析的领导者。Storm易于设置和操作，并且它保证每个消息将通过拓扑至少处理一次。 Apache Storm vs Hadoop基本上Hadoop和Storm框架用于分析大数据。两者互补，在某些方面有所不同。Apache Storm执行除持久性之外的所有操作，而Hadoop在所有方面都很好，但滞后于实时计算。下表比较了Storm和Hadoop的属性。 Storm Hadoop 实时流处理 批量处理 无状态 有状态 主/从架构与基于ZooKeeper的协调。主节点称为nimbus，从属节点是主管。 具有/不具有基于ZooKeeper的协调的主 - 从结构。主节点是作业跟踪器，从节点是任务跟踪器。 Storm流过程在集群上每秒可以访问数万条消息。 Hadoop分布式文件系统（HDFS）使用MapReduce框架来处理大量的数据，需要几分钟或几小时。 Storm拓扑运行直到用户关闭或意外的不可恢复故障。 MapReduce作业按顺序执行并最终完成。 两者都是分布式和容错的 如果nimbus / supervisor死机，重新启动使它从它停止的地方继续，因此没有什么受到影响。 如果JobTracker死机，所有正在运行的作业都会丢失。 Apache Storm优势下面是Apache Storm提供的好处列表： Storm是开源的，强大的，用户友好的。它可以用于小公司和大公司。 Storm是容错的，灵活的，可靠的，并且支持任何编程语言。 允许实时流处理。 Storm是令人难以置信的快，因为它具有巨大的处理数据的力量。 Storm可以通过线性增加资源来保持性能，即使在负载增加的情况下。它是高度可扩展的。 Storm在几秒钟或几分钟内执行数据刷新和端到端传送响应取决于问题。它具有非常低的延迟。 Storm有操作智能。 Storm提供保证的数据处理，即使群集中的任何连接的节点死或消息丢失。 2. Apache Storm核心概念Apache Storm从一端读取实时数据的原始流，并将其传递通过一系列小处理单元，并在另一端输出处理/有用的信息。 下图描述了Apache Storm的核心概念。 组件 描述 Tuple Tuple是Storm中的主要数据结构。它是有序元素的列表。默认情况下，Tuple支持所有数据类型。通常，它被建模为一组逗号分隔的值，并传递到Storm集群。 Stream 流是元组的无序序列。 Spouts 流的源。通常，Storm从原始数据源（如Twitter Streaming API，Apache Kafka队列，Kestrel队列等）接受输入数据。否则，您可以编写spouts以从数据源读取数据。“ISpout”是实现spouts的核心接口，一些特定的接口是IRichSpout，BaseRichSpout，KafkaSpout等。 Bolts Bolts是逻辑处理单元。Spouts将数据传递到Bolts和Bolts过程，并产生新的输出流。Bolts可以执行过滤，聚合，加入，与数据源和数据库交互的操作。Bolts接收数据并发射到一个或多个Bolts。 “IBolt”是实现Bolts的核心接口。一些常见的接口是IRichBolt，IBasicBolt等。 拓扑Spouts和Bolts连接在一起，形成拓扑结构。实时应用程序逻辑在Storm拓扑中指定。简单地说，拓扑是有向图，其中顶点是计算，边缘是数据流。 简单拓扑从spouts开始。Spouts将数据发射到一个或多个Bolts。Bolt表示拓扑中具有最小处理逻辑的节点，并且Bolts的输出可以发射到另一个Bolts作为输入。 Storm保持拓扑始终运行，直到您终止拓扑。Apache Storm的主要工作是运行拓扑，并在给定时间运行任意数量的拓扑。 任务现在你有一个关于Spouts和Bolts的基本想法。它们是拓扑的最小逻辑单元，并且使用单个Spout和Bolt阵列构建拓扑。应以特定顺序正确执行它们，以使拓扑成功运行。Storm执行的每个Spout和Bolt称为“任务”。简单来说，任务是Spouts或Bolts的执行。在给定时间，每个Spout和Bolt可以具有在多个单独的螺纹中运行的多个实例。 进程拓扑在多个工作节点上以分布式方式运行。Storm将所有工作节点上的任务均匀分布。工作节点的角色是监听作业，并在新作业到达时启动或停止进程。 流分组数据流从Spouts流到Bolts，或从一个Bolts流到另一个Bolts。流分组控制元组在拓扑中的路由方式，并帮助我们了解拓扑中的元组流。有四个内置分组，如下所述。 随机分组在随机分组中，相等数量的元组随机分布在执行Bolts的所有工人中。下图描述了结构。 字段分组元组中具有相同值的字段组合在一起，其余的元组保存在外部。然后，具有相同字段值的元组被向前发送到执行Bolts的同一进程。例如，如果流由字段“字”分组，则具有相同字符串“Hello”的元组将移动到相同的工作者。下图显示了字段分组的工作原理。 全局分组所有流可以分组并向前到一个Bolts。此分组将源的所有实例生成的元组发送到单个目标实例（具体来说，选择具有最低ID的工作程序）。 所有分组所有分组将每个元组的单个副本发送到接收Bolts的所有实例。这种分组用于向Bolts发送信号。所有分组对于连接操作都很有用。 3. Apache Storm集群架构Apache Storm的主要亮点是，它是一个容错，快速，没有“单点故障”（SPOF）分布式应用程序。我们可以根据需要在多个系统中安装Apache Storm，以增加应用程序的容量。 让我们看看Apache Storm集群如何设计和其内部架构。下图描述了集群设计。 Apache Storm有两种类型的节点，Nimbus（主节点）和Supervisor（工作节点）。Nimbus是Apache Storm的核心组件。Nimbus的主要工作是运行Storm拓扑。Nimbus分析拓扑并收集要执行的任务。然后，它将任务分配给可用的supervisor。 Supervisor将有一个或多个工作进程。Supervisor将任务委派给工作进程。工作进程将根据需要产生尽可能多的执行器并运行任务。Apache Storm使用内部分布式消息传递系统来进行Nimbus和管理程序之间的通信。 组件 描述 Nimbus（主节点） Nimbus是Storm集群的主节点。集群中的所有其他节点称为工作节点。主节点负责在所有工作节点之间分发数据，向工作节点分配任务和监视故障。 Supervisor（工作节点） 遵循指令的节点被称为Supervisors。Supervisor有多个工作进程，它管理工作进程以完成由nimbus分配的任务。 Worker process（工作进程） 工作进程将执行与特定拓扑相关的任务。工作进程不会自己运行任务，而是创建执行器并要求他们执行特定的任务。工作进程将有多个执行器。 Executor（执行者） 执行器只是工作进程产生的单个线程。执行器运行一个或多个任务，但仅用于特定的spout或bolt。 Task（任务） 任务执行实际的数据处理。所以，它是一个spout或bolt。 ZooKeeper framework（ZooKeeper框架） Apache的ZooKeeper的是使用群集（节点组）自己和维护具有强大的同步技术共享数据之间进行协调的服务。Nimbus是无状态的，所以它依赖于ZooKeeper来监视工作节点的状态。ZooKeeper的帮助supervisor与nimbus交互。它负责维持nimbus，supervisor的状态。 4. Apache Storm工作流程一个工作的Storm集群应该有一个Nimbus和一个或多个supervisors。另一个重要的节点是Apache ZooKeeper，它将用于nimbus和supervisors之间的协调。 现在让我们仔细看看Apache Storm的工作流程 − 最初，nimbus将等待“Storm拓扑”提交给它。 一旦提交拓扑，它将处理拓扑并收集要执行的所有任务和任务将被执行的顺序。 然后，nimbus将任务均匀分配给所有可用的supervisors。 在特定的时间间隔，所有supervisor将向nimbus发送心跳以通知它们仍然运行着。 当supervisor终止并且不向心跳发送心跳时，则nimbus将任务分配给另一个supervisor。 当nimbus本身终止时，supervisor将在没有任何问题的情况下对已经分配的任务进行工作。 一旦所有的任务都完成后，supervisor将等待新的任务进去。 同时，终止nimbus将由服务监控工具自动重新启动。 重新启动的网络将从停止的地方继续。同样，终止supervisor也可以自动重新启动。由于网络管理程序和supervisor都可以自动重新启动，并且两者将像以前一样继续，因此Storm保证至少处理所有任务一次。 一旦处理了所有拓扑，则网络管理器等待新的拓扑到达，并且类似地，管理器等待新的任务。 默认情况下，Storm集群中有两种模式： 本地模式 -此模式用于开发，测试和调试，因为它是查看所有拓扑组件协同工作的最简单方法。在这种模式下，我们可以调整参数，使我们能够看到我们的拓扑如何在不同的Storm配置环境中运行。在本地模式下，storm拓扑在本地机器上在单个JVM中运行。 生产模式 -在这种模式下，我们将拓扑提交到工作Storm集群，该集群由许多进程组成，通常运行在不同的机器上。如在storm的工作流中所讨论的，工作集群将无限地运行，直到它被关闭。 5. Apache Storm分布式消息系统Apache Storm处理实时数据，并且输入通常来自消息排队系统。外部分布式消息系统将提供实时计算所需的输入。Spout将从消息系统读取数据，并将其转换为元组并输入到Apache Storm中。有趣的是，Apache Storm在内部使用其自己的分布式消息传递系统，用于其nimbus和主管之间的通信。 什么是分布式消息系统？分布式消息传递基于可靠消息队列的概念。消息在客户端应用程序和消息系统之间异步排队。分布式消息传递系统提供可靠性，可扩展性和持久性的好处。 大多数消息模式遵循发布 - 订阅模型（简称发布 - 订阅）,一旦消息已经被发送者发布，订阅者可以在过滤选项的帮助下接收所选择的消息。通常我们有两种类型的过滤，一种是基于主题的过滤，另一种是基于内容的过滤。 pub-sub模型只能通过消息进行通信。它是一个非常松散耦合的架构 分布式消息系统 描述 Apache Kafka Kafka是在LinkedIn公司开发的，后来它成为Apache的一个子项目。 Apache Kafka基于brokerenabled的，持久的，分布式的发布订阅模型。 Kafka是快速，可扩展和高效的。 RabbitMQ RabbitMQ是一个开源的分布式鲁棒消息应用程序。它易于使用并在所有平台上运行。 JMS(Java Message Service) JMS是一个开源API，支持创建，读取和从一个应用程序向另一个应用程序发送消息。它提供有保证的消息传递并遵循发布 - 订阅模型。 ActiveMQ ActiveMQ消息系统是JMS的开源API。 ZeroMQ ZeroMQ是无代理的对等体消息处理。它提供推拉，路由器 - 经销商消息模式。 Kestrel Kestrel是一个快速，可靠，简单的分布式消息队列。 Thrift协议Thrift在Facebook上构建，用于跨语言服务开发和远程过程调用（RPC）。Apache Thrift是一种接口定义语言，允许以容易的方式在定义的数据类型之上定义新的数据类型和服务实现。Storm广泛使用Thrift协议进行内部通信和数据定义。Storm拓扑只是Thrift Structs。在Apache Storm中运行拓扑的Storm Nimbus是一个Thrift服务。]]></content>
      <categories>
        <category>大数据</category>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 简介]]></title>
    <url>%2F2018%2F03%2F15%2Fredis%2F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Redis 简介Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 BSD开源协议是一个给于使用者很大自由的协议。可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。当你发布使用了BSD协议的代码，或者以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件： 如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议。 如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。 不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。 BSD代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销 售，因此是对商业集成很友好的协议。 很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者 二次开发。 Redis 优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 Redis与其他key-value存储有什么不同？ Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 Redis 数据类型Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 String（字符串）string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。 string类型是Redis最基本的数据类型，一个键最大能存储512MB。 实例1234redis 127.0.0.1:6379&gt; SET name &quot;runoob&quot;OKredis 127.0.0.1:6379&gt; GET name&quot;runoob&quot; 在以上实例中我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 runoob。 注意：一个键最大能存储512MB。 Hash（哈希）Redis hash 是一个键名对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 实例123456789127.0.0.1:6379&gt; HMSET user:1 username runoob password runoob points 200OK127.0.0.1:6379&gt; HGETALL user:11) &quot;username&quot;2) &quot;runoob&quot;3) &quot;password&quot;4) &quot;runoob&quot;5) &quot;points&quot;6) &quot;200&quot; 以上实例中 hash 数据类型存储了包含用户脚本信息的用户对象。 实例中我们使用了 Redis HMSET, HGETALL 命令，user:1 为键值。 每个 hash 可以存储 2 32 -1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 实例1234567891011redis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; lpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;redis&quot;redis 127.0.0.1:6379&gt; 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个string元素到,key对应的set集合中，成功返回1,如果元素已经在集合中返回0,key对应的set不存在返回错误。 1sadd key member 实例12345678910111213redis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;redis&quot; 注意：以上实例中 rabitmq 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应score 1zadd key score member 实例12345678910111213redis 127.0.0.1:6379&gt; zadd runoob 0 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 0redis 127.0.0.1:6379&gt; ZRANGEBYSCORE runoob 0 10001) &quot;redis&quot;2) &quot;mongodb&quot;3) &quot;rabitmq&quot; Redis 命令Redis 命令用于在 redis 服务上执行操作。 要在 redis 服务上执行命令需要一个 redis 客户端。Redis 客户端在我们之前下载的的 redis 的安装包中。 语法Redis 客户端的基本语法为： 1$ redis-cli 实例以下实例讲解了如何启动 redis 客户端： 启动 redis 客户端，打开终端并输入命令 redis-cli。该命令会连接本地的 redis 服务。 12345$redis-cliredis 127.0.0.1:6379&gt;redis 127.0.0.1:6379&gt; PINGPONG 在以上实例中我们连接到本地的 redis 服务并执行 PING 命令，该命令用于检测 redis 服务是否启动。 在远程服务上执行命令如果需要在远程 redis 服务上执行命令，同样我们使用的也是 redis-cli 命令。 语法1$ redis-cli -h host -p port -a password 实例以下实例演示了如何连接到主机为 127.0.0.1，端口为 6379 ，密码为 mypass 的 redis 服务上。 12345$redis-cli -h 127.0.0.1 -p 6379 -a &quot;mypass&quot;redis 127.0.0.1:6379&gt;redis 127.0.0.1:6379&gt; PINGPONG Redis 键(key)Redis 键命令用于管理 redis 的键。 语法Redis 键命令的基本语法如下： 1redis 127.0.0.1:6379&gt; COMMAND KEY_NAME 实例1234redis 127.0.0.1:6379&gt; SET runoobkey redisOKredis 127.0.0.1:6379&gt; DEL runoobkey(integer) 1 在以上实例中 DEL 是一个命令， runoobkey 是一个键。 如果键被删除成功，命令执行后输出 (integer) 1，否则将输出 (integer) 0 Redis keys 命令下表给出了与 Redis 键相关的基本命令： 序号 命令及描述 1 DEL key该命令用于在 key 存在时删除 key。 2 DUMP key 序列化给定 key ，并返回被序列化的值。 3 EXISTS key 检查给定 key 是否存在。 4 EXPIRE key seconds为给定 key 设置过期时间。 5 EXPIREAT key timestamp EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 6 PEXPIRE key milliseconds 设置 key 的过期时间以毫秒计。 7 PEXPIREAT key milliseconds-timestamp 设置 key 过期时间的时间戳(unix timestamp) 以毫秒计 8 KEYS pattern 查找所有符合给定模式( pattern)的 key 。 9 MOVE key db 将当前数据库的 key 移动到给定的数据库 db 当中。 10 PERSIST key 移除 key 的过期时间，key 将持久保持。 11 PTTL key 以毫秒为单位返回 key 的剩余的过期时间。 12 TTL key 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 13 RANDOMKEY 从当前数据库中随机返回一个 key 。 14 RENAME key newkey 修改 key 的名称 15 RENAMENX key newkey 仅当 newkey 不存在时，将 key 改名为 newkey 。 16 TYPE key 返回 key 所储存的值的类型。 更多命令请参考：https://redis.io/commands]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据类型]]></title>
    <url>%2F2018%2F03%2F15%2Fredis%2F%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Redis 字符串(String)Redis 字符串数据类型的相关命令用于管理 redis 字符串值，基本语法如下： 语法1redis 127.0.0.1:6379&gt; COMMAND KEY_NAME 实例1234redis 127.0.0.1:6379&gt; SET runoobkey redisOKredis 127.0.0.1:6379&gt; GET runoobkey&quot;redis&quot; 在以上实例中我们使用了 SET 和 GET 命令，键为 runoobkey。 Redis 字符串命令下表列出了常用的 redis 字符串命令： 序号 命令及描述 1 SET key value 设置指定 key 的值 2 GET key 获取指定 key 的值。 3 GETRANGE key start end 返回 key 中字符串值的子字符 4 GETSET key value将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 5 GETBIT key offset对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 6 MGET key1 [key2..]获取所有(一个或多个)给定 key 的值。 7 SETBIT key offset value对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 8 SETEX key seconds value将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位)。 9 SETNX key value只有在 key 不存在时设置 key 的值。 10 SETRANGE key offset value用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始。 11 STRLEN key返回 key 所储存的字符串值的长度。 12 MSET key value [key value …]同时设置一个或多个 key-value 对。 13 MSETNX key value [key value …] 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 14 PSETEX key milliseconds value这个命令和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，而不是像 SETEX 命令那样，以秒为单位。 15 INCR key将 key 中储存的数字值增一。 16 INCRBY key increment将 key 所储存的值加上给定的增量值（increment） 。 17 INCRBYFLOAT key increment将 key 所储存的值加上给定的浮点增量值（increment） 。 18 DECR key将 key 中储存的数字值减一。 19 DECRBY key decrementkey 所储存的值减去给定的减量值（decrement） 。 20 APPEND key value如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾。 更多命令请参考：https://redis.io/commands Redis 哈希(Hash)Redis hash 是一个string类型的field和value的映射表，hash特别适合用于存储对象。 Redis 中每个 hash 可以存储 232 - 1 键值对（40多亿）。 实例1234567891011127.0.0.1:6379&gt; HMSET runoobkey name &quot;redis tutorial&quot; description &quot;redis basic commands for caching&quot; likes 20 visitors 23000OK127.0.0.1:6379&gt; HGETALL runoobkey1) &quot;name&quot;2) &quot;redis tutorial&quot;3) &quot;description&quot;4) &quot;redis basic commands for caching&quot;5) &quot;likes&quot;6) &quot;20&quot;7) &quot;visitors&quot;8) &quot;23000&quot; 在以上实例中，我们设置了 redis 的一些描述信息(name, description, likes, visitors) 到哈希表的 runoobkey 中。 Redis hash 命令下表列出了 redis hash 基本的相关命令： 序号 命令及描述 1 HDEL key field2 [field2] 删除一个或多个哈希表字段 2 HEXISTS key field 查看哈希表 key 中，指定的字段是否存在。 3 HGET key field 获取存储在哈希表中指定字段的值。 4 HGETALL key 获取在哈希表中指定 key 的所有字段和值 5 HINCRBY key field increment 为哈希表 key 中的指定字段的整数值加上增量 increment 。 6 HINCRBYFLOAT key field increment 为哈希表 key 中的指定字段的浮点数值加上增量 increment 。 7 HKEYS key 获取所有哈希表中的字段 8 HLEN key 获取哈希表中字段的数量 9 HMGET key field1 [field2] 获取所有给定字段的值 10 HMSET key field1 value1 [field2 value2 ] 同时将多个 field-value (域-值)对设置到哈希表 key 中。 11 HSET key field value 将哈希表 key 中的字段 field 的值设为 value 。 12 HSETNX key field value 只有在字段 field 不存在时，设置哈希表字段的值。 13 HVALS key 获取哈希表中所有值 14 HSCAN key cursor [MATCH pattern][COUNT count] 迭代哈希表中的键值对。 更多命令请参考：https://redis.io/commands Redis 列表(List)Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。 实例1234567891011redis 127.0.0.1:6379&gt; LPUSH runoobkey redis(integer) 1redis 127.0.0.1:6379&gt; LPUSH runoobkey mongodb(integer) 2redis 127.0.0.1:6379&gt; LPUSH runoobkey mysql(integer) 3redis 127.0.0.1:6379&gt; LRANGE runoobkey 0 101) &quot;mysql&quot;2) &quot;mongodb&quot;3) &quot;redis&quot; 在以上实例中我们使用了 LPUSH 将三个值插入了名为 runoobkey 的列表当中。 Redis 列表命令下表列出了列表相关的基本命令： 序号 命令及描述 1 BLPOP key1 [key2 ] timeout 移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 2 BRPOP key1 [key2 ] timeout 移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 3 BRPOPLPUSH source destination timeout 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 4 LINDEX key index 通过索引获取列表中的元素 5 [LINSERT key BEFORE\ AFTER pivot value](http://www.runoob.com/redis/lists-linsert.html) 在列表的元素前或者后插入元素 6 LLEN key 获取列表长度 7 LPOP key 移出并获取列表的第一个元素 8 LPUSH key value1 [value2] 将一个或多个值插入到列表头部 9 LPUSHX key value 将一个值插入到已存在的列表头部 10 LRANGE key start stop 获取列表指定范围内的元素 11 LREM key count value 移除列表元素 12 LSET key index value 通过索引设置列表元素的值 13 LTRIM key start stop 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 14 RPOP key 移除并获取列表最后一个元素 15 RPOPLPUSH source destination 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 16 RPUSH key value1 [value2] 在列表中添加一个或多个值 17 RPUSHX key value 为已存在的列表添加值 Redis 集合(Set)Redis的Set是string类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 Redis 中 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。 实例12345678910111213redis 127.0.0.1:6379&gt; SADD runoobkey redis(integer) 1redis 127.0.0.1:6379&gt; SADD runoobkey mongodb(integer) 1redis 127.0.0.1:6379&gt; SADD runoobkey mysql(integer) 1redis 127.0.0.1:6379&gt; SADD runoobkey mysql(integer) 0redis 127.0.0.1:6379&gt; SMEMBERS runoobkey1) &quot;mysql&quot;2) &quot;mongodb&quot;3) &quot;redis&quot; 在以上实例中我们通过 SADD 命令向名为 runoobkey 的集合插入的三个元素。 Redis 集合命令下表列出了 Redis 集合基本命令： 序号 命令及描述 1 SADD key member1 [member2] 向集合添加一个或多个成员 2 SCARD key 获取集合的成员数 3 SDIFF key1 [key2] 返回给定所有集合的差集 4 SDIFFSTORE destination key1 [key2] 返回给定所有集合的差集并存储在 destination 中 5 SINTER key1 [key2] 返回给定所有集合的交集 6 SINTERSTORE destination key1 [key2] 返回给定所有集合的交集并存储在 destination 中 7 SISMEMBER key member 判断 member 元素是否是集合 key 的成员 8 SMEMBERS key 返回集合中的所有成员 9 SMOVE source destination member 将 member 元素从 source 集合移动到 destination 集合 10 SPOP key 移除并返回集合中的一个随机元素 11 SRANDMEMBER key [count] 返回集合中一个或多个随机数 12 SREM key member1 [member2] 移除集合中一个或多个成员 13 SUNION key1 [key2] 返回所有给定集合的并集 14 SUNIONSTORE destination key1 [key2] 所有给定集合的并集存储在 destination 集合中 15 SSCAN key cursor [MATCH pattern] [COUNT count] 迭代集合中的元素 Redis 有序集合(sorted set)Redis 有序集合和集合一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 有序集合的成员是唯一的,但分数(score)却可以重复。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。 实例123456789101112131415161718redis 127.0.0.1:6379&gt; ZADD runoobkey 1 redis(integer) 1redis 127.0.0.1:6379&gt; ZADD runoobkey 2 mongodb(integer) 1redis 127.0.0.1:6379&gt; ZADD runoobkey 3 mysql(integer) 1redis 127.0.0.1:6379&gt; ZADD runoobkey 3 mysql(integer) 0redis 127.0.0.1:6379&gt; ZADD runoobkey 4 mysql(integer) 0redis 127.0.0.1:6379&gt; ZRANGE runoobkey 0 10 WITHSCORES1) &quot;redis&quot;2) &quot;1&quot;3) &quot;mongodb&quot;4) &quot;2&quot;5) &quot;mysql&quot;6) &quot;4&quot; 在以上实例中我们通过命令 ZADD 向 redis 的有序集合中添加了三个值并关联上分数。 Redis 有序集合命令下表列出了 redis 有序集合的基本命令: 序号 命令及描述 1 ZADD key score1 member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 2 ZCARD key 获取有序集合的成员数 3 ZCOUNT key min max 计算在有序集合中指定区间分数的成员数 4 ZINCRBY key increment member 有序集合中对指定成员的分数加上增量 increment 5 ZINTERSTORE destination numkeys key [key …] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 6 ZLEXCOUNT key min max 在有序集合中计算指定字典区间内成员数量 7 ZRANGE key start stop [WITHSCORES] 通过索引区间返回有序集合成指定区间内的成员 8 ZRANGEBYLEX key min max [LIMIT offset count] 通过字典区间返回有序集合的成员 9 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] 通过分数返回有序集合指定区间内的成员 10 ZRANK key member 返回有序集合中指定成员的索引 11 ZREM key member [member …] 移除有序集合中的一个或多个成员 12 ZREMRANGEBYLEX key min max 移除有序集合中给定的字典区间的所有成员 13 ZREMRANGEBYRANK key start stop 移除有序集合中给定的排名区间的所有成员 14 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 15 ZREVRANGE key start stop [WITHSCORES] 返回有序集中指定区间内的成员，通过索引，分数从高到底 16 ZREVRANGEBYSCORE key max min [WITHSCORES] 返回有序集中指定分数区间内的成员，分数从高到低排序 17 ZREVRANK key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 18 ZSCORE key member 返回有序集中，成员的分数值 19 ZUNIONSTORE destination numkeys key [key …] 计算给定的一个或多个有序集的并集，并存储在新的 key 中 20 ZSCAN key cursor [MATCH pattern] [COUNT count] 迭代有序集合中的元素（包括元素成员和元素分值）]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis HyperLogLog基数统计]]></title>
    <url>%2F2018%2F03%2F15%2Fredis%2FHyperLogLog-%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85-%E4%BA%8B%E5%8A%A1-%E8%BF%9E%E6%8E%A5-%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Redis HyperLogLogRedis 在 2.8.9 版本添加了 HyperLogLog 结构。 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数?比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 实例以下实例演示了 HyperLogLog 的工作过程： 123456789101112131415redis 127.0.0.1:6379&gt; PFADD runoobkey &quot;redis&quot;1) (integer) 1redis 127.0.0.1:6379&gt; PFADD runoobkey &quot;mongodb&quot;1) (integer) 1redis 127.0.0.1:6379&gt; PFADD runoobkey &quot;mysql&quot;1) (integer) 1redis 127.0.0.1:6379&gt; PFCOUNT runoobkey(integer) 3 Redis HyperLogLog 命令下表列出了 redis HyperLogLog 的基本命令： 序号 命令及描述 1 PFADD key element [element …] 添加指定元素到 HyperLogLog 中。 2 PFCOUNT key [key …] 返回给定 HyperLogLog 的基数估算值。 3 PFMERGE destkey sourcekey [sourcekey …] 将多个 HyperLogLog 合并为一个 HyperLogLog Redis 发布订阅Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 Redis 客户端可以订阅任意数量的频道。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 实例以下实例演示了发布订阅是如何工作的。在我们实例中我们创建了订阅频道名为 redisChat: 123456redis 127.0.0.1:6379&gt; SUBSCRIBE redisChatReading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;redisChat&quot;3) (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 redisChat 发布两次消息，订阅者就能接收到消息。 123456789101112131415redis 127.0.0.1:6379&gt; PUBLISH redisChat &quot;Redis is a great caching technique&quot;(integer) 1redis 127.0.0.1:6379&gt; PUBLISH redisChat &quot;Learn redis by runoob.com&quot;(integer) 1# 订阅者的客户端会显示如下消息1) &quot;message&quot;2) &quot;redisChat&quot;3) &quot;Redis is a great caching technique&quot;1) &quot;message&quot;2) &quot;redisChat&quot;3) &quot;Learn redis by runoob.com&quot; Redis 发布订阅命令下表列出了 redis 发布订阅常用命令： 序号 命令及描述 1 PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道。 2 [PUBSUB subcommand argument [argument …]] 查看订阅与发布系统状态。 3 PUBLISH channel message 将信息发送到指定的频道。 4 [PUNSUBSCRIBE pattern [pattern …]] 退订所有给定模式的频道。 5 SUBSCRIBE channel [channel …] 订阅给定的一个或多个频道的信息。 6 [UNSUBSCRIBE channel [channel …]] 指退订给定的频道。 Redis 事务Redis 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 一个事务从开始到执行会经历以下三个阶段： 开始事务。 命令入队。 执行事务。 实例以下是一个事务的例子， 它先以 MULTI 开始一个事务， 然后将多个命令入队到事务中， 最后由 EXEC 命令触发事务， 一并执行事务中的所有命令： 12345678910111213141516171819202122redis 127.0.0.1:6379&gt; MULTIOKredis 127.0.0.1:6379&gt; SET book-name &quot;Mastering C++ in 21 days&quot;QUEUEDredis 127.0.0.1:6379&gt; GET book-nameQUEUEDredis 127.0.0.1:6379&gt; SADD tag &quot;C++&quot; &quot;Programming&quot; &quot;Mastering Series&quot;QUEUEDredis 127.0.0.1:6379&gt; SMEMBERS tagQUEUEDredis 127.0.0.1:6379&gt; EXEC1) OK2) &quot;Mastering C++ in 21 days&quot;3) (integer) 34) 1) &quot;Mastering Series&quot; 2) &quot;C++&quot; 3) &quot;Programming&quot; Redis 事务命令下表列出了 redis 事务的相关命令： 序号 命令及描述 1 DISCARD 取消事务，放弃执行事务块内的所有命令。 2 EXEC 执行所有事务块内的命令。 3 MULTI 标记一个事务块的开始。 4 UNWATCH 取消 WATCH 命令对所有 key 的监视。 5 WATCH key [key …] 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 Redis 脚本Redis 脚本使用 Lua 解释器来执行脚本。 Reids 2.6 版本通过内嵌支持 Lua 环境。执行脚本的常用命令为 EVAL。 语法Eval 命令的基本语法如下： 1redis 127.0.0.1:6379&gt; EVAL script numkeys key [key ...] arg [arg ...] 实例以下实例演示了 redis 脚本工作过程： 123456redis 127.0.0.1:6379&gt; EVAL &quot;return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;&quot; 2 key1 key2 first second1) &quot;key1&quot;2) &quot;key2&quot;3) &quot;first&quot;4) &quot;second&quot; Redis 脚本命令下表列出了 redis 脚本常用命令： 序号 命令及描述 1 EVAL script numkeys key [key …] arg [arg …] 执行 Lua 脚本。 2 EVALSHA sha1 numkeys key [key …] arg [arg …] 执行 Lua 脚本。 3 SCRIPT EXISTS script [script …] 查看指定的脚本是否已经被保存在缓存当中。 4 SCRIPT FLUSH 从脚本缓存中移除所有脚本。 5 SCRIPT KILL 杀死当前正在运行的 Lua 脚本。 6 SCRIPT LOAD script 将脚本 script 添加到脚本缓存中，但并不立即执行这个脚本。 Redis 连接Redis 连接命令主要是用于连接 redis 服务。 实例以下实例演示了客户端如何通过密码验证连接到 redis 服务，并检测服务是否在运行： 1234redis 127.0.0.1:6379&gt; AUTH &quot;password&quot;OKredis 127.0.0.1:6379&gt; PINGPONG Redis 连接命令下表列出了 redis 连接的基本命令： 序号 命令及描述 1 AUTH password 验证密码是否正确 2 ECHO message 打印字符串 3 PING 查看服务是否运行 4 QUIT 关闭当前连接 5 SELECT index 切换到指定的数据库 Redis 服务器Redis 服务器命令主要是用于管理 redis 服务。 实例以下实例演示了如何获取 redis 服务器的统计信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788redis 127.0.0.1:6379&gt; INFO# Serverredis_version:2.8.13redis_git_sha1:00000000redis_git_dirty:0redis_build_id:c2238b38b1edb0e2redis_mode:standaloneos:Linux 3.5.0-48-generic x86_64arch_bits:64multiplexing_api:epollgcc_version:4.7.2process_id:3856run_id:0e61abd297771de3fe812a3c21027732ac9f41fetcp_port:6379uptime_in_seconds:11554uptime_in_days:0hz:10lru_clock:16651447config_file:# Clientsconnected_clients:1client-longest_output_list:0client-biggest_input_buf:0blocked_clients:0# Memoryused_memory:589016used_memory_human:575.21Kused_memory_rss:2461696used_memory_peak:667312used_memory_peak_human:651.67Kused_memory_lua:33792mem_fragmentation_ratio:4.18mem_allocator:jemalloc-3.6.0# Persistenceloading:0rdb_changes_since_last_save:3rdb_bgsave_in_progress:0rdb_last_save_time:1409158561rdb_last_bgsave_status:okrdb_last_bgsave_time_sec:0rdb_current_bgsave_time_sec:-1aof_enabled:0aof_rewrite_in_progress:0aof_rewrite_scheduled:0aof_last_rewrite_time_sec:-1aof_current_rewrite_time_sec:-1aof_last_bgrewrite_status:okaof_last_write_status:ok# Statstotal_connections_received:24total_commands_processed:294instantaneous_ops_per_sec:0rejected_connections:0sync_full:0sync_partial_ok:0sync_partial_err:0expired_keys:0evicted_keys:0keyspace_hits:41keyspace_misses:82pubsub_channels:0pubsub_patterns:0latest_fork_usec:264# Replicationrole:masterconnected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# CPUused_cpu_sys:10.49used_cpu_user:4.96used_cpu_sys_children:0.00used_cpu_user_children:0.01# Keyspacedb0:keys=94,expires=1,avg_ttl=41638810db1:keys=1,expires=0,avg_ttl=0db3:keys=1,expires=0,avg_ttl=0 Redis 服务器命令下表列出了 redis 服务器的相关命令: 序号 命令及描述 1 BGREWRITEAOF 异步执行一个 AOF（AppendOnly File） 文件重写操作 2 BGSAVE 在后台异步保存当前数据库的数据到磁盘 3 CLIENT KILL [ip:port] [ID client-id] 关闭客户端连接 4 CLIENT LIST 获取连接到服务器的客户端连接列表 5 CLIENT GETNAME 获取连接的名称 6 CLIENT PAUSE timeout 在指定时间内终止运行来自客户端的命令 7 CLIENT SETNAME connection-name 设置当前连接的名称 8 CLUSTER SLOTS 获取集群节点的映射数组 9 COMMAND 获取 Redis 命令详情数组 10 COMMAND COUNT 获取 Redis 命令总数 11 COMMAND GETKEYS 获取给定命令的所有键 12 TIME 返回当前服务器时间 13 COMMAND INFO command-name [command-name …] 获取指定 Redis 命令描述的数组 14 CONFIG GET parameter 获取指定配置参数的值 15 CONFIG REWRITE 对启动 Redis 服务器时所指定的 redis.conf 配置文件进行改写 16 CONFIG SET parameter value 修改 redis 配置参数，无需重启 17 CONFIG RESETSTAT 重置 INFO 命令中的某些统计数据 18 DBSIZE 返回当前数据库的 key 的数量 19 DEBUG OBJECT key 获取 key 的调试信息 20 DEBUG SEGFAULT 让 Redis 服务崩溃 21 FLUSHALL 删除所有数据库的所有key 22 FLUSHDB 删除当前数据库的所有key 23 INFO [section] 获取 Redis 服务器的各种信息和统计数值 24 LASTSAVE 返回最近一次 Redis 成功将数据保存到磁盘上的时间，以 UNIX 时间戳格式表示 25 MONITOR 实时打印出 Redis 服务器接收到的命令，调试用 26 ROLE 返回主从实例所属的角色 27 SAVE 异步保存数据到硬盘 28 SHUTDOWN [NOSAVE] [SAVE] 异步保存数据到硬盘，并关闭服务器 29 SLAVEOF host port 将当前服务器转变为指定服务器的从属服务器(slave server) 30 SLOWLOG subcommand [argument] 管理 redis 的慢日志 31 SYNC 用于复制功能(replication)的内部命令]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>HyperLogLog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIVE sql函数参考]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhive%2F%E5%87%BD%E6%95%B0%E5%8F%82%E8%80%83%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[函数分类 HIVE CLI命令显示当前会话有多少函数可用SHOW FUNCTIONS; 显示函数的描述信息DESC FUNCTION concat; 显示函数的扩展描述信息DESC FUNCTION EXTENDED concat; 简单函数函数的计算粒度为单条记录。关系运算数学运算逻辑运算数值计算类型转换日期函数条件函数字符串函数统计函数 聚合函数函数处理的数据粒度为多条记录。sum()—求和count()—求数据量avg()—求平均直distinct—求不同值数min—求最小值max—求最人值 集合函数复合类型构建复杂类型访问复杂类型长度 特殊函数窗口函数应用场景用于分区排序动态Group ByTop N累计计算层次查询 Windowing functions 1234leadlagFIRST_VALUELAST_VALUE 分析函数Analytics functions 123456RANKROW_NUMBERDENSE_RANKCUME_DISTPERCENT_RANKNTILE 混合函数1java_method(class,method [,arg1 [,arg2])reflect(class,method [,arg1 [,arg2..]])hash(a1 [,a2...]) UDTF1lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,‘ columnAlias)* fromClause: FROM baseTable (lateralView)* ateral view用于和split,explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。 常用函数Demo：123456789create table employee( id string, money double, type string)row format delimited fields terminated by &apos;\t&apos; lines terminated by &apos;\n&apos; stored as textfile; load data local inpath ‘/liguodong/hive/data‘ into table employee;select * from employee; 12优先级依次为NOT AND ORselect id,money from employee where (id=‘1001‘ or id=‘1002‘) and money=‘100‘; cast类型转换 1select cast(1.5 as int); if**判断** 1if(con,‘‘,‘‘); 123hive (default)&gt; select if(2&gt;1,‘YES‘,‘NO‘);YES 123case when con then ‘‘ when con then ‘‘ else ‘‘ end (‘‘里面类型要一样)select case when id=‘1001‘ then ‘v1001‘ when id=‘1002‘ then ‘v1002‘ else ‘v1003‘ end from employee; get_json_object 1get_json_object(json 解析函数，用来处理json，必须是json格式)select get_json_object(‘&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:&quot;20&quot;&#125;‘,‘$.name‘); URL**解析函数** 123parse_url(string urlString, string partToExtract [, string keyToExtract])select parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1‘, ‘HOST‘) from employee limit 1; 字符串连接函数： concat语法: concat(string A, string B…)返回值: string说明：返回输入字符串连接后的结果，支持任意个输入字符串举例： 1hive&gt; select concat(‘abc‘,‘def’,‘gh‘) from lxw_dual; 带分隔符字符串连接函数： concat_ws语法: concat_ws(string SEP, string A, string B…)返回值: string说明：返回输入字符串连接后的结果， SEP 表示各个字符串间的分隔符 12345concat_ws(string SEP, array&lt;string&gt;)举例：hive&gt; select concat_ws(‘,‘,‘abc‘,‘def‘,‘gh‘) from lxw_dual;abc,def,gh 列出该字段所有不重复的值，相当于去重123456collect_set(id) //返回的是数组列出该字段所有的值，列出来不去重 collect_list(id) //返回的是数组select collect_set(id) from taborder; 求和 1sum(money) 统计列数123count(*)select sum(num),count(*) from taborder; 窗口函数 1234first_value(第一行值)first_value(money) over (partition by id order by money)select ch,num,first_value(num) over (partition by ch order by num) from taborder; last_value 最后一行值 1hive (liguodong)&gt; select ch,num,last_value(num) over (partition by ch) from taborder; lead 12去当前行后面的第二行的值lead(money,2) over (order by money) lag 12去当前行前面的第二行的值lag(money,2) over (order by money) rank排名 12345rank() over(partition by id order by money)select ch, num, rank() over(partition by ch order by num) as rank from taborder;select ch, num, dense_rank() over(partition by ch order by num) as dense_rank from taborder; cume_dist 123456789101112cume_dist (相同值的最大行号/行数)cume_dist() over (partition by id order by money)percent_rank (相同值的最小行号-1)/(行数-1)第一个总是从0开始percent_rank() over (partition by id order by money) select ch,num,cume_dist() over (partition by ch order by num) as cume_dist, percent_rank() over (partition by ch order by num) as percent_rank from taborder; ntile分片123ntile(2) over (order by money desc) 分两份 select ch,num,ntile(2) over (order by num desc) from taborder; 混合函数 1select id,java_method(&quot;java.lang,Math&quot;,&quot;sqrt&quot;,cast(id as double)) as sqrt from hiveTest; UDTF 12345select id,adid from employee lateral view explode(split(type,‘B‘)) tt as adid;explode 把一列转成多行 123hive (liguodong)&gt; select id,adid &gt; from hiveDemo &gt; lateral view explode(split(str,‘,‘)) tt as adid; 正则表达式使用正则表达式的函数regexp_replace(string subject A,string B,string C)regexp_extract(string subject,string pattern,int index) 123456hive&gt; select regexp_replace(‘foobar‘, ‘oo|ar‘, ‘‘) from lxw_dual;output&gt;&gt;fbhive&gt; select regexp_replace(‘979|7.10.80|8684‘, ‘.*\\|(.*)‘,1) from hiveDemo limit 1;]]></content>
      <categories>
        <category>大数据</category>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS java api]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhdfs%2Fhdfs%20java%20api%2F</url>
    <content type="text"><![CDATA[4 hdfs java开发依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 4.1 获取api中的客户端对象12Configuration conf = new Configuration()FileSystem fs = FileSystem.get(conf) 我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例。get方法是从conf中的一个参数 fs.defaultFS的配置值判断用哪种文件系统。如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的是一个本地文件系统的客户端对象 4.2 DistributedFileSystem实例对象所具备的方法 4.3 HDFS客户端操作数据代码示例文件操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public class HdfsClient &#123; FileSystem fs = null; @Before public void init() throws Exception &#123; // 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI // 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://hdp-node01:9000"); /** * 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 */ conf.set("dfs.replication", "3"); // 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例 // fs = FileSystem.get(conf); // 如果这样去获取，那conf里面就可以不要配"fs.defaultFS"参数，而且，这个客户端的身份标识已经是hadoop用户 fs = FileSystem.get(new URI("hdfs://hdp-node01:9000"), conf, "hadoop"); &#125; /** * 往hdfs上传文件 * * @throws Exception */ @Test public void testAddFileToHdfs() throws Exception &#123; // 要上传的文件所在的本地路径 Path src = new Path("g:/redis-recommend.zip"); // 要上传到hdfs的目标路径 Path dst = new Path("/aaa"); fs.copyFromLocalFile(src, dst); fs.close(); &#125; /** * 从hdfs中复制文件到本地文件系统 */ @Test public void testDownloadFileToLocal() throws IllegalArgumentException, IOException &#123; fs.copyToLocalFile(new Path("/jdk-7u65-linux-i586.tar.gz"), new Path("d:/")); fs.close(); &#125; @Test public void testMkdirAndDeleteAndRename() throws IllegalArgumentException, IOException &#123; // 创建目录 fs.mkdirs(new Path("/a1/b1/c1")); // 删除文件夹 ，如果是非空文件夹，参数2必须给值true fs.delete(new Path("/aaa"), true); // 重命名文件或文件夹 fs.rename(new Path("/a1"), new Path("/a2")); &#125; /** * 查看目录信息，只显示文件 */ @Test public void testListFiles() throws FileNotFoundException, IllegalArgumentException, IOException &#123; // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) &#123; System.out.println("block-length:" + bl.getLength() + "--" + "block-offset:" + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println("--------------为angelababy打印的分割线--------------"); &#125; &#125; /** * 查看文件及文件夹信息 */ @Test public void testListAll() throws FileNotFoundException, IllegalArgumentException, IOException &#123; FileStatus[] listStatus = fs.listStatus(new Path("/")); String flag = "d-- "; for (FileStatus fstatus : listStatus) &#123; if (fstatus.isFile()) flag = "f-- "; System.out.println(flag + fstatus.getPath().getName()); &#125; &#125;&#125; 通过流的方式访问hdfs 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 相对那些封装好的方法而言的更底层一些的操作方式 * 上层那些mapreduce spark等运算框架，去hdfs中获取数据的时候，就是调的这种底层的api * @author * */public class StreamAccess &#123; FileSystem fs = null; @Before public void init() throws Exception &#123; Configuration conf = new Configuration(); fs = FileSystem.get(new URI("hdfs://hdp-node01:9000"), conf, "hadoop"); &#125; /** * 通过流的方式上传文件到hdfs */ @Test public void testUpload() throws Exception &#123; FSDataOutputStream outputStream = fs.create(new Path("/angelababy.love"), true); FileInputStream inputStream = new FileInputStream("c:/angelababy.love"); IOUtils.copy(inputStream, outputStream); &#125; @Test public void testDownLoadFileToLocal() throws IllegalArgumentException, IOException&#123; //先获取一个文件的输入流----针对hdfs上的 FSDataInputStream in = fs.open(new Path("/jdk-7u65-linux-i586.tar.gz")); //再构造一个文件的输出流----针对本地的 FileOutputStream out = new FileOutputStream(new File("c:/jdk.tar.gz")); //再将输入流中数据传输到输出流 IOUtils.copyBytes(in, out, 4096); &#125; /** * hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度 * 用于上层分布式运算框架并发处理数据 */ @Test public void testRandomAccess() throws IllegalArgumentException, IOException&#123; //先获取一个文件的输入流----针对hdfs上的 FSDataInputStream in = fs.open(new Path("/iloveyou.txt")); //可以将流的起始偏移量进行自定义 in.seek(22); //再构造一个文件的输出流----针对本地的 FileOutputStream out = new FileOutputStream(new File("c:/iloveyou.line.2.txt")); IOUtils.copyBytes(in,out,19L,true); &#125; /** * 显示hdfs上文件的内容 */ @Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path("/iloveyou.txt")); IOUtils.copyBytes(in, System.out, 1024); &#125;&#125; 获取一个文件的所有block位置信息，然后读取指定block中的内容 123456789101112131415161718192021222324252627282930@Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path("/weblog/input/access.log.10")); //拿到文件信息 FileStatus[] listStatus = fs.listStatus(new Path("/weblog/input/access.log.10")); //获取这个文件的所有block的信息 BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(listStatus[0], 0L, listStatus[0].getLen()); //第一个block的长度 long length = fileBlockLocations[0].getLength(); //第一个block的起始偏移量 long offset = fileBlockLocations[0].getOffset(); System.out.println(length); System.out.println(offset); //获取第一个block写入输出流// IOUtils.copyBytes(in, System.out, (int)length); byte[] b = new byte[4096]; FileOutputStream os = new FileOutputStream(new File("d:/block0")); while(in.read(offset, b, 0, 4096)!=-1)&#123; os.write(b); offset += 4096; if(offset&gt;=length) return; &#125;; os.flush(); os.close(); in.close(); &#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS架构]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhdfs%2FHDFS%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[3 Hdfs架构设计： 超大文件 流式数据访问 运行在廉价商用硬件上 以时间延时为代价的高吞吐量 写入只支持单个写入，且修改只支持追加 概念数据块hdfs文件划分为多个块，块作为独立存储单元。大小默认128M，目的为了最小化寻址开销，但也不能太大，map函数通常一次处理一个块的数据 联邦HDFS解决单个namenode内存限制，创建多个namenode，每个namenode负责文件系统命名空间的一部分，如/user,/share， 每个namenode维护一个命名空间卷，由命名空间的元数据和一个数据块池组成，命名空间卷相互独立，两两之间互不通信 3.1 概述 HDFS集群分为两大角色：NameNode、DataNode (、Secondary Namenode) NameNode负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 3.2 Namenode 和 DatanodeHDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。 Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。 集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。 从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。 集群中单一Namenode的结构大大简化了系统的架构。Namenode是所有HDFS元数据的仲裁者和管理者，这样，用户数据永远不会流过Namenode。 3.3 文件系统的命名空间（namespace）HDFS支持传统的层次型文件组织结构。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件和目录。 Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。 3.4 数据复制HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。副本系数可以在文件创建的时候指定，也可以在之后改变。==HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者==。 Namenode全权管理数据块的复制，它周期性地从集群中的每个datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。 HDFS的一致性分析 为什么HDFS不支持多个writer同时写一个文件,即不支持并发写? 多个reducer对同一文件执行写操作,即多个writer同时向HDFS的同一文件执行写操作, 这需要昂贵的同步机制不说, 最重要的是这种做法将各reducer的写操作顺序化, 不利于各reduce任务的并行。 3.4.1 副本存放副本的存放是HDFS可靠性和性能的关键。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。 通过一个机架感知的过程，Namenode可以确定每个Datanode所属的机架id。将副本存放在不同的机架上，有效防止当整个机架失效时数据的丢失，但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。 在大多数情况下，副本系数是3，HDFS的存放策略是: 如果写请求方所在机器是其中一个datanode,则直接存放在本地,否则随机在集群中选择一个datanode. 第二个副本存放于不同第一个副本的所在的机架. 第三个副本存放于第二个副本所在的机架,但是属于不同的节点. 其他副本均匀分布在剩下的机架中。如果replication factor大于3，则第4个以后的数据随机放在其他机架上，但保证每个机架副本数&lt;(replicas - 1) / racks + 2 3.4.2 副本选择为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。 3.4.3 安全模式Namenode启动后会进入一个称为安全模式的特殊状态。处于安全模式的Namenode是不会进行数据块的复制的。Namenode从所有的Datanode接收心跳信号和块状态报告。块状态报告包括了某个Datanode所有的数据块列表。每个数据块都有一个指定的最小副本数。当Namenode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的；在一定百分比（这个参数可配置）的数据块被Namenode检测确认是安全之后（加上一个额外的30秒等待时间），Namenode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他Datanode上。 3.4.4 文件系统元数据的持久化Namenode上保存着HDFS的名字空间。对于任何对文件系统元数据产生修改的操作，Namenode都会使用一种称为EditLog的事务日志记录下来。Namenode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的名字空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在Namenode所在的本地文件系统上。 Namenode在内存中保存着整个文件系统的名字空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的Namenode足够支撑大量的文件和目录。当Namenode启动时或者到达checkpoint 的条件时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog。这个过程是checkpoint 12dfs.namenode.checkpoint.perioddfs.namenode.checkpoint.txns Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到Namenode，这个报告就是块状态报告。 3.5 通信协议所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到Namenode，通过ClientProtocol协议与Namenode交互。而Datanode使用DatanodeProtocol协议与Namenode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。在设计上，Namenode不会主动发起RPC，而是响应来自客户端或 Datanode 的RPC请求。 3.6 健壮性HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：Namenode出错, Datanode出错和网络隔离(network partitions)。 3.6.1 磁盘数据错误，心跳检测和重新复制每个Datanode节点周期性地向Namenode发送心跳信号。网络割裂可能导致一部分Datanode跟Namenode失去联系。Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。 3.6.2 群集的负载均衡HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。 3.6.3 数据完整性checksum。从某个Datanode获取的数据块有可能是损坏的，可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。 3.6.4元数据磁盘错误FsImage和Editlog是HDFS的核心数据结构。Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这可能会降低Namenode每秒处理事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。 3.6.5 快照回滚。快照支持某一特定时刻的数据的复制备份。利用快照，可以让HDFS在数据损坏时恢复到过去一个已知正确的时间点。 3.7 数据组织3.7.1 数据块HDFS被设计成支持大文件，适用需要处理大规模的数据集的应用。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是128MB。因而，HDFS中的文件总是按照128M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。 小于块大小的小文件不会占用整个HDFS块空间。也就是说，较多的小文件会占用更多的NAMENODE的内存（记录了文件的位置等信息） 3.7.2 Staging客户端创建文件的请求其实并没有立即发送给Namenode，会先将文件数据缓存到本地的一个临时文件。 先将文件数据缓存到本地的一个临时文件 当这个临时文件累积的数据量超过一个数据块的大小，客户端才会联系Namenode。Namenode将文件名插入文件系统的层次结构中，并返回Datanode的标识符和目标数据块给客户端。 客户端将这块数据从本地临时文件上传到指定的Datanode上 客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到日志里进行存储。如果Namenode在文件关闭前宕机了，则该文件将丢失 3.7.3 流水线复制当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。当本地临时文件累积到一个数据块的大小时，客户端会从Namenode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。 3.8 存储空间回收3.8.1 文件的删除和恢复当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的 3.8.2 减少副本系数当一个文件的副本系数被减小后，Namenode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。 3.9 HDFS写文件流程客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本 ​ 初始化FileSystem，客户端调用create()来创建文件 FileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件，元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。 FileSystem返回FSDataOutputStream，客户端用于写数据，客户端开始写入数据。FSDataOutputStream封装了DFSOutputStream，用于和datanode通信 DFSOutputStream将数据分成块，写入data queue。data queue由DataStreamer处理，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。DataStreamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。 DFSOutputStream为发出去的数据包保存了ack queue，收到pipeline中的==所有==数据节点确认信息才删除数据包。 当客户端结束写入数据，则调用stream的close函数。此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。 通知元数据节点写入完毕。 容错机制： 多个client同时写入一个文件 写入时会在命名空间新建一个文件，但该文件还没有相应数据块。不需要考虑文件写入失败需要删除文件，因为是先创建文件再写入的，参考window写word 写文件时datanode异常 如果数据节点在写入的过程中失败，关闭pipeline，将ack queue中的数据块放入data queue的开始，已经写入的错误数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，会被删除。失败的数据节点从pipeline中移除，另外的数据块则写入pipeline中的另外两个数据节点。元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。只要满足最小副本要求即可 3.10 HDFS读文件流程客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件 初始化FileSystem，然后客户端(client)用FileSystem的open()函数打开文件 FileSystem用RPC调用元数据节点，得到文件的数据块信息，对于每一个数据块，元数据节点返回保存数据块的数据节点的地址。 FileSystem返回FSDataInputStream给客户端，用来读取数据，客户端调用stream的read()函数开始读取数据。 DFSInputStream连接保存此文件第一个数据块的最近的数据节点，data从数据节点读到客户端(client) 当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。 当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。 在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。8. 失败的数据节点将被记录，以后不再连接。【注意：这里的序号不是一一对应的关系】 3.11 NAMENODE工作机制学习目标：理解namenode的工作机制尤其是元数据管理机制，以增强对HDFS工作原理的理解，及培养hadoop集群运营中“性能调优”、“namenode”故障问题的分析解决能力 问题场景： 集群启动后，可以查看文件，但是上传文件时报错，打开web页面可看到namenode正处于safemode状态，怎么处理？ Namenode服务器的磁盘故障导致namenode**宕机，如何挽救集群及数据？ Namenode是否可以有多个？namenode内存要配置多大？namenode跟集群数据存储能力有关系吗？ 文件的blocksize究竟调大好还是调小好？ …… 诸如此类问题的回答，都需要基于对namenode自身的工作原理的深刻理解 3.11.1 NAMENODE职责NAMENODE职责： 负责客户端请求的响应 元数据的管理（查询，修改） 3.11.2 元数据管理namenode对数据的管理采用了三种存储形式： 内存元数据(NameSystem) 磁盘元数据镜像文件FsImage 数据操作日志文件Editlog（可通过日志运算出元数据） 3.11.3 查看FsImage和Editlog状态Offline Edits Viewer is a tool to parse the Edits log file. 12bash$ bin/hdfs oev -p xml -i edits -o edits.xmlbash$ bin/hdfs oev -i edits -o edits.xml 3.11.4 元数据的checkpoint每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint） checkpoint的详细过程 SecondaryNameNode有两个作用，一是镜像备份，二是日志与镜像的定期合并。 SecondaryNameNode通知NameNode准备提交edits文件，此时主节点将新的写操作数据记录到一个新的文件edits.new中。 SecondaryNameNode通过HTTP GET方式获取NameNode的fsimage与edits文件（在SecondaryNameNode的current同级目录下可见到 temp.check-point或者previous-checkpoint目录，这些目录中存储着从namenode拷贝来的镜像文件）。 SecondaryNameNode开始合并获取的上述两个文件，产生一个新的fsimage文件fsimage.ckpt。 SecondaryNameNode用HTTP POST方式发送fsimage.ckpt至NameNode。 NameNode将fsimage.ckpt与edits.new文件分别重命名为fsimage与edits，然后更新fstime，整个checkpoint过程到此结束。 checkpoint操作的触发条件配置参数 12345678dfs.namenode.checkpoint.check.period=60 #检查触发条件是否满足的频率，60秒dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary#以上两个参数做checkpoint操作时，secondary namenode的本地工作目录dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;dfs.namenode.checkpoint.max-retries=3 #最大重试次数dfs.namenode.checkpoint.period=3600 #两次checkpoint之间的时间间隔3600秒dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录 3.11.5 元数据目录说明在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘： 1$HADOOP_HOME/bin/hdfs namenode -format 格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构 123456current/|-- VERSION|-- edits_*|-- fsimage_0000000000008547077|-- fsimage_0000000000008547077.md5`-- seen_txid 其中的dfs.name.dir是在hdfs-site.xml文件中配置的，默认值file://\${hadoop.tmp.dir}/dfs/name, hadoop.tmp.dir是在core-site.xml中配置的，默认值/tmp/hadoop-\${user.name} dfs.namenode.name.dir属性可以配置多个目录，如/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,….。各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存。 $dfs.namenode.name.dir/current/目录 VERSION文件是Java属性文件，内容大致如下： 1234567#Wed Dec 06 08:57:50 CST 2017namespaceID=1084867803clusterID=CID-2489a7d6-7f2e-4088-8f29-349ce1f5275fcTime=1511719134838storageType=NAME_NODEblockpoolID=BP-1249732924-192.168.2.32-1511719134838layoutVersion=-63 namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的； storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）； cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameNode升级之后，cTime将会记录更新时间戳； layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用； clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它； blockpoolID：是针对每一个Namespace所对应的blockpool的ID seen_txid非常重要，是存放transactionId的文件。format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。 $dfs.namenode.name.dir/current目录下在format的同时也会生成fsimage和edits文件，及其对应的md5校验文件。 3.12 DATANODE的工作机制问题场景： 集群容量不够，怎么扩容？ 如果有一些datanode宕机，该怎么办？ datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？ Datanode工作职责: 存储管理用户的文件块数据 定期向namenode汇报自身所持有的block信息（通过心跳信息上报） 这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题 123456&gt; &lt;property&gt;&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;&gt; &lt;value&gt;3600000&lt;/value&gt;&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&gt; &lt;/property&gt;&gt; Datanode掉线判断时限参数 ​ datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： ​ timeout = 2 heartbeat.recheck.interval + 10 dfs.heartbeat.interval 默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒 3.13 一致模型HDFS 为了性能牺牲了一部分POSIX要求，文件读写的可见性有所不同 默认同flush() flush()不立即可见，数据超过一个块后，第一个块才可见，之后的块也一样 hflush()立即可见，但仅保证所有数据传输到所有datanode的内存（pipeline）中 hsync()同hflush()，但所有数据同步到磁盘中 close()，同flush() 3.14 目录结构namenode目录结构1234567891011├── current│ ├── edits_0000000000000000001-0000000000000000428│ ├── edits_0000000000000000428-0000000000000000429│ ├── edits_inprogress_0000000000000000430│ ├── fsimage_0000000000000000427│ ├── fsimage_0000000000000000427.md5│ ├── fsimage_0000000000000000429│ ├── fsimage_0000000000000000429.md5│ ├── seen_txid│ └── VERSION└── in_use.lock VERSION包含hdfs版本信息 1234567#Thu May 10 14:24:22 CST 2018namespaceID=1435979192 # 文件系统命名空间唯一标识clusterID=CID-da2d62ad-2fd2-4a2c-b7f9-aa1a8e07915a # hdfs集群唯一标识，对联邦hdfs很重要cTime=0 # 系统创建时间storageType=NAME_NODE # 存储类型blockpoolID=BP-1294332222-192.168.199.163-1525600695333 # 数据块池唯一标识，包含namenode包含管理的所有文件layoutVersion=-60 #描述hdfs持久化数据结构的版本 in_use.lock：namenode使用该文件对存储目录加锁，文本内容是本机hadoop的pid edits_*：编辑日志，写操作会先记录到日志中。编辑日志修改时，相关元数据也会修改 edits_inprogress_*：任何时刻只有一个文件处于打开状态，写操作时只有当每个事务完成后才更新该文件 fsimage_*：元数据一个完整的永久检查点 seen_txid：存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_inprogress_*文件的尾数 datanode目录结构12345678910111213141516├── current│ ├── BP-1294332222-192.168.199.163-1525600695333│ │ ├── current│ │ │ ├── finalized│ │ │ │ └── subdir0│ │ │ │ └── subdir0│ │ │ │ ├── blk_1073741829│ │ │ │ ├── blk_1073741829_1005.meta│ │ │ │ ├── blk_1073741830│ │ │ │ ├── blk_1073741830_1006.meta│ │ │ ├── rbw│ │ │ └── VERSION│ │ ├── scanner.cursor│ │ └── tmp│ └── VERSION└── in_use.lock hdfs数据块存储在以blk_为前缀的文件中，包含该块的原始字节数，并关联一个.meta的元数据文件，包括头部和该块各区段一系列的校验和。 BP-*：每个块属于一个数据块池，每个数据块池都有自己的目录，目录是namenode的VERSION中的数据块池id finalized保存所有的块，当块数量过多，会创建子目录，默认64，方便管理 in_use.lock同namenode]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS介绍和命令]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhdfs%2FHDFS%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 HDFS基本概念篇Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。 1.1 前沿 设计思想 分而治之：将大文件、大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析； 在大数据系统中作用： 为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务 重点概念：文件切块，副本存放，元数据 1.2 HDFS的概念和特性首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色； 重要特性如下： HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data 目录结构及文件分块信息(元数据)的管理由namenode节点承担。namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器） 文件的各个block的存储管理由datanode节点承担 datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication） （1）HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改，支持追加 注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高 1.3 前提和设计目标硬件错误​ 硬件错误是常态而不是异常。HDFS可能由成百上千的服务器所构成，每个服务器上存储着文件系统的部分数据。任一组件都有可能失效，因此错误检测和快速、自动的恢复是HDFS最核心的架构目标。 流式数据访问​ 运行在HDFS上的应用需要流式访问它们的数据集。HDFS的设计中更多的考虑到了数据批处理，而不是用户交互处理。比之数据访问的低延迟问题，更关键的在于数据访问的高吞吐量。 大规模数据集​ 运行在HDFS上的应用具有很大的数据集。HDFS上的一个典型文件大小一般都在G字节至T字节。因此，HDFS被调节以支持大文件存储。它应该能提供整体上高的数据传输带宽，能在一个集群里扩展到数百个节点。一个单一的HDFS实例应该能支撑数以千万计的文件。 简单的一致性模型​ HDFS应用需要一个“一次写入多次读取”的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。Map/Reduce应用或者网络爬虫应用都非常适合这个模型。目前扩充这个模型，使之支持文件的附加写操作。 “移动计算比移动数据更划算”​ 一个应用请求的计算，离它操作的数据越近就越高效，在数据达到海量级别的时候更是如此。HDFS为应用提供了将它们自己移动到数据附近的接口。 异构软硬件平台间的可移植性​ HDFS在设计的时候就考虑到平台的可移植性。这种特性方便了HDFS作为大规模数据应用平台的推广。 2 HDFS基本操作篇2.1 HDFS的shell(命令行客户端)操作2.1.1 支持的命令参数123456789101112131415161718192021222324252627282930313233343536[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;][-cat [-ignoreCrc] &lt;src&gt; ...][-checksum &lt;src&gt; ...][-chgrp [-R] GROUP PATH...][-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...][-chown [-R] [OWNER][:[GROUP]] PATH...][-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-count [-q] &lt;path&gt; ...][-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;][-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]][-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;][-df [-h] [&lt;path&gt; ...]][-du [-s] [-h] &lt;path&gt; ...][-expunge][-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-getfacl [-R] &lt;path&gt;][-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;][-help [cmd ...]][-ls [-d] [-h] [-R] [&lt;path&gt; ...]][-mkdir [-p] &lt;path&gt; ...][-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;][-moveToLocal &lt;src&gt; &lt;localdst&gt;][-mv &lt;src&gt; ... &lt;dst&gt;][-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;][-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...][-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...][-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]][-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...][-stat [format] &lt;path&gt; ...][-tail [-f] &lt;file&gt;][-test -[defsz] &lt;path&gt;][-text [-ignoreCrc] &lt;src&gt; ...][-touchz &lt;path&gt; ...][-usage [cmd ...]] 2.1.2 常用命令参数介绍 -help 功能：输出这个命令参数手册 -ls 功能：显示目录信息 示例： hadoop fs -ls hdfs://hadoop-server01:9000/ 备注：这些参数中，所有的hdfs路径都可以简写 –&gt;hadoop fs -ls / 等同于上一条命令的效果 -mkdir 功能：在hdfs上创建目录 示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd -moveFromLocal 功能：从本地剪切粘贴到hdfs 示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal 功能：从hdfs剪切粘贴到本地 示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt –appendToFile 功能：追加一个文件到已经存在的文件末尾 示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop-server01:9000/hello.txt 可以简写为： Hadoop fs -appendToFile ./hello.txt /hello.txt -cat 功能：显示文件内容 示例：hadoop fs -cat /hello.txt -tail 功能：显示一个文件的末尾 示例：hadoop fs -tail /weblog/access_log.1 -text 功能：以字符形式打印一个文件的内容 示例：hadoop fs -text /weblog/access_log.1 -chgrp -chmod -chown 功能：linux 文件系统中的用法一样，对文件所属权限 示例： hadoop fs -chmod 666 /hello.txt hadoop fs -chown someuser:somegrp /hello.txt -copyFromLocal 功能：从本地文件系统中拷贝文件到hdfs路径去 示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal 功能：从hdfs拷贝到本地 示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp 功能：从hdfs的一个路径拷贝hdfs的另一个路径 示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv 功能：在hdfs目录中移动文件 示例： hadoop fs -mv /aaa/jdk.tar.gz / -get 功能：等同于copyToLocal**，就是从hdfs**下载文件到本地 示例：hadoop fs -get /aaa/jdk.tar.gz -getmerge 功能：合并下载多个文件 示例：比如hdfs的目录 /aaa/**下有多个文件:log.1, log.2,log.3,… hadoop fs -getmerge /aaa/log.* ./log.sum -put 功能：等同于copyFromLocal 示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm 功能：删除文件或文件夹 示例：hadoop fs -rm -r /aaa/bbb/ -rmdir 功能：删除空目录 示例：hadoop fs -rmdir /aaa/bbb/ccc -df 功能：统计文件系统的可用空间信息 示例：hadoop fs -df -h / -du 功能：统计文件夹的大小信息 示例： *hadoop fs -du -s -h /aaa/** -count 功能：统计一个指定目录下的文件节点数量 示例：hadoop fs -count /aaa/ -setrep 功能：设置hdfs中文件的副本数量 示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIVE运行过程]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhive%2Fmapreduce%E8%BD%AC%E6%8D%A2%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Hive与HadoopHive的执行入口是Driver，执行的SQL语句首先提交到Drive驱动，然后调用compiler解释驱动，最终解释成MapReduce任务去执行。 Hive的服务端组件 Driver组件：该组件包括：Compiler、Optimizer、Executor,它可以将Hive的编译、解析、优化转化为MapReduce任务提交给Hadoop1中的JobTracker或者是Hadoop2中的SourceManager来进行实际的执行相应的任务。 MetaStore组件：存储着hive的元数据信息，将自己的元数据存储到了关系型数据库当中，支持的数据库主要有：Mysql、Derby、支持把metastore独立出来放在远程的集群上面，使得hive更加健壮。元数据主要包括了表的名称、表的列、分区和属性、表的属性（是不是外部表等等）、表的数据所在的目录。** 用户接口：CLI（Command Line Interface)(常用的接口：命令行模式）、Client:Hive的客户端用户连接至Hive Server ,在启动Client的时候，需要制定Hive Server所在的节点，并且在该节点上启动Hive Server、WUI:通过浏览器的方式访问Hive。 Hive的工作原理 流程大致步骤为： 用户提交查询等任务给Driver。 编译器获得该用户的任务Plan。 编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息。 编译器Compiler得到元数据信息，对任务进行编译，先将HiveQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。 将最终的计划提交给Driver。 Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker或者SourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作。 获取执行的结果。 取得并返回执行结果。 http://blog.csdn.net/u010738184/article/details/70893161]]></content>
      <categories>
        <category>大数据</category>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis高级教程]]></title>
    <url>%2F2018%2F03%2F15%2Fredis%2F%E9%AB%98%E7%BA%A7%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Redis 数据备份与恢复Redis SAVE 命令用于创建当前数据库的备份。 语法redis Save 命令基本语法如下： 1redis 127.0.0.1:6379&gt; SAVE 实例12redis 127.0.0.1:6379&gt; SAVE OK 该命令将在 redis 安装目录中创建dump.rdb文件。 恢复数据如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可。获取 redis 目录可以使用 CONFIG 命令，如下所示： 123redis 127.0.0.1:6379&gt; CONFIG GET dir1) &quot;dir&quot;2) &quot;/usr/local/redis/bin&quot; 以上命令 CONFIG GET dir 输出的 redis 安装目录为 /usr/local/redis/bin。 Bgsave创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。 实例123127.0.0.1:6379&gt; BGSAVEBackground saving started Redis 安全我们可以通过 redis 的配置文件设置密码参数，这样客户端连接到 redis 服务就需要密码验证，这样可以让你的 redis 服务更安全。 实例我们可以通过以下命令查看是否设置了密码验证： 123127.0.0.1:6379&gt; CONFIG get requirepass1) &quot;requirepass&quot;2) &quot;&quot; 默认情况下 requirepass 参数是空的，这就意味着你无需通过密码验证就可以连接到 redis 服务。 你可以通过以下命令来修改该参数： 12345127.0.0.1:6379&gt; CONFIG set requirepass &quot;runoob&quot;OK127.0.0.1:6379&gt; CONFIG get requirepass1) &quot;requirepass&quot;2) &quot;runoob&quot; 设置密码后，客户端连接 redis 服务就需要密码验证，否则无法执行命令。 语法AUTH 命令基本语法格式如下： 1127.0.0.1:6379&gt; AUTH password 实例123456127.0.0.1:6379&gt; AUTH &quot;runoob&quot;OK127.0.0.1:6379&gt; SET mykey &quot;Test value&quot;OK127.0.0.1:6379&gt; GET mykey&quot;Test value&quot; Redis 性能测试Redis 性能测试是通过同时执行多个命令实现的。 语法redis 性能测试的基本命令如下： 1redis-benchmark [option] [option value] 实例以下实例同时执行 10000 个请求来检测性能： 1234567891011121314151617redis-benchmark -n 10000PING_INLINE: 141043.72 requests per secondPING_BULK: 142857.14 requests per secondSET: 141442.72 requests per secondGET: 145348.83 requests per secondINCR: 137362.64 requests per secondLPUSH: 145348.83 requests per secondLPOP: 146198.83 requests per secondSADD: 146198.83 requests per secondSPOP: 149253.73 requests per secondLPUSH (needed to benchmark LRANGE): 148588.42 requests per secondLRANGE_100 (first 100 elements): 58411.21 requests per secondLRANGE_300 (first 300 elements): 21195.42 requests per secondLRANGE_500 (first 450 elements): 14539.11 requests per secondLRANGE_600 (first 600 elements): 10504.20 requests per secondMSET (10 keys): 93283.58 requests per second redis 性能测试工具可选参数如下所示： 序号 选项 描述 默认值 1 -h 指定服务器主机名 127.0.0.1 2 -p 指定服务器端口 6379 3 -s 指定服务器 socket 4 -c 指定并发连接数 50 5 -n 指定请求数 10000 6 -d 以字节的形式指定 SET/GET 值的数据大小 2 7 -k 1=keep alive 0=reconnect 1 8 -r SET/GET/INCR 使用随机 key, SADD 使用随机值 9 -P 通过管道传输 请求 1 10 -q 强制退出 redis。仅显示 query/sec 值 11 –csv 以 CSV 格式输出 12 -l 生成循环，永久执行测试 13 -t 仅运行以逗号分隔的测试命令列表。 14 -I Idle 模式。仅打开 N 个 idle 连接并等待。 实例以下实例我们使用了多个参数来测试 redis 性能： 1234redis-benchmark -h 127.0.0.1 -p 6379 -t set,lpush -n 10000 -qSET: 146198.83 requests per secondLPUSH: 145560.41 requests per second 以上实例中主机为 127.0.0.1，端口号为 6379，执行的命令为 set,lpush，请求数为 10000，通过 -q 参数让结果只显示每秒执行的请求数。 Redis 客户端连接Redis 通过监听一个 TCP 端口或者 Unix socket 的方式来接收来自客户端的连接，当一个连接建立后，Redis 内部会进行以下一些操作： 首先，客户端 socket 会被设置为非阻塞模式，因为 Redis 在网络事件处理上采用的是非阻塞多路复用模型。 然后为这个 socket 设置 TCP_NODELAY 属性，禁用 Nagle 算法 然后创建一个可读的文件事件用于监听这个客户端 socket 的数据发送 最大连接数在 Redis2.4 中，最大连接数是被直接硬编码在代码里面的，而在2.6版本中这个值变成可配置的。 maxclients 的默认值是 10000，你也可以在 redis.conf 中对这个值进行修改。 1234config get maxclients1) &quot;maxclients&quot;2) &quot;10000&quot; 实例以下实例我们在服务启动时设置最大连接数为 100000： 1redis-server --maxclients 100000 客户端命令 S.N. 命令 描述 1 CLIENT LIST 返回连接到 redis 服务的客户端列表 2 CLIENT SETNAME 设置当前连接的名称 3 CLIENT GETNAME 获取通过 CLIENT SETNAME 命令设置的服务名称 4 CLIENT PAUSE 挂起客户端连接，指定挂起的时间以毫秒计 5 CLIENT KILL 关闭客户端连接 Redis 管道技术Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。 服务端处理命令，并将结果返回给客户端。 Redis 管道技术Redis 管道技术可以在服务端未响应时，客户端可以继续向服务端发送请求，并最终一次性读取所有服务端的响应。 实例查看 redis 管道，只需要启动 redis 实例并输入以下命令： 12345678$(echo -en &quot;PING\r\n SET runoobkey redis\r\nGET runoobkey\r\nINCR visitor\r\nINCR visitor\r\nINCR visitor\r\n&quot;; sleep 10) | nc localhost 6379+PONG+OKredis:1:2:3 以上实例中我们通过使用 PING 命令查看redis服务是否可用， 之后我们们设置了 runoobkey 的值为 redis，然后我们获取 runoobkey 的值并使得 visitor 自增 3 次。 在返回的结果中我们可以看到这些命令一次性向 redis 服务提交，并最终一次性读取所有服务端的响应 管道技术的优势管道技术最显著的优势是提高了 redis 服务的性能。 一些测试数据在下面的测试中，我们将使用Redis的Ruby客户端，支持管道技术特性，测试管道技术对速度的提升效果。 123456789101112131415161718192021222324252627require &apos;rubygems&apos; require &apos;redis&apos;def bench(descr) start = Time.now yield puts &quot;#&#123;descr&#125; #&#123;Time.now-start&#125; seconds&quot; enddef without_pipelining r = Redis.new 10000.times &#123; r.ping &#125; enddef with_pipelining r = Redis.new r.pipelined &#123; 10000.times &#123; r.ping &#125; &#125; endbench(&quot;without pipelining&quot;) &#123; without_pipelining &#125; bench(&quot;with pipelining&quot;) &#123; with_pipelining &#125; 从处于局域网中的Mac OS X系统上执行上面这个简单脚本的数据表明，开启了管道操作后，往返时延已经被改善得相当低了。 12without pipelining 1.185238 seconds with pipelining 0.250783 seconds 如你所见，开启管道后，我们的速度效率提升了5倍。 Redis 分区分区是分割数据到多个Redis实例的处理过程，因此每个实例只保存key的一个子集。 分区的优势 通过利用多台计算机内存的和值，允许我们构造更大的数据库。 通过多核和多台计算机，允许我们扩展计算能力；通过多台计算机和网络适配器，允许我们扩展网络带宽。 分区的不足redis的一些特性在分区方面表现的不是很好： 涉及多个key的操作通常是不被支持的。举例来说，当两个set映射到不同的redis实例上时，你就不能对这两个set执行交集操作。 涉及多个key的redis事务不能使用。 当使用分区时，数据处理较为复杂，比如你需要处理多个rdb/aof文件，并且从多个实例和主机备份持久化文件。 增加或删除容量也比较复杂。redis集群大多数支持在运行时增加、删除节点的透明数据平衡的能力，但是类似于客户端分区、代理等其他系统则不支持这项特性。然而，一种叫做presharding的技术对此是有帮助的。 分区类型Redis 有两种类型分区。 假设有4个Redis实例 R0，R1，R2，R3，和类似user:1，user:2这样的表示用户的多个key，对既定的key有多种不同方式来选择这个key存放在哪个实例中。也就是说，有不同的系统来映射某个key到某个Redis服务。 范围分区最简单的分区方式是按范围分区，就是映射一定范围的对象到特定的Redis实例。 比如，ID从0到10000的用户会保存到实例R0，ID从10001到 20000的用户会保存到R1，以此类推。 这种方式是可行的，并且在实际中使用，不足就是要有一个区间范围到实例的映射表。这个表要被管理，同时还需要各 种对象的映射表，通常对Redis来说并非是好的方法。 哈希分区另外一种分区方法是hash分区。这对任何key都适用，也无需是object_name:这种形式，像下面描述的一样简单： 用一个hash函数将key转换为一个数字，比如使用crc32 hash函数。对key foobar执行crc32(foobar)会输出类似93024922的整数。 对这个整数取模，将其转化为0-3之间的数字，就可以将这个整数映射到4个Redis实例中的一个了。93024922 % 4 = 2，就是说key foobar应该被存到R2实例中。注意：取模操作是取除的余数，通常在多种编程语言中用%操作符实现。 Java 使用 Redis安装开始在 Java 中使用 Redis 前， 我们需要确保已经安装了 redis 服务及 Java redis 驱动，且你的机器上能正常使用 Java。 Java的安装配置可以参考我们的 Java开发环境配置 接下来让我们安装 Java redis 驱动： 首先你需要下载驱动包 下载 jedis.jar，确保下载最新驱动包。 在你的 classpath 中包含该驱动包。 本站提供了 2.9.0 jar 版本下载： jedis-2.9.0.jar 连接到 redis 服务实例1234567891011import redis.clients.jedis.Jedis;public class RedisJava &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis("localhost"); System.out.println("连接成功"); //查看服务是否运行 System.out.println("服务正在运行: "+jedis.ping()); &#125;&#125; 编译以上 Java 程序，确保驱动包的路径是正确的。 12连接成功服务正在运行: PONG Redis Java String(字符串) 实例实例123456789101112import redis.clients.jedis.Jedis;public class RedisStringJava &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis("localhost"); System.out.println("连接成功"); //设置 redis 字符串数据 jedis.set("runoobkey", "www.runoob.com"); // 获取存储的数据并输出 System.out.println("redis 存储的字符串为: "+ jedis.get("runoobkey")); &#125;&#125; 编译以上程序。 12连接成功redis 存储的字符串为: www.runoob.com Redis Java List(列表) 实例实例12345678910111213141516171819import java.util.List;import redis.clients.jedis.Jedis; public class RedisListJava &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis("localhost"); System.out.println("连接成功"); //存储数据到列表中 jedis.lpush("site-list", "Runoob"); jedis.lpush("site-list", "Google"); jedis.lpush("site-list", "Taobao"); // 获取存储的数据并输出 List&lt;String&gt; list = jedis.lrange("site-list", 0 ,2); for(int i=0; i&lt;list.size(); i++) &#123; System.out.println("列表项为: "+list.get(i)); &#125; &#125;&#125; 编译以上程序。 1234连接成功列表项为: Taobao列表项为: Google列表项为: Runoob Redis Java Keys 实例实例12345678910111213141516171819import java.util.Iterator;import java.util.Set;import redis.clients.jedis.Jedis; public class RedisKeyJava &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis("localhost"); System.out.println("连接成功"); // 获取数据并输出 Set&lt;String&gt; keys = jedis.keys("*"); Iterator&lt;String&gt; it=keys.iterator() ; while(it.hasNext())&#123; String key = it.next(); System.out.println(key); &#125; &#125;&#125; 编译以上程序。 123连接成功runoobkeysite-list]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
</search>
